{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will implement the encoder–decoder architecture of [Sutskever et al. (2014)](https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf), including the attention-based extension of [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473), and evaluate this architecture on a machine translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the models in this notebook requires significant compute power, and we strongly recommend using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a system that translates from German (our **source language**) to English (our **target language**). The dataset is a collection of parallel English–German sentences taken from translations of subtitles for TED talks. It was derived from the [TED2013](https://opus.nlpl.eu/TED2013-v1.1.php) dataset, which is available in the [OPUS](http://opus.nlpl.eu/) collection. The code cell below prints the first lines in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-de.txt') as src, open('train-en.txt') as tgt:\n",
    "    for i, src_sentence, tgt_sentence in zip(range(5), src, tgt):\n",
    "        print(f'{i}: {src_sentence.rstrip()} / {tgt_sentence.rstrip()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some ‘sentences’ are actually *sequences* of sentences, but we will use the term *sentence* nevertheless. All sentences are whitespace-tokenised and lowercased. To make your life a bit easier, we have removed sentences longer than 25 words. \n",
    "\n",
    "The next cell contains code that yields the sentences contained in a file as lists of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences(filename):\n",
    "    with open(filename) as source:\n",
    "        for line in source:\n",
    "            yield line.rstrip().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Build the vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to build the vocabularies for the data, one vocabulary for each language. Each vocabulary should contain the 10,000 most frequent words in the training data for the respective language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(sentences, max_size):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation must comply with the following specification:\n",
    "\n",
    "**make_vocab** (*sentences*, *max_size*)\n",
    "\n",
    "> Returns a dictionary that maps the most frequent words in the *sentences* to a contiguous range of integers starting at&nbsp;0. The first four mappings in this dictionary are reserved for the pseudowords `<pad>` (padding, id&nbsp;0), `<bos>` (beginning of sequence, id&nbsp;1), `<eos>` (end of sequence, id&nbsp;2), and `<unk>` (unknown word, id&nbsp;3). The parameter *max_size* caps the size of the dictionary, including the pseudowords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function, we can construct the vocabularies as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = make_vocab(sentences('train-de.txt'), 10000)\n",
    "tgt_vocab = make_vocab(sentences('train-en.txt'), 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "To test you code, check that each vocabulary contains 10,000 words, including the pseudowords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell defines a class for the parallel dataset. We sub-class the abstract [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class, which represents map-style datasets in PyTorch. This will let us use standard infrastructure related to the loading and automatic batching of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, src_vocab, src_filename, tgt_vocab, tgt_filename):\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "        # We hard-wire the codes for <bos> (1), <eos> (2), and <unk> (3).\n",
    "        self.src = [[self.src_vocab.get(w, 3) for w in s] for s in sentences(src_filename)]\n",
    "        self.tgt = [[self.tgt_vocab.get(w, 3) for w in s] + [2] for s in sentences(tgt_filename)]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.tgt[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(src_vocab, 'train-de.txt', tgt_vocab, 'train-en.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will be helpful for debugging. It extracts a single source–target pair of sentences from the specified *dataset* and converts it into batches of size&nbsp;1, which can be fed into the encoder–decoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example(dataset, i):\n",
    "    src, tgt = dataset[i]\n",
    "    return torch.LongTensor(src).unsqueeze(0), torch.LongTensor(tgt).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example(train_dataset, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: The encoder–decoder architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement the encoder–decoder architecture, including the extension of that architecture by an attention mechanism. The implementation consists of four parts: the encoder, the attention mechanism, the decoder, and a class that wraps the complete architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1: Implement the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is relatively straightforward. We look up word embeddings and unroll a bidirectional GRU over the embedding vectors to compute a representation at each token position. We then take the last hidden state of the forward GRU and the last hidden state of the backward GRU, concatenate them, and pass them through a linear layer. This produces a summary of the source sentence, which we will later feed into the decoder.\n",
    "\n",
    "To solve this problem, complete the skeleton code in the next code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_words, embedding_dim=256, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        # TODO: Add your code here\n",
    "\n",
    "    def forward(self, src):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code must comply with the following specification:\n",
    "\n",
    "**__init__** (*num_words*, *embedding_dim* = 256, *hidden_dim* = 512)\n",
    "\n",
    "> Initialises the encoder. The encoder consists of an embedding layer that maps each of *num_words* words to an embedding vector of size *embedding_dim*, a bidirectional GRU that maps each embedding vector to a position-specific representation of size 2 × *hidden_dim*, and a final linear layer that projects the concatenation of the final hidden states of the GRU (the final hidden state of the forward direction and the final hidden state of the backward direction) to a single vector of size *hidden_dim*.\n",
    "\n",
    "**forward** (*self*, *src*)\n",
    "\n",
    "> Takes a tensor *src* with source-language word ids and sends it through the encoder. The input tensor has shape (*batch_size*, *src_len*), where *src_len* is the length of the sentences in the batch. (We will make sure that all sentences in the same batch have the same length.) The method returns a pair of tensors (*output*, *hidden*), where *output* has shape (*batch_size*, *src_len*, 2 × *hidden_dim*), and *hidden* has shape (*batch_size*, *hidden_dim*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "To test your code, instantiate an encoder, feed it the first source sentence in the training data, and check that the tensors returned by the encoder have the expected shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2: Implement the attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to implement the attention mechanism. Recall that the purpose of this mechanism is to inform the decoder when generating the translation of the next word. For this, attention has access to the previous hidden state of the decoder, as well as the complete output of the encoder. It returns the attention-weighted sum of the encoder output, the so-called *context* vector. For later usage, we also return the attention weights.\n",
    "\n",
    "As mentioned in the lecture, attention can be implemented in various ways. One very simple implementation is *uniform attention*, which assigns equal weight to each position-specific representation in the output of the encoder, and completely ignores the hidden state of the decoder. This mechanism is implemented in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class UniformAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_output, src_mask):\n",
    "        batch_size, src_len, _ = encoder_output.shape\n",
    "\n",
    "        # Set all attention scores to the same constant value (0). After\n",
    "        # the softmax, we will have uniform weights.\n",
    "        scores = torch.zeros(batch_size, src_len, device=encoder_output.device)\n",
    "\n",
    "        # Mask out the attention scores for the padding tokens. We set\n",
    "        # them to -inf. After the softmax, we will have 0.\n",
    "        scores.data.masked_fill_(~src_mask, -float('inf'))\n",
    "\n",
    "        # Convert scores into weights\n",
    "        alpha = F.softmax(scores, dim=1)\n",
    "\n",
    "        # The context is the alpha-weighted sum of the encoder outputs.\n",
    "        context = torch.bmm(alpha.unsqueeze(1), encoder_output).squeeze(1)\n",
    "\n",
    "        return context, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One technical detail in this code is our use of a mask *src_mask* to compute attention weights only for the ‘real’ tokens in the source sentences, but not for the padding tokens that we introduce to bring all sentences in a batch to the same length. Your task now is to implement the attention mechanism from the paper by [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473). The relevant equation is in Section&nbsp;A.1.2.\n",
    "\n",
    "Here is the skeleton code for this problem. As you can see, your specific task is to initialise the required parameters and to compute the attention scores (*scores*); the rest of the code is the same as for the uniform attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        # TODO: Add your code here\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_output, src_mask):\n",
    "        batch_size, src_len, _ = encoder_output.shape\n",
    "\n",
    "        # TODO: Replace the next line with your own code\n",
    "        scores = torch.zeros(batch_size, src_len, device=encoder_output.device)\n",
    "\n",
    "        # The rest of the code is as in UniformAttention\n",
    "\n",
    "        # Mask out the attention scores for the padding tokens. We set\n",
    "        # them to -inf. After the softmax, we will have 0.\n",
    "        scores.data.masked_fill_(~src_mask, -float('inf'))\n",
    "\n",
    "        # Convert scores into weights\n",
    "        alpha = F.softmax(scores, dim=1)\n",
    "\n",
    "        # The context vector is the alpha-weighted sum of the encoder outputs.\n",
    "        context = torch.bmm(alpha.unsqueeze(1), encoder_output).squeeze(1)\n",
    "\n",
    "        return context, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code must comply with the following specification:\n",
    "\n",
    "**forward** (*decoder_hidden*, *encoder_output*, *src_mask*)\n",
    "\n",
    "> Takes the previous hidden state of the decoder (*decoder_hidden*) and the encoder output (*encoder_output*) and returns a pair (*context*, *alpha*) where *context* is the context as computed as in [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473), and *alpha* are the corresponding attention weights. The hidden state has shape (*batch_size*, *hidden_dim*), the encoder output has shape (*batch_size*, *src_len*, 2 × *hidden_dim*), the context has shape (*batch_size*, 2 × *hidden_dim*), and the attention weights have shape (*batch_size*, *src_len*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "To test your code, extend your test from Problem&nbsp;2.1: Feed the output of your encoder into your attention class. As the previous hidden state of the decoder, you can use the hidden state returned by the encoder. You will also need to create a source mask; this can be done as follows:\n",
    "\n",
    "```\n",
    "src_mask = (src != 0)\n",
    "```\n",
    "\n",
    "Check that the context tensor and the attention weights returned by the attention class have the expected shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3: Implement the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to implement the decoder. Like the encoder, the decoder is based on a GRU; but this time we use a unidirectional network, as we generate the target sentences left-to-right.\n",
    "\n",
    "**⚠️ We expect that solving this problem will take you the longest time in this lab.**\n",
    "\n",
    "Because the decoder is an autoregressive model, we need to unroll the GRU ‘manually’: At each position, we take the previous hidden state as well as the new input, and apply the GRU for one step. The initial hidden state comes from the encoder. The new input is the embedding of the previous word, concatenated with the context vector from the attention model. To produce the final output, we take the output of the GRU, concatenate the embedding vector and the context vector (residual connection), and feed the result into a linear layer. Here is a graphical representation:\n",
    "\n",
    "<img src=\"https://gitlab.liu.se/nlp/nlp-course/-/raw/master/labs/l5/decoder.svg\" width=\"50%\" alt=\"Decoder architecture\"/>\n",
    "\n",
    "We need to implement this manual unrolling for two very similar tasks: When *training*, both the inputs to and the target outputs of the GRU come from the training data. When *decoding*, the outputs of the GRU are used to generate new target-side words, and these words become the inputs to the next step of the unrolling. We have implemented methods `forward` and `decode` for these two different modes of usage. Your task is to implement a method `step` that takes a single step with the GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_words, attention, embedding_dim=256, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_words, embedding_dim)\n",
    "        self.attention = attention\n",
    "        # TODO: Add your own code\n",
    "\n",
    "    def forward(self, encoder_output, hidden, src_mask, tgt):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "\n",
    "        # Lookup the embeddings for the previous words\n",
    "        embedded = self.embedding(tgt)\n",
    "\n",
    "        # Initialise the list of outputs (in each sentence)\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(tgt_len):\n",
    "            # Get the embedding for the previous word (in each sentence)\n",
    "            prev_embedded = embedded[:, i]\n",
    "\n",
    "            # Take one step with the RNN\n",
    "            output, hidden, alpha = self.step(encoder_output, hidden, src_mask, prev_embedded)\n",
    "\n",
    "            # Update the list of outputs (in each sentence)\n",
    "            outputs.append(output.unsqueeze(1))\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "    def decode(self, encoder_output, hidden, src_mask, max_len):\n",
    "        batch_size = encoder_output.size(0)\n",
    "\n",
    "        # Initialise the list of generated words and attention weights (in each sentence)\n",
    "        generated = [torch.ones(batch_size, dtype=torch.long, device=hidden.device)]\n",
    "        alphas = []\n",
    "\n",
    "        for i in range(max_len):\n",
    "            # Get the embedding for the previous word (in each sentence)\n",
    "            prev_embedded = self.embedding(generated[-1])\n",
    "\n",
    "            # Take one step with the RNN\n",
    "            output, hidden, alpha = self.step(encoder_output, hidden, src_mask, prev_embedded)\n",
    "\n",
    "            # Update the list of generated words and attention weights (in each sentence)\n",
    "            generated.append(output.argmax(-1))\n",
    "            alphas.append(alpha)\n",
    "\n",
    "        generated = [x.unsqueeze(1) for x in generated[1:]]\n",
    "        alphas = [x.unsqueeze(1) for x in alphas]\n",
    "            \n",
    "        return torch.cat(generated, dim=1), torch.cat(alphas, dim=1)\n",
    "\n",
    "    def step(self, encoder_output, hidden, src_mask, prev_embedded):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should comply with the following specification:\n",
    "\n",
    "**step** (*self*, *encoder_output*, *hidden*, *src_mask*, *prev_embedded*)\n",
    "\n",
    "> Performs a single step in the manual unrolling of the decoder GRU. This takes the output of the encoder (*encoder_output*), the previous hidden state of the decoder (*hidden*), the source mask as described in Problem&nbsp;2.2 (*src_mask*), and the embedding vector of the previous word (*prev_embedded*), and computes the output as described above.\n",
    ">\n",
    "> The shape of *encoder_output* is (*batch_size*, *src_len*, 2 × *hidden_dim*); the shape of *hidden* is (*batch_size*, *hidden_dim*); the shape of *src_mask* is (*batch_size*, *src_len*); and the shape of *prev_embedded* is (*batch_size*, *embedding_dim*).\n",
    ">\n",
    "> The method returns a triple of tensors (*output*, *hidden*, *alpha*) where *output* is the position-specific output of the GRU, of shape (*batch_size*, *num_words*); *hidden* is the new hidden state, of shape (*batch_size*, *hidden_dim*); and *alpha* are the attention weights that were used to compute the *output*, of shape (*batch_size*, *src_len*).\n",
    "\n",
    "#### 💡 Hints on the implementation\n",
    "\n",
    "**Batch first!** Per default, the GRU implementation in PyTorch (just as the LSTM implementation) expects its input to be a three-dimensional tensor of the form (*seq_len*, *batch_size*, *input_size*). We find it conceptually easier to change this default behaviour and let the models take their input in the form (*batch_size*, *seq_len*, *input_size*). To do so, set *batch_first=True* when instantiating the GRU.\n",
    "\n",
    "**Unsqueeze and squeeze.** When doing the unrolling manually, we get the input in the form (*batch_size*, *input_size*). To convert between this representation and the (*batch_size*, *seq_len*, *input_size*) representation, you can use [`unsqueeze`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html) and [`squeeze`](https://pytorch.org/docs/stable/generated/torch.squeeze.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "To test your code, extend your test from the previous problems, and simulate a complete forward pass of the encoder–decoder architecture on the example sentence. Check the shapes of the resulting tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder–decoder wrapper class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part of the implementation is a class that wraps the encoder and the decoder as a single model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, attention):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size)\n",
    "        self.decoder = Decoder(tgt_vocab_size, attention)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        encoder_output, hidden = self.encoder(src)\n",
    "        return self.decoder.forward(encoder_output, hidden, src != 0, tgt)\n",
    "\n",
    "    def decode(self, src, max_len):\n",
    "        encoder_output, hidden = self.encoder(src)\n",
    "        return self.decoder.decode(encoder_output, hidden, src != 0, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "As a final test, instantiate an encoder–decoder model and use it to decode the example sentence. Check the shapes of the resulting tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Train a translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the pieces to build and train a complete translation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translator class\n",
    "\n",
    "We first define a class `Translator` that initialises an encoder–decoder model and uses it to translate sentences. It can also return the attention weights that were used to produce the translation of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(object):\n",
    "\n",
    "    def __init__(self, src_vocab, tgt_vocab, attention, device=torch.device('cpu')):\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.device = device\n",
    "        self.model = EncoderDecoder(len(src_vocab), len(tgt_vocab), attention).to(device)\n",
    "\n",
    "    def translate_with_attention(self, sentences):\n",
    "        # Encode each sentence\n",
    "        encoded = [[self.src_vocab.get(w, 3) for w in s.split()] for s in sentences]\n",
    "\n",
    "        # Determine the maximal length of an encoded sentence\n",
    "        max_len = max(len(e) for e in encoded)\n",
    "\n",
    "        # Build the input tensor, padding all sequences to the same length\n",
    "        src = torch.LongTensor([e + [0] * (max_len - len(e)) for e in encoded]).to(self.device)\n",
    "\n",
    "        # Run the decoder and convert the result into nested lists\n",
    "        with torch.no_grad():\n",
    "            decoded, alphas = tuple(d.cpu().numpy().tolist() for d in self.model.decode(src, 2 * max_len))\n",
    "\n",
    "        # Prune each decoded sentence after the first <eos>\n",
    "        i2w = {i: w for w, i in self.tgt_vocab.items()}\n",
    "        result = []\n",
    "        for d, a in zip(decoded, alphas):\n",
    "            d = [i2w[i] for i in d]\n",
    "            try:\n",
    "                eos_index = d.index('<eos>')\n",
    "                del d[eos_index:]\n",
    "                del a[eos_index:]\n",
    "            except:\n",
    "                pass\n",
    "            result.append((' '.join(d), a))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def translate(self, sentences):\n",
    "        translated, alphas = zip(*self.translate_with_attention(sentences))\n",
    "        return translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how this class is supposed to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(src_vocab, tgt_vocab, BahdanauAttention())\n",
    "translator.translate(['ich weiß nicht .', 'das haus ist klein .'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function\n",
    "\n",
    "As mentioned in the lectures, machine translation systems are typically evaluated using the BLEU metric. Here we use the implementation of this metric from the `sacrebleu` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If sacrebleu is not found, uncomment the next line:\n",
    "# !pip install sacrebleu\n",
    "\n",
    "import sacrebleu\n",
    "\n",
    "def bleu(translator, src, ref):\n",
    "    translated = translator.translate(src)\n",
    "    return sacrebleu.raw_corpus_bleu(translated, [ref], 0.01).score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will report the BLEU score on the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('valid-de.txt') as src, open('valid-en.txt') as ref:\n",
    "    valid_src = [line.rstrip() for line in src]\n",
    "    valid_ref = [line.rstrip() for line in ref]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batcher class\n",
    "\n",
    "To prepare the training, we next create a class that takes a batch of encoded parallel sentences (a pair of lists of integers) and transforms it into two tensors, one for the source side and one for the target side. Each tensor contains sequences padded to the length of the longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationBatcher(object):\n",
    "\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        srcs, tgts = zip(*batch)\n",
    "\n",
    "        # Determine the maximal length of a source/target sequence\n",
    "        max_src_len = max(len(s) for s in srcs)\n",
    "        max_tgt_len = max(len(t) for t in tgts)\n",
    "\n",
    "        # Create the source/target tensors\n",
    "        S = torch.LongTensor([s + [0] * (max_src_len - len(s)) for s in srcs])\n",
    "        T = torch.LongTensor([t + [0] * (max_tgt_len - len(t)) for t in tgts])\n",
    "\n",
    "        return S.to(self.device), T.to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "The training loop is pretty standard. We use a few new utilities from the PyTorch ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(n_epochs=2, batch_size=128, lr=5e-4):\n",
    "    # Build the vocabularies\n",
    "    vocab_src = make_vocab(sentences('train-de.txt'), 10000)\n",
    "    vocab_tgt = make_vocab(sentences('train-en.txt'), 10000)\n",
    "\n",
    "    # Prepare the dataset\n",
    "    train_dataset = TranslationDataset(vocab_src, 'train-de.txt', vocab_tgt, 'train-en.txt')\n",
    "\n",
    "    # Prepare the data loaders\n",
    "    batcher = TranslationBatcher(device)\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=batcher)\n",
    "\n",
    "    # Build the translator\n",
    "    translator = Translator(src_vocab, tgt_vocab, BahdanauAttention(), device=device)\n",
    "\n",
    "    # Initialise the optimiser\n",
    "    optimizer = torch.optim.Adam(translator.model.parameters(), lr=lr)\n",
    "\n",
    "    # Make it possible to interrupt the training\n",
    "    try:\n",
    "        for epoch in range(n_epochs):\n",
    "            losses = []\n",
    "            bleu_valid = 0\n",
    "            sample = '<none>'\n",
    "            with tqdm(total=len(train_dataset)) as pbar:\n",
    "                for i, (src_batch, tgt_batch) in enumerate(train_loader):\n",
    "                    # Create a shifted version of tgt_batch containing the previous words\n",
    "                    batch_size, tgt_len = tgt_batch.shape\n",
    "                    bos = torch.ones(batch_size, 1, dtype=torch.long, device=tgt_batch.device)\n",
    "                    tgt_batch_shifted = torch.cat((bos, tgt_batch[:, :-1]), dim=1)\n",
    "\n",
    "                    translator.model.train()\n",
    "\n",
    "                    # Forward pass\n",
    "                    scores = translator.model(src_batch, tgt_batch_shifted)\n",
    "                    scores = scores.view(-1, len(tgt_vocab))\n",
    "\n",
    "                    # Backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = F.cross_entropy(scores, tgt_batch.view(-1), ignore_index=0)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Update the diagnostics\n",
    "                    losses.append(loss.item())\n",
    "                    pbar.set_postfix(loss=(sum(losses) / len(losses)), bleu_valid=bleu_valid, sample=sample)\n",
    "                    pbar.update(len(src_batch))\n",
    "\n",
    "                    if i % 50 == 0:\n",
    "                        translator.model.eval()\n",
    "                        bleu_valid = int(bleu(translator, valid_src, valid_ref))\n",
    "                        sample = translator.translate(['das haus ist klein .'])[0]\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    return translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to train the system. During training, two diagnostics will be printed periodically: the running average of the training loss, the BLEU score on the validation data, and the translation of a sample sentence, *das haus ist klein* (which should translate into *the house is small*).\n",
    "\n",
    "As mentioned before, training the translator takes quite a bit of compute power and time. Even with a GPU, you should expect training times per epoch of about 5–6 minutes. The default number of epochs is&nbsp;2; however, you may want to interrupt the training prematurely and use a partially trained model in case you run out of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⚠️ Your submitted notebook must contain output demonstrating at least 16 BLEU points on the validation data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Visualising attention (reflection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure&nbsp;3 in the paper by [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473) shows some heatmaps of attention weights in selected sentences. In the last problem of this lab, we ask you to inspect attention weights for your trained translation system. We define a function `plot_attention` that visualizes the attention weights. The *x* axis corresponds to the words in the source sentence (German) and the *y* axis to the generated target sentence (English). The heatmap colors represent the strengths of the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "def plot_attention(translator, sentence):\n",
    "    translation, weights = translator.translate_with_attention([sentence])[0]\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(weights, cmap='Blues_r')\n",
    "\n",
    "    ax.set_xticklabels(sentence.split(), minor=False, rotation='vertical')\n",
    "    ax.set_yticklabels(translation.split(), minor=False)\n",
    "\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_xticks(np.arange(weights.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(weights.shape[0]) + 0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.colorbar(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(translator, 'das haus ist klein .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these heatmaps to inspect the attention patterns for selected German sentences. Try to find sentences for which the model produces reasonably good English translations. If your German is a bit rusty (or non-existent), use sentences from the validation data. It might be interesting to look at examples where the German and the English word order differ substantially. Document your exploration in a short reflection piece (ca. 150 words). Respond to the following prompts:\n",
    "\n",
    "* What sentences did you try out? What patterns did you spot? Include example heatmaps in your notebook.\n",
    "* Based on what you know about attention, did you expect your results? Was there anything surprising in them?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lJl85LrCtYVV",
    "uDNPVkMDtYVd",
    "JPrnZ50HtYVe",
    "ctSveh9wtYVe"
   ],
   "name": "NLP-L5-WithSolutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
