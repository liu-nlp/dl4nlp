{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The LSTM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to implement a classifier based on the Long Short-Term Memory (LSTM) network. The architecture for this classifier is that of the *encoder* introduced in Lecture&nbsp;1.3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same data as in *Document classification with simple neural networks*.\n",
    "\n",
    "The following helper function loads the sentiment- or product-labelled sentences from a tab-separated file. It returns a list of pairs where the first component of each pair is a tokenised review (represented as a list of string tokens) and the second component is the corresponding label (an integer). By default, we cap the documents after the first 100 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DocumentDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename, max_len=100):\n",
    "        self.items = []\n",
    "        with open(filename, 'rt', encoding='utf-8') as fp:\n",
    "            for line in fp:\n",
    "                doc, label = line.rstrip().split('\\t')\n",
    "                self.items.append((doc.split()[:max_len], int(label)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this function to load the training data and the development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DocumentDataset('reviews-product-train.txt')\n",
    "dev_data = DocumentDataset('reviews-product-dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent each document by a vector containing the word ids of the tokens in the document.\n",
    "\n",
    "We first construct our vocabulary. Note that we reserve the word id&nbsp;0 for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(data):\n",
    "    vocab = {'<pad>': 0, '<unk>': 1}\n",
    "    for doc, label in data:\n",
    "        for token in doc:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the vocabulary from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = make_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create our document batcher. In each batch we right-pad shorter documents with zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentBatcher(object):\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        docs, labels = zip(*batch)\n",
    "\n",
    "        # Replace each token by its integer id\n",
    "        xs = [[self.vocab.get(t, self.vocab['<unk>']) for t in d] for d in docs]\n",
    "\n",
    "        # Right-pad each document with zeroes\n",
    "        max_len = max(len(x) for x in xs)\n",
    "        xs = [x + [self.vocab['<pad>']] * (max_len - len(x)) for x in xs]\n",
    "\n",
    "        return torch.as_tensor(xs), torch.as_tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the batcher for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcher = DocumentBatcher(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate our classify using accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y):\n",
    "    return torch.mean(torch.eq(y_pred, y).float()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to set up the LSTM model and train it using cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Unidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first consider an encoder based on a standard unidirectional LSTM. This encoder consists of three parts: an embedding layer, an LSTM, and a linear layer that projects the final hidden state of the LSTM down to the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "        # Unidirectional LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Final linear layer that projects down to the class labels\n",
    "        self.linear = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape of x: [batch_size, sequence_length]\n",
    "\n",
    "        # Replace each word id with its embedding vector\n",
    "        x = self.embedding(x)\n",
    "        # Shape of x: [batch_size, sequence_length, embedding_dim]\n",
    "\n",
    "        # Unroll the LSTM\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        # Shape of h_n: [1, batch_size, hidden_dim]\n",
    "\n",
    "        # Extract the last hidden state\n",
    "        x = h_n[-1]\n",
    "        # Shape of x: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # Shape of x: [batch_size, hidden_dim]\n",
    "\n",
    "        # Send x through the final linear layer\n",
    "        x = self.linear(x)\n",
    "        # Shape of x: [batch_size, num_classes]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider an encoder based on a bidirectional LSTM. The basic architecture is the same as before. The final hidden state is the concatenation of the final hidden states of the forward and the backward LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Final linear layer that projects down to the class labels\n",
    "        self.linear = nn.Linear(2 * hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape of x: [batch_size, sequence_length]\n",
    "\n",
    "        # Replace each word id with its embedding vector\n",
    "        x = self.embedding(x)\n",
    "        # Shape of x: [batch_size, sequence_length, embedding_dim]\n",
    "\n",
    "        # Unroll the Bi-LSTM\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        # Shape of h_n: [2, batch_size, hidden_dim]\n",
    "\n",
    "        # Extract the last hidden states\n",
    "        x = torch.cat((h_n[-1], h_n[-2]), axis=-1)\n",
    "        # Shape of x: [batch_size, 2 * hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # Shape of x: [batch_size, 2 * hidden_dim]\n",
    "\n",
    "        # Send x through the final linear layer\n",
    "        x = self.linear(x)\n",
    "        # Shape of x: [batch_size, num_classes]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "We directly present the embellished version of the training loop, which includes code for plotting and loading pre-trained embeddings (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "def train(n_epochs=5, batch_size=32, lr=1e-2, pretrained=False):\n",
    "    # Initialize the model\n",
    "    model = LSTMModel(len(vocab), 100, 50, 6)\n",
    "    nn.init.xavier_uniform_(model.embedding.weight)\n",
    "    if pretrained:\n",
    "        embeddings = torch.load('glove.pt')\n",
    "        model.embedding = nn.Embedding.from_pretrained(embeddings, freeze=False)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    # We will keep track of the losses on the two datasets\n",
    "    train_losses = []\n",
    "    dev_losses = []\n",
    "    dev_accuracies = []\n",
    "\n",
    "    # We use DataLoaders for automatic batching\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=batcher, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_data, batch_size=batch_size, collate_fn=batcher)\n",
    "\n",
    "    for t in range(n_epochs):\n",
    "        with tqdm.tqdm(total=len(train_data)) as pbar:\n",
    "            pbar.set_description(f'Epoch {t+1}')\n",
    "\n",
    "            # Start training\n",
    "            model.train()\n",
    "            running_loss = 0\n",
    "            for bx, by in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model.forward(bx)\n",
    "                loss = F.cross_entropy(output, by)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * len(bx)\n",
    "                pbar.update(len(bx))\n",
    "            train_losses.append(running_loss / len(train_data))\n",
    "\n",
    "            # Start evaluation\n",
    "            model.eval()\n",
    "            dev_loss = 0\n",
    "            dev_y = []\n",
    "            dev_y_pred = []\n",
    "            with torch.no_grad():\n",
    "                for bx, by in dev_loader:\n",
    "                    output = model.forward(bx)\n",
    "                    loss = F.cross_entropy(output, by)\n",
    "                    dev_loss += loss.item() * len(bx)\n",
    "                    dev_y.append(by)\n",
    "                    dev_y_pred.append(torch.argmax(output, axis=1))\n",
    "            dev_losses.append(dev_loss / len(dev_data))\n",
    "            dev_y = torch.hstack(dev_y)\n",
    "            dev_y_pred = torch.hstack(dev_y_pred)\n",
    "            dev_acc = accuracy(dev_y_pred, dev_y)\n",
    "            dev_accuracies.append(dev_acc)\n",
    "\n",
    "        print(f'dev loss={dev_loss / len(dev_data):.4f} dev acc={dev_acc:.4f}')\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(dev_losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average loss')\n",
    "    plt.subplot(122)\n",
    "    plt.plot(dev_accuracies)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Development set accuracy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What accuracy do you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🤔 Exploration 1: Architecture tuning**\n",
    "\n",
    "> Try to tune the architecture of your classifier to see which one gives the highest accuracy on the development data. For example, you could try to stack several LSTM layers on top of each other.\n",
    "\n",
    "**🤔 Exploration 2: Initializing embeddings**\n",
    "\n",
    "> One of the recent advances of deep learning research is the discovery of new techniques for initializing the parameters of neural networks. For example, [Glorot and Bengio (2010)](https://proceedings.mlr.press/v9/glorot10a.html) propose a method for choosing the random distribution from which the parameters are drawn based on the architecture of the network. This method is implemented in the PyTorch functions [`nn.init.xavier_uniform_`](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_) and [`nn.init.xavier_normal_`](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_normal_). Test this method by calling it on `model.embedding.weight`. What effect does this have on accuracy?\n",
    "\n",
    "**🤔 Exploration 3: Pre-trained embeddings**\n",
    "\n",
    "> Does it help your classifier to use pre-trained word embeddings instead of task-specific embeddings? For example, try to load pre-trained embeddings obtained from the [GloVe project](https://nlp.stanford.edu/projects/glove/). Is it better to ‘freeze’ the embeddings or train them along with the rest of the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s all folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
