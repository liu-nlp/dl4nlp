<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>DL4NLP - Assignment 1: Language modelling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">DL4NLP</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-modules" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Modules</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-modules">    
        <li>
    <a class="dropdown-item" href="../../modules/module0/index.html">
 <span class="dropdown-text">Review</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../modules/module1/index.html">
 <span class="dropdown-text">Module 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../modules/module2/index.html">
 <span class="dropdown-text">Module 2</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../modules/module3/index.html">
 <span class="dropdown-text">Module 3</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-assignments" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Assignments</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-assignments">    
        <li>
    <a class="dropdown-item" href="../../assignments/assignment1/index.html">
 <span class="dropdown-text">Assignment 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../assignments/assignment2/index.html">
 <span class="dropdown-text">Assignment 2</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../project/index.html"> 
<span class="menu-text">Project</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
    <a href="https://github.com/liu-nlp/dl4nlp" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#data" id="toc-data" class="nav-link active" data-scroll-target="#data">Data</a></li>
  <li><a href="#problem-1-fixed-window-model" id="toc-problem-1-fixed-window-model" class="nav-link" data-scroll-target="#problem-1-fixed-window-model">Problem 1: Fixed-window model</a>
  <ul class="collapse">
  <li><a href="#problem-1.1-vectorise-the-data" id="toc-problem-1.1-vectorise-the-data" class="nav-link" data-scroll-target="#problem-1.1-vectorise-the-data">Problem 1.1: Vectorise the data</a></li>
  <li><a href="#problem-1.2-implement-the-model" id="toc-problem-1.2-implement-the-model" class="nav-link" data-scroll-target="#problem-1.2-implement-the-model">Problem 1.2: Implement the model</a></li>
  <li><a href="#problem-1.3-train-the-model" id="toc-problem-1.3-train-the-model" class="nav-link" data-scroll-target="#problem-1.3-train-the-model">Problem 1.3: Train the model</a></li>
  </ul></li>
  <li><a href="#problem-2-recurrent-neural-network-model" id="toc-problem-2-recurrent-neural-network-model" class="nav-link" data-scroll-target="#problem-2-recurrent-neural-network-model">Problem 2: Recurrent neural network model</a>
  <ul class="collapse">
  <li><a href="#problem-2.1-vectorise-the-data" id="toc-problem-2.1-vectorise-the-data" class="nav-link" data-scroll-target="#problem-2.1-vectorise-the-data">Problem 2.1: Vectorise the data</a></li>
  <li><a href="#problem-2.2-implement-the-model" id="toc-problem-2.2-implement-the-model" class="nav-link" data-scroll-target="#problem-2.2-implement-the-model">Problem 2.2: Implement the model</a></li>
  <li><a href="#problem-2.3-train-the-model" id="toc-problem-2.3-train-the-model" class="nav-link" data-scroll-target="#problem-2.3-train-the-model">Problem 2.3: Train the model</a></li>
  </ul></li>
  <li><a href="#problem-3-transformer-model-optional" id="toc-problem-3-transformer-model-optional" class="nav-link" data-scroll-target="#problem-3-transformer-model-optional">Problem 3: Transformer model (optional)</a></li>
  <li><a href="#problem-4-generation" id="toc-problem-4-generation" class="nav-link" data-scroll-target="#problem-4-generation">Problem 4: Generation</a></li>
  <li><a href="#problem-5-parameter-initialisation" id="toc-problem-5-parameter-initialisation" class="nav-link" data-scroll-target="#problem-5-parameter-initialisation">Problem 5: Parameter initialisation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Assignment 1: Language modelling</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In this assignment you will implement and train two or three neural language models: the fixed-window model, the recurrent neural network model from Unit&nbsp;1-2, and optionally a model based on the Transformer architecture from Unit&nbsp;1-3. You will evaluate these models by computing their perplexity on a benchmark dataset.</p>
<div id="cell-3" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For this lab, you should use the GPU if you have one:</p>
<div id="cell-5" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    # NVIDIA</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># device = torch.device('mps')    # Apple Silicon</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<p>The data for this assignment is <a href="https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/">WikiText</a>, a collection of more than 100 million tokens extracted from the “Good” and “Featured” articles on Wikipedia. We will use the small version of the dataset, which contains slightly more than 2.5 million tokens.</p>
<p>The next cell contains code for an object that will act as a container for the “training” and the “validation” section of the data. We fill this container by reading the corresponding text files. The only processing we do is to whitespace-tokenise and to replace each newline with an end-of-sentence token.</p>
<div id="cell-8" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> WikiText(<span class="bu">object</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.word2idx <span class="op">=</span> {}</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.idx2word <span class="op">=</span> []</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.train <span class="op">=</span> <span class="va">self</span>.read_data(<span class="st">'wiki.train.tokens'</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.valid <span class="op">=</span> <span class="va">self</span>.read_data(<span class="st">'wiki.valid.tokens'</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> read_data(<span class="va">self</span>, path):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        ids <span class="op">=</span> []</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(path, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> source:</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> line <span class="kw">in</span> source:</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> word <span class="kw">in</span> line.split() <span class="op">+</span> [<span class="st">'&lt;eos&gt;'</span>]:</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.word2idx:</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.word2idx[word] <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.word2idx)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>                        <span class="va">self</span>.idx2word.append(word)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>                    ids.append(<span class="va">self</span>.word2idx[word])</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ids</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The cell below loads the data and prints the total number of tokens and the size of the vocabulary.</p>
<div id="cell-10" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>wikitext <span class="op">=</span> WikiText()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Tokens in train:'</span>, <span class="bu">len</span>(wikitext.train))</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Tokens in valid:'</span>, <span class="bu">len</span>(wikitext.valid))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Vocabulary size:'</span>, <span class="bu">len</span>(wikitext.word2idx))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="problem-1-fixed-window-model" class="level2">
<h2 class="anchored" data-anchor-id="problem-1-fixed-window-model">Problem 1: Fixed-window model</h2>
<p>In this section, you will implement and train the fixed-window neural language model proposed by <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">Bengio et al.&nbsp;(2003)</a> and presented in the lectures. Recall that an input to the network takes the form of a vector of <span class="math inline">\(n-1\)</span> integers representing the preceding words. Each integer is mapped to a vector via an embedding layer. (All positions share the same embedding.) The embedding vectors are then concatenated and sent through a two-layer feed-forward network with a non-linearity in the form of a rectified linear unit (ReLU) and a final softmax layer.</p>
<section id="problem-1.1-vectorise-the-data" class="level3">
<h3 class="anchored" data-anchor-id="problem-1.1-vectorise-the-data">Problem 1.1: Vectorise the data</h3>
<p>Your first task is to write code for transforming the data in the WikiText container into a vectorised form that can be fed to the fixed-window model. Concretely, you will implement a <a href="https://pytorch.org/docs/stable/data.html#dataloader-collate-fn">collate function</a> in the form of a callable vectoriser object. Complete the skeleton code in the cell below:</p>
<div id="cell-15" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FixedWindowVectorizer(<span class="bu">object</span>):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n <span class="op">=</span> n</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, data):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: Replace the following line with your own code</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span>, <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Your code should implement the following specification:</p>
<p><strong><strong>init</strong></strong> (<em>self</em>, <em>n</em>)</p>
<blockquote class="blockquote">
<p>Creates a new vectoriser with n-gram order <span class="math inline">\(n\)</span>. Your code should be able to handle arbitrary n-gram orders <span class="math inline">\(n \geq 1\)</span>.</p>
</blockquote>
<p><strong><strong>call</strong></strong> (<em>self</em>, <em>data</em>)</p>
<blockquote class="blockquote">
<p>Transforms WikiText <em>data</em> (a list of word ids) into a pair of tensors <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathbf{y}\)</span> that can be used to train the fixed-window model. Let <span class="math inline">\(N\)</span> be the total number of <span class="math inline">\(n\)</span>-grams from the token list; then <span class="math inline">\(\mathbf{X}\)</span> is a matrix with shape <span class="math inline">\((N, n-1)\)</span> and <span class="math inline">\(\mathbf{y}\)</span> is a vector with length <span class="math inline">\(N\)</span>.</p>
</blockquote>
<section id="test-your-code" class="level4">
<h4 class="anchored" data-anchor-id="test-your-code">🤞 Test your code</h4>
<p>Test your implementation by running the code in the next cell. Does the output match your expectation?</p>
<div id="cell-18" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>valid_x, valid_y <span class="op">=</span> FixedWindowVectorizer(<span class="dv">3</span>)(wikitext.valid)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(valid_x.size(), valid_y.size())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="problem-1.2-implement-the-model" class="level3">
<h3 class="anchored" data-anchor-id="problem-1.2-implement-the-model">Problem 1.2: Implement the model</h3>
<p>Your next task is to implement the fixed-window model based on the graphical specification given in the lecture.</p>
<div id="cell-21" class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FixedWindowModel(nn.Module):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n, n_words, embedding_dim<span class="op">=</span><span class="dv">64</span>, hidden_dim<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: Add your own code</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: Replace the next line with your own code</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="va">NotImplemented</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here is the specification of the two methods:</p>
<p><strong><strong>init</strong></strong> (<em>self</em>, <em>n</em>, <em>n_words</em>, <em>embedding_dim</em>=64, <em>hidden_dim</em>=64)</p>
<blockquote class="blockquote">
<p>Creates a new fixed-window neural language model. The argument <em>n</em> specifies the model’s <span class="math inline">\(n\)</span>-gram order. The argument <em>n_words</em> is the number of words in the vocabulary. The arguments <em>embedding_dim</em> and <em>hidden_dim</em> specify the dimensionalities of the embedding layer and the hidden layer of the feedforward network, respectively; their default value is 64.</p>
</blockquote>
<p><strong>forward</strong> (<em>self</em>, <em>x</em>)</p>
<blockquote class="blockquote">
<p>Computes the network output on an input batch <em>x</em>. The shape of <em>x</em> is <span class="math inline">\((B, n-1)\)</span>, where <span class="math inline">\(B\)</span> is the batch size. The output of the forward pass is a tensor of shape <span class="math inline">\((B, V)\)</span> where <span class="math inline">\(V\)</span> is the number of words in the vocabulary.</p>
</blockquote>
<section id="test-your-code-1" class="level4">
<h4 class="anchored" data-anchor-id="test-your-code-1">🤞 Test your code</h4>
<p>Test your code by instantiating the model and feeding it a batch of examples from the training data.</p>
</section>
</section>
<section id="problem-1.3-train-the-model" class="level3">
<h3 class="anchored" data-anchor-id="problem-1.3-train-the-model">Problem 1.3: Train the model</h3>
<p>Next, write code to train the fixed-window model using minibatch gradient descent and the cross-entropy loss function. This should be a straightforward generalisation of the training loops that you have seen so far. Complete the skeleton code in the cell below:</p>
<div id="cell-25" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_fixed_window(n, n_epochs<span class="op">=</span><span class="dv">2</span>, batch_size<span class="op">=</span><span class="dv">3072</span>, lr<span class="op">=</span><span class="fl">1e-2</span>):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Replace the following line with your own code</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here is the specification of the training function:</p>
<p><strong>train_fixed_window</strong> (<em>n</em>, <em>n_epochs</em> = 2, <em>batch_size</em> = 3072, <em>lr</em> = 0.01)</p>
<blockquote class="blockquote">
<p>Trains a fixed-window neural language model of order <em>n</em> using minibatch gradient descent and returns it. The parameters <em>n_epochs</em> and <em>batch_size</em> specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the <a href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam">Adam optimizer</a> with learning rate <em>lr</em>. After each epoch, prints the perplexity of the model on the validation data.</p>
</blockquote>
<p>The code in the cell below trains a trigram model.</p>
<div id="cell-28" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>model_fixed_window <span class="op">=</span> train_fixed_window(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="performance-goal" class="level4">
<h4 class="anchored" data-anchor-id="performance-goal">Performance goal</h4>
<p>Your submitted notebook must contain output demonstrating a validation perplexity of <strong>at most 360</strong> after training for two epochs with the default parameters.</p>
<p>⚠️ Computing the validation perplexity in one go (for the full validation set) will most probably exhaust your computer’s memory and/or take a lot of time. Instead, do the computation at the minibatch level and aggregate the results.</p>
</section>
<section id="test-your-code-2" class="level4">
<h4 class="anchored" data-anchor-id="test-your-code-2">🤞 Test your code</h4>
<p>To see whether your network is learning something, print or plot the loss and/or the perplexity on the training data. If the two values do not decrease during training, try to find the problem before wasting time (and electricity) on useless computation.</p>
<p>Training and even evaluation will take some time – on a CPU, you should expect several minutes per epoch, depending on hardware. Our reference implementation uses a GPU and runs in 45&nbsp;seconds on a MacBook Pro (2023).</p>
</section>
</section>
</section>
<section id="problem-2-recurrent-neural-network-model" class="level2">
<h2 class="anchored" data-anchor-id="problem-2-recurrent-neural-network-model">Problem 2: Recurrent neural network model</h2>
<p>In this section, you will implement the recurrent neural network language model. Recall that an input to this model is a vector of word ids. Each integer is mapped to an embedding vector. The sequence of embedded vectors is then fed into an unrolled LSTM. At each position <span class="math inline">\(i\)</span> in the sequence, the hidden state of the LSTM at that position is sent through a linear transformation into a final softmax layer representing the probability distribution over the words at position <span class="math inline">\(i+1\)</span>. In theory, the input vector could represent the complete training data; for practical reasons, however, we will truncate the input to some fixed value <em>bptt_len</em>. This length is called the <strong>backpropagation-through-time horizon</strong>.</p>
<section id="problem-2.1-vectorise-the-data" class="level3">
<h3 class="anchored" data-anchor-id="problem-2.1-vectorise-the-data">Problem 2.1: Vectorise the data</h3>
<p>As in the previous problem, your first task is to transform the data in the WikiText container into a vectorised form that can be fed to the model.</p>
<div id="cell-35" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RNNVectorizer(<span class="bu">object</span>):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, bptt_len):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bptt_len <span class="op">=</span> bptt_len</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, data):</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: Replace the following line with your own code</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span>, <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Your vectoriser should meet the following specification:</p>
<p><strong><strong>init</strong></strong> (<em>self</em>, <em>bptt_len</em>)</p>
<blockquote class="blockquote">
<p>Creates a new vectoriser. The parameter <em>bptt_len</em> specifies the backpropagation-through-time horizon.</p>
</blockquote>
<p><strong><strong>call</strong></strong> (<em>self</em>, <em>data</em>)</p>
<blockquote class="blockquote">
<p>Transforms a list of token indexes <em>data</em> into a pair of tensors <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathbf{Y}\)</span> that can be used to train the recurrent neural language model. The rows of both tensors represent contiguous subsequences of token indexes of length <em>bptt_len</em>. Compared to the sequences in <span class="math inline">\(\mathbf{X}\)</span>, the corresponding sequences in <span class="math inline">\(\mathbf{Y}\)</span> are shifted one position to the right. More precisely, if the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{X}\)</span> is the sequence that starts at token position <span class="math inline">\(j\)</span>, then the same row of <span class="math inline">\(\mathbf{Y}\)</span> is the sequence that starts at position <span class="math inline">\(j+1\)</span>.</p>
</blockquote>
<section id="test-your-code-3" class="level4">
<h4 class="anchored" data-anchor-id="test-your-code-3">🤞 Test your code</h4>
<p>Test your implementation by running the following code:</p>
<div id="cell-38" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>valid_x, valid_y <span class="op">=</span> RNNVectorizer(<span class="dv">32</span>)(wikitext.valid)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(valid_x.size(), valid_y.size())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="problem-2.2-implement-the-model" class="level3">
<h3 class="anchored" data-anchor-id="problem-2.2-implement-the-model">Problem 2.2: Implement the model</h3>
<p>Your next task is to implement the recurrent neural network model based on the graphical specification.</p>
<div id="cell-41" class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RNNModel(nn.Module):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_words, embedding_dim<span class="op">=</span><span class="dv">64</span>, hidden_dim<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: Add your own code</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">TODO</span><span class="co">: Replace the next line with your own code</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="va">NotImplemented</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Your implementation should follow this specification:</p>
<p><strong><strong>init</strong></strong> (<em>self</em>, <em>n_words</em>, <em>embedding_dim</em> = 64, <em>hidden_dim</em> = 64)</p>
<blockquote class="blockquote">
<p>Creates a new recurrent neural network language model based on an LSTM. The argument <em>n_words</em> is the number of words in the vocabulary. The arguments <em>embedding_dim</em> and <em>hidden_dim</em> specify the dimensionalities of the embedding layer and the LSTM hidden layer, respectively; their default value is 64.</p>
</blockquote>
<p><strong>forward</strong> (<em>self</em>, <em>x</em>)</p>
<blockquote class="blockquote">
<p>Computes the network output on an input batch <em>x</em>. The shape of <em>x</em> is <span class="math inline">\((B, H)\)</span>, where <span class="math inline">\(B\)</span> is the batch size and <span class="math inline">\(H\)</span> is the length of each input sequence. The shape of the output tensor is <span class="math inline">\((B, H, V)\)</span>, where <span class="math inline">\(V\)</span> is the size of the vocabulary.</p>
</blockquote>
<section id="test-your-code-4" class="level4">
<h4 class="anchored" data-anchor-id="test-your-code-4">🤞 Test your code</h4>
<p>Test your code by instantiating the model and feeding it a batch of examples from the training data.</p>
</section>
</section>
<section id="problem-2.3-train-the-model" class="level3">
<h3 class="anchored" data-anchor-id="problem-2.3-train-the-model">Problem 2.3: Train the model</h3>
<p>The training loop for the recurrent neural network model is essentially identical to the loop that you wrote for the feed-forward model. The only thing to note is that the cross-entropy loss function expects its input to be a two-dimensional tensor; you will therefore have to re-shape the output tensor from the LSTM as well as the gold-standard output tensor in a suitable way. The most efficient way to do so is to use the <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view"><code>view()</code></a> method.</p>
<div id="cell-46" class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_rnn(n_epochs<span class="op">=</span><span class="dv">2</span>, batch_size<span class="op">=</span><span class="dv">3072</span>, bptt_len<span class="op">=</span><span class="dv">32</span>, lr<span class="op">=</span><span class="fl">1e-2</span>):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Replace the next line with your own code</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here is the specification of the training function:</p>
<p><strong>train_rnn</strong> (<em>n_epochs</em> = 2, <em>batch_size</em> = 3072, <em>bptt_len</em> = 32, <em>lr</em> = 0.01)</p>
<blockquote class="blockquote">
<p>Trains a recurrent neural network language model on the WikiText data using minibatch gradient descent and returns it. The parameters <em>n_epochs</em> and <em>batch_size</em> specify the number of training epochs and the minibatch size, respectively. The parameter <em>bptt_len</em> specifies the length of the backpropagation-through-time horizon, that is, the length of the input and output sequences. Training uses the cross-entropy loss function and the <a href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam">Adam optimizer</a> with learning rate <em>lr</em>. After each epoch, prints the perplexity of the model on the validation data.</p>
</blockquote>
<p>Evaluate your model by running the following code cell:</p>
<div id="cell-49" class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>model_rnn <span class="op">=</span> train_rnn()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="performance-goal-1" class="level4">
<h4 class="anchored" data-anchor-id="performance-goal-1">Performance goal</h4>
<p>Your submitted notebook must contain output demonstrating a validation perplexity of <strong>at most 280</strong> after training for two epochs with the default hyperparameters.</p>
</section>
</section>
</section>
<section id="problem-3-transformer-model-optional" class="level2">
<h2 class="anchored" data-anchor-id="problem-3-transformer-model-optional">Problem 3: Transformer model (optional)</h2>
<p>If you are up for a challenge, try implementing a Transformer-based language model. The required vectoriser is identical to the vectoriser for the RNN model. For the model itself, you can use the Pytorch modules <a href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html"><code>nn.TransformerEncoder</code></a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html"><code>nn.TransformerEncoderLayer</code></a>. To represent positional information, follow the approach from the original Transformer paper and use sine and cosine functions of different frequencies (<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding">details</a>), or learn position-specific embeddings. Can you get a lower perplexity than for the RNN model?</p>
</section>
<section id="problem-4-generation" class="level2">
<h2 class="anchored" data-anchor-id="problem-4-generation">Problem 4: Generation</h2>
<p>In this section, you will implement a simple generation mechanism for the language models you have implemented.</p>
<p>Recall that one way to generate text with a language model is to repeatedly sample from the model’s output distribution, conditioning on some context. More specifically, this involves treating the softmax-normalised logits of the model as a multinomial distribution. The “creativeness” of the generation can be controlled with the temperature parameter of the softmax distribution.</p>
<p>To implement this recipe, we first ask you to extend each model with a <code>generate</code> method according to the following specification:</p>
<p><strong>generate</strong> (<em>self</em>, <em>context</em>, <em>n_tokens</em> = 10, <em>temperature</em> = 1.0)</p>
<blockquote class="blockquote">
<p>Takes a batch of context tokens <em>context</em> and extends it by sampling <em>n_tokens</em> new tokens from the model’s output distribution, scaled with the temperature <em>temperature</em>. Returns the extended context.</p>
</blockquote>
<p>In a second stage, you should implement a convenience function <code>generate</code> that allows you to easily generate text with different models, like this:</p>
<pre><code>generate(model_fixed_window, 'i like', max_tokens=10, temperature=1.5)</code></pre>
<div id="cell-55" class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate(model, context, max_tokens<span class="op">=</span><span class="dv">3</span>, temperature<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co">: Replace the next line with your own code</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> context</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here is the specification of the convenience function:</p>
<p><strong>generate</strong> (<em>model</em>, <em>context</em>, <em>max_tokens</em> = 10, <em>temperature</em> = 1.0)</p>
<blockquote class="blockquote">
<p>Takes a context sentence <em>context</em>, tokenises and vectorises it, and passes it to the specified <em>model</em> to generate new text. The new text consists of at most <em>max_tokens</em>, but is cut off at the first <code>&lt;eos&gt;</code> token. Returns the generated text (including the context).</p>
</blockquote>
</section>
<section id="problem-5-parameter-initialisation" class="level2">
<h2 class="anchored" data-anchor-id="problem-5-parameter-initialisation">Problem 5: Parameter initialisation</h2>
<p>The error surfaces explored when training neural networks can be very complex. Because of this, it is important to choose “good” initial values for the parameters. In PyTorch, the weights of the embedding layer are initialised by sampling from the standard normal distribution <span class="math inline">\(\mathcal{N}(0, 1)\)</span>. Test how changing the initialisation affects the perplexity of your language models. Find research articles that propose different initialisation strategies.</p>
<p>Write a short (150&nbsp;words) report about your experiments and literature search. Use the following prompts:</p>
<ul>
<li>What different initialisation did you try? What results did you get?</li>
<li>How do your results compare to what was suggested by the research articles?</li>
<li>What did you learn? How, exactly, did you learn it? Why does this learning matter?</li>
</ul>
<p><em>TODO: Enter your text here</em></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/liu-nlp\.github\.io\/dl4nlp\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>