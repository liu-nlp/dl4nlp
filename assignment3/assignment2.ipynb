{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will implement and evaluate a **dependency parser** and summarise your experimental findings in a short report. The main goal of the assignment is to give you insights into how to implement a non-trivial neural architecture for structured prediction.\n",
    "\n",
    "**Submission:** Please email your submission to Marco ([marco.kuhlmann@liu.se](mailto:marco.kuhlmann@liu.se))\n",
    "\n",
    "**Due date:** 2 June (extended from 25 May)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set for the lab is the English Web Treebank from the [Universal Dependencies Project](http://universaldependencies.org), a corpus containing more than 16,000 sentences (254,000&nbsp;tokens) annotated with dependency trees (among other things). The Universal Dependencies Project distributes its data in the [CoNLL-U format](https://universaldependencies.org/format.html). The code in the next cell can be used to read data in this format from a file-like object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = ('<root>', '<root>', 0)  # Pseudoword; see comment below\n",
    "\n",
    "def read_data(source):\n",
    "    buffer = [ROOT]\n",
    "    for line in source:\n",
    "        if not line.startswith('#'):  # Skip lines with comments\n",
    "            line = line.rstrip()\n",
    "            if not line:\n",
    "                yield buffer\n",
    "                buffer = [ROOT]\n",
    "            else:\n",
    "                columns = line.split('\\t')\n",
    "                if columns[0].isdigit():  # Skip range tokens\n",
    "                    buffer.append((columns[1], columns[3], int(columns[6])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we will use the training section and the development section of the [English Web Treebank](https://github.com/UniversalDependencies/UD_English-EWT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_ewt-ud-train-projectivized.conllu') as source:\n",
    "    train_data = list(read_data(source))\n",
    "\n",
    "with open('en_ewt-ud-dev.conllu') as source:\n",
    "    dev_data = list(read_data(source))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both data sets consist of syntactically analyzed sentences. In this assignment, an analyzed sentence is represented as a list of triples, where the first component of each triple represents a word form, the second component represents the word’s tag, and the third component is an integer specifying the position of the word’s syntactic head, i.e., its parent in the dependency tree. Run the following code cell to see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the head of the word *I* is the word at position&nbsp;2, *ran*; the dependents of *ran* are *I* (position&nbsp;1), *item* (position&nbsp;5), *Internet* (position&nbsp;8), as well as the final punctuation mark. Note that each sentence is preceded by the pseudoword `<root>` (position&nbsp;0); this turns out to be useful for technical reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement a dependency parser, train this parser on the training data, and evaluate its performance on the development data. We provide skeleton code for the implementation of a simple transition-based dependency parser (&lsquo;baseline parser&rsquo;) in the vein of [Chen and Manning (2014)](https://www.aclweb.org/anthology/D14-1082/); however, you are free to implement any architecture that you would like to try. For example, you could try to implement the arc-hybrid system and dynamic oracle described in [Goldberg and Nivre (2013)](https://www.aclweb.org/anthology/Q13-1033/), or one of the architectures described in [Kiperwasser and Goldberg (2016)](https://www.aclweb.org/anthology/Q16-1023/).\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "Submit a Jupyter notebook with your code as well as a brief report that describes your implementation and your experimental results. It is required that the notebook can be run easily, maybe just with some trivial modification to point to the location of the data.\n",
    "\n",
    "### Grading\n",
    "\n",
    "To pass the assignment, you must submit a complete and working implementation of a dependency parser and a well-written report. Your parser must reach an unlabelled attachment score (UAS) on the development data of at least 70%; see the end of this notebook for details. If you choose to implement other methods, you should be able to obtain UAS scores that are significantly higher than 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will walk you through the implementation of a simple transition-based parser in the style of [Chen and Manning (2014)](https://www.aclweb.org/anthology/D14-1082/). This &lsquo;baseline&rsquo; parser is based on the arc-standard algorithm that was presented in class.\n",
    "\n",
    "We present the skeleton code for the baseline parser in six steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Encoding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be convenient to represent the strings in the data by indexes as integers. Your first task then is to implement a function that constructs these vocabularies, and a second function that encodes the data into integer form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocabs(gold_sentences):\n",
    "    raise NotImplementedError\n",
    "\n",
    "def encode(vocab_words, vocab_tags, gold_sentences):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should confirm to the following specification:\n",
    "\n",
    "**make_vocabs** (*gold_sentences*)\n",
    "\n",
    "> Returns a pair of two vocabularies, represented as dictionaries: a *word vocabulary* and a *tag vocabulary*. The word vocabulary maps the unique words in the *gold_sentences* to a contiguous range of integers between $0$ and $W+1$, where $W$ is the total number of unique words. Similarly, the tag vocabulary maps the unique part-of-speech tags in the *gold_sentences* to a range between $0$ and $T$, where $T$ is the total number of unique tags. The special words `<pad>` (used later for padding undefined values) and `<unk>` (used in place of unknown words at prediction time) are mapped to the indexes&nbsp;0 and&nbsp;1, respectively. The special tag `<pad>` is mapped to the index&nbsp;0.\n",
    "\n",
    "**encode** (*vocab_words*, *vocab_tags*, *gold_sentences*)\n",
    "\n",
    "> Returns an encoded version of the *gold_sentences* where each word form is replaced by its index in the word vocabulary (*vocab_words*) and where each tag is replaced by its index in the tag vocabulary (*vocab_tags*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, whenever we say &lsquo;words&rsquo; and &lsquo;tags&rsquo; we really mean the integer versions of words and tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Parser, static part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline parser consists of two parts: a static part that implements the logic of the arc-standard transition system, and a non-static part that contains the learning component. In this section we cover the static part; the non-static part is covered in the next section.\n",
    "\n",
    "In the arc-standard algorithm, the next move (transition) of the parser is predicted based on features extracted from the current parser configuration, with references to the words and part-of-speech tags of the input sentence. On the Python side of things, we represent parser configurations as triples\n",
    "\n",
    "$$\n",
    "(i, \\mathit{stack}, \\mathit{heads})\n",
    "$$\n",
    "\n",
    "where $i$ is an integer specifying the position of the next word in the buffer, $\\mathit{stack}$ is a list of integers specifying the positions of the words currently on the stack (with the topmost element last in the list), and $\\mathit{heads}$ is a list of integers specifying the positions of the currently assigned head words. To illustrate this representation, the initial configuration for the sample sentence above is"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(0, [], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a possible final configuration is"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(10, [0], [0, 2, 0, 5, 5, 2, 8, 8, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains skeleton code for the static part of the baseline parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "\n",
    "    # Parser moves are specified as integers.\n",
    "\n",
    "    MOVES = tuple(range(3))\n",
    "\n",
    "    SH, LA, RA = MOVES\n",
    "\n",
    "    @staticmethod\n",
    "    def initial_config(num_words):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_moves(config):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def next_config(config, move):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def is_final_config(config):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement this interface according the following specification:\n",
    "\n",
    "**initial_config** (*num_words*)\n",
    "\n",
    "> Returns the initial configuration for a sentence with the specified number of words.\n",
    "\n",
    "**valid_moves** (*config*)\n",
    "\n",
    "> Returns the list of valid moves for the specified configuration. Note that moves are represented as integers.\n",
    "\n",
    "**next_config** (*config*, *move*)\n",
    "\n",
    "> Applies the specified move (an integer) to the specified configuration and returns the new configuration.\n",
    "\n",
    "**is_final_config** (*config*)\n",
    "\n",
    "> Tests whether the specified configuration is a final configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your implementation, you can run the code below. This code creates the initial configuration for the example sentence, simulates a sequence of moves, and then checks that the resulting configuration is the expected final configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = [0, 0, 0, 1, 0, 0, 0, 1, 1, 2, 0, 0, 0, 1, 1, 2, 0, 2, 2]\n",
    "\n",
    "parser = Parser()\n",
    "config = parser.initial_config(len(dev_data[100]))\n",
    "for move in moves:\n",
    "    assert move in parser.valid_moves(config)\n",
    "    config = parser.next_config(config, move)\n",
    "assert parser.is_final_config(config)\n",
    "\n",
    "assert config == (10, [0], [0, 2, 0, 5, 5, 2, 8, 8, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Parser, non-static part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heart of the non-static part of the baseline parser is the *next move classifier*, which is implemented by a feedforward network. The input to this network is a vector of integers representing words and tags, as described in the article by [Chen and Manning (2014)](https://www.aclweb.org/anthology/D14-1082/). For example, a simple feature model would look at the next word in the buffer and the topmost two words on the stack. The network processes this input as follows:\n",
    "\n",
    "1. embed the words and tags and concatenate the resulting embeddings\n",
    "2. send the concatenated embeddings through a linear layer followed by a ReLU\n",
    "3. pass the output of the non-linearity into a final softmax layer\n",
    "\n",
    "The next cell contains skeleton code for the non-static part of the parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineParser(Parser):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def featurize(self, words, tags, config):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, words, tags):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should comply with the following specification:\n",
    "\n",
    "**__init__** (*self*, *vocab_words*, *vocab_tags*)\n",
    "\n",
    "> Creates a new parser, including the neural network that implements the next move classifier. The arguments *vocab_words* and *vocab_tags* are the dictionaries that you created in Step&nbsp;1.\n",
    "\n",
    "**featurize** (*self*, *words*, *tags*, *config*)\n",
    "\n",
    "> Returns the input vector to the next move classifier for the specified parser configuration. If you implement the simple feature model described above, this will be a vector of length&nbsp;6. The *words* and *tags* are the words and part-of-speech tags for the current input sentence, and *config* is a parser configuration as in Step&nbsp;2.\n",
    "\n",
    "**predict** (*self*, *words*, *tags*)\n",
    "\n",
    "> Predicts the list of all heads for the specified input sentence. The input sentence is specified in terms of the list of its *words* and the list of its *tags*. This method runs the arc-standard parsing algorithm, at each step asking the next move classifier for the next transition to take.\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "The following choices are reasonable defaults for the hyperparameters of the network architecture used by the parser:\n",
    "\n",
    "* width of the word embedding: 50\n",
    "* width of the tag embedding: 10\n",
    "* size of the hidden layer: 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Creating the training data for the next move classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the next move classifier, we need training samples of the form $(\\mathbf{x}, m)$, where $\\mathbf{x}$ is a feature vector extracted from a given parser configuration&nbsp;$c$, and $m$ is the corresponding gold-standard move. To construct this dataset, we need an **oracle**. As you have learned in class, there are different ways to implement the oracle; here we ask you to implement the static oracle, which generates training data using teacher forcing.\n",
    "\n",
    "The following cell contains skeleton code for a function that implements the oracle, and for a class `NextMoveDataset` that holds the training samples of the next move classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def oracle_moves(gold_heads):\n",
    "    raise NotImplementedError\n",
    "\n",
    "class NextMoveDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, gold_sentences, parser):\n",
    "        self.xs = []\n",
    "        self.ys = []\n",
    "        # TODO: Insert code here\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.xs[idx], self.ys[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should conform to the following specification:\n",
    "\n",
    "**oracle_moves** (*gold_heads*)\n",
    "\n",
    "> Translates a gold-standard head assignment for a sentence-specific head-assignment (`gold_heads`) into the corresponding stream of oracle moves. More specifically, this yields pairs $(c, m)$ where $m$ is a move and $c$ is the configuration in which $m$ was taken.\n",
    "\n",
    "**NextMoveDataset** (*gold_sentences*, *parser*)\n",
    "\n",
    "> Constructs a PyTorch dataset for the next move classifier. The dataset is constructed by calling *oracle_moves* on each gold-standard sentence in *gold_sentences*, and applying the `featurize` function to the resulting configurations to obtain feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test your oracle by executing the cell below. This extracts the oracle move sequence from the example sentence and compares it to the gold-standard move sequence `gold_moves`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_heads = [h for w, t, h in dev_data[100]]\n",
    "gold_moves = [0, 0, 0, 1, 0, 0, 0, 1, 1, 2, 0, 0, 0, 1, 1, 2, 0, 2, 2]\n",
    "\n",
    "assert list(m for _, m in oracle_moves(gold_heads)) == gold_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last piece of the implementation of the baseline parser is the training loop. This should hold no surprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, n_epochs=1, batch_size=100):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation, we use **unlabelled attachment score (UAS)**, which is defined as the percentage of all tokens to which the parser assigns the correct head (as per the gold standard). Note that the calculation excludes the pseudoword at position&nbsp;0 in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uas(parser, gold_sentences):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for sentence in gold_sentences:\n",
    "        words, tags, gold_heads = zip(*sentence)\n",
    "        pred_heads = parser.predict(words, tags)\n",
    "        for gold, pred in zip(gold_heads[1:], pred_heads[1:]):\n",
    "            total += 1\n",
    "            correct += int(gold == pred)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to test your parser. Note that, during development, you may want to restrict the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = train(train_data, n_epochs=3)\n",
    "print('{:.4f}'.format(uas(parser, encode(parser.vocab_words, parser.vocab_tags, dev_data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training for 3&nbsp;epochs with the default parameters, you should achieve an UAS of around 71%.\n",
    "\n",
    "In case you want to play around with the baseline parser, here are some extensions that you can experiment with to increase performance:\n",
    "\n",
    "* Increase the dimensions of the embeddings.\n",
    "* Experiment with different random distributions when initialising embeddings.\n",
    "* Use pre-trained word embeddings instead of random initialisation.\n",
    "* Add dropout or other forms of regularisation.\n",
    "* Invest time into preprocessing, such as normalisation of numbers and URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That&rsquo;s all, folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
