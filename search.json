[
  {
    "objectID": "modules/module3/index.html",
    "href": "modules/module3/index.html",
    "title": "Website",
    "section": "",
    "text": "This is a Quarto website. Test.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "modules/module1/index.html",
    "href": "modules/module1/index.html",
    "title": "Module 1",
    "section": "",
    "text": "This module introduces one of the most fundamental ideas in modern NLP: the idea that words can be represented as vectors which can be learned from text data. On the application side of things, we focus on one of the most fundamental NLP tasks: text categorization.\nBefore working with the material in this module, you may want to have a look at the following:"
  },
  {
    "objectID": "modules/module1/index.html#unit-1-1-introduction-to-representations-of-words-and-documents",
    "href": "modules/module1/index.html#unit-1-1-introduction-to-representations-of-words-and-documents",
    "title": "Module 1",
    "section": "Unit 1-1: Introduction to representations of words and documents",
    "text": "Unit 1-1: Introduction to representations of words and documents\nIn the first unit, we introduce the idea of word representations: the basic building block that we use to build deep learning models that process language. Specifically, we look at word embeddings and how they can be trained. We also discuss some challenges in representing words that can be solved by working with lower-level subword units.\n\n\n\nTitle\nSlides\nVideo\n\n\n\n\nIntroduction to word representations\n[slides]\n[video]\n\n\nLearning word embeddings with neural networks\n[slides]\n[video]\n\n\nSubword models\n[slides]\n[video]"
  },
  {
    "objectID": "modules/module1/index.html#unit-1-2-language-modelling-and-contextualized-embeddings",
    "href": "modules/module1/index.html#unit-1-2-language-modelling-and-contextualized-embeddings",
    "title": "Module 1",
    "section": "Unit 1-2: Language modelling and contextualized embeddings",
    "text": "Unit 1-2: Language modelling and contextualized embeddings\nThis unit begins with an overview of language modelling. It highlights the historical significance of n-gram models in NLP, which laid the foundation for the transition to neural language models. We continue with an exploration of pre-Transformer neural architectures, specifically focusing on recurrent neural networks (RNNs) and the pivotal Long Short-Term Memory (LSTM) architecture. At the end of the unit, we explore the use of RNNs as language models and introduce the concept of contextualised embeddings, which recognize the varying meanings of words across different contexts.\n\nLecture videos\n\n\n\nTitle\nSlides\nVideo\n\n\n\n\nIntroduction to language modelling\n[slides]\n[video] (14:49)\n\n\nN-gram language models\n[slides]\n[video] (14:46)\n\n\nNeural language models\n[slides]\n[video] (14:49)\n\n\nRecurrent neural networks\n[slides]\n[video] (14:47)\n\n\nThe LSTM architecture\n[slides]\n[video] (14:45)\n\n\nRNN language models\n[slides]\n[video] (14:32)\n\n\nContextualized word embeddings\n[slides]\n[video] (14:48)"
  },
  {
    "objectID": "modules/module1/index.html#unit-1-3-transformer-based-language-models",
    "href": "modules/module1/index.html#unit-1-3-transformer-based-language-models",
    "title": "Module 1",
    "section": "Unit 1-3: Transformer-based language models",
    "text": "Unit 1-3: Transformer-based language models\nThis unit explores the evolution of transformer-based language models, starting with the sequence-to-sequence or encoder–decoder architecture for neural machine translation. We then delve into the concept of attention, followed by the Transformer architecture as such, which sets a benchmark in machine translation and various natural language processing applications. We go through Transformer-based models, specifically the GPT family, which derives from the Transformer’s decoder side, and BERT, which utilizes the encoder side. The unit ends by discussing text generation algorithms, including beam search generation.\n\nLecture videos\n\n\n\nTitle\nSlides\nVideo\n\n\n\n\nNeural machine translation\n[slides]\n[video] (14:48)\n\n\nAttention\n[slides]\n[video] (14:49)\n\n\nThe Transformer architecture\n[slides]\n[video] (14:45)\n\n\nDecoder-based language models (GPT)\n[slides]\n[video] (14:50)\n\n\nEncoder-based language models (BERT)\n[slides]\n[video] (14:44)\n\n\nGeneration algorithms\n[slides]\n[video] (26:42)\n\n\n\n\n\nReading\n\nCheng et al. (2016)\nVaswani et al. (2017)\nSerrano and Smith (2019)\nRadford et al. (2018)\nDevlin et al. (2019)"
  },
  {
    "objectID": "modules/module0/index.html",
    "href": "modules/module0/index.html",
    "title": "Review",
    "section": "",
    "text": "This page collects review material for the course."
  },
  {
    "objectID": "modules/module0/index.html#entry-requirements",
    "href": "modules/module0/index.html#entry-requirements",
    "title": "Review",
    "section": "Entry requirements",
    "text": "Entry requirements\nAs stated in the course syllabus, we assume you have a background in mathematics corresponding to the contents of the WASP course “Mathematics and Machine Learning”.\nYou will need solid programming experience in a high-level language; the programming assignments will use Python.\nYou should be comfortable with modern deep learning techniques and frameworks, for instance as taught by the WASP course “Deep Learning and GANs”. We do not assume previous knowledge of NLP."
  },
  {
    "objectID": "modules/module0/index.html#linguistics",
    "href": "modules/module0/index.html#linguistics",
    "title": "Review",
    "section": "Linguistics",
    "text": "Linguistics\nWe do not expect you to have any background in linguistics, but will often use linguistic terminology. (No natural language processing without language!) The following video provides a compact, 30-minute introduction to some of the essentials of linguistics that you may encounter in the course.\nEssentials of linguistics"
  },
  {
    "objectID": "modules/module0/index.html#basic-text-processing",
    "href": "modules/module0/index.html#basic-text-processing",
    "title": "Review",
    "section": "Basic text processing",
    "text": "Basic text processing\nThe exercises and assignments in this course require solid programming experience. In particular, you will benefit from previous experience with text processing in Python. If you need to get up to speed on this, we have a notebook that introduces some basic concepts and techniques for processing text data.\nBasic text processing"
  },
  {
    "objectID": "modules/module0/index.html#pytorch",
    "href": "modules/module0/index.html#pytorch",
    "title": "Review",
    "section": "PyTorch",
    "text": "PyTorch\nAs our deep learning framework, we will use PyTorch. Many good introductions to PyTorch are available online. The notebook we provide here is focused on those concepts and techniques that you will encounter in the exercises and assignments.\nIntroduction to PyTorch"
  },
  {
    "objectID": "modules/module0/index.html#using-colab",
    "href": "modules/module0/index.html#using-colab",
    "title": "Review",
    "section": "Using Colab",
    "text": "Using Colab\nMost of the assignments and exercises will require you to use a GPU-based machine. If you do not have easy access to such a machine at your institution, using Google’s free Colab service may be an alternative. We have written some instructions describing the basics of how to work with Colab.\nUsing Colab"
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Project",
    "section": "",
    "text": "Independent project\nIn the project work, your task will be to\n\nfind an interesting language processing task that can be addressed with machine learning methods\ngive a short talk that introduces your idea\nfind (or annotate) an appropriate dataset for that task\nlook around for related work\nimplement software to train some machine learning model for your task\nevaluate your system\nwrite up a report about your results\n\nThe projects should normally be carried out individually. If you want to collaborate, please ask us for permission.\n\n\nPreliminary project description\nPlease submit a short text that describes what you intend to work on. For suggestions of topics, see below.\nThe text should include a project title and about 1 page giving a rough outline of how you intend to work. If you have found any related work or relevant dataset, this is useful information to include. The deadline for this preliminary description is going to be in late May, after which we will send you our comments a few days later.\n\n\nProject pitch\nIn early June, each project idea will be presented in a 5-minute talk.\nIn your talk, the most important thing is to describe the task that you will work on. You may add a couple of examples to make the task a bit more understandable to someone who has never heard about it before. In addition, if you have found a dataset, you may want to mention a few details about it.\n\n\n\n\nThe report\nThe text should be structured as a typical technical report, including an abstract, statement of problem, method description, experiments and results, and a conclusion. The deadline for submitting the report is going to be in mid-June. If you do not manage to submit by that date, there is a second chance by the end of August.\n\n\nFinding a topic\nYou can work on any project that is small and manageable enough for a short project of this kind. Ideally, try to find a topic that is relevant to your own research interests. It’s OK if you end up with preliminary results, as long as you have some tidbit to present at the end.\nIf you need help deciding on a topic, here are a few suggestions. Some of these are very easy and some more challenging, and this list is not sorted by the difficulty level.\n\nTry out a benchmark. There are some popular benchmark sets such as GLUE and the more recently released Super GLUE that are used to evaluate language understanding systems. Build a model and evaluate it on one of these benchmarks.\nCrosslingual applications. For some application, e.g. some categorization or tagging task we’ve seen in the course, investigate how well crosslingual representations such as multilingual BERT allows us to train with one language and evaluate with other languages.\nShared tasks. Every year, several NLP competitions are arranged at the SemEval conference. Participate in one of the 2020 tasks or get the data from some task from previous years. Additionally, there are shared tasks organized by CoNLL, BioNLP and other conferences, where datasets can often be accessed easily.\n\nAlternatively, if you don’t have an idea and you don’t like the suggested topics, just talk to Marco and Richard."
  },
  {
    "objectID": "assignments/assignment2/Assignment2.html",
    "href": "assignments/assignment2/Assignment2.html",
    "title": "Assignment 2: Machine translation",
    "section": "",
    "text": "In this lab, you will implement the encoder–decoder architecture of Sutskever et al. (2014), including the attention-based extension of Bahdanau et al. (2015), and evaluate this architecture on a machine translation task.\nimport torch\nTraining the models in this notebook requires significant compute power, and we strongly recommend using a GPU.\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
  },
  {
    "objectID": "assignments/assignment2/Assignment2.html#the-data",
    "href": "assignments/assignment2/Assignment2.html#the-data",
    "title": "Assignment 2: Machine translation",
    "section": "The data",
    "text": "The data\nWe will build a system that translates from German (our source language) to English (our target language). The dataset is a collection of parallel English–German sentences taken from translations of subtitles for TED talks. It was derived from the TED2013 dataset, which is available in the OPUS collection. The code cell below prints the first lines in the training data:\n\nwith open('train-de.txt') as src, open('train-en.txt') as tgt:\n    for i, src_sentence, tgt_sentence in zip(range(5), src, tgt):\n        print(f'{i}: {src_sentence.rstrip()} / {tgt_sentence.rstrip()}')\n\nAs you can see, some ‘sentences’ are actually sequences of sentences, but we will use the term sentence nevertheless. All sentences are whitespace-tokenised and lowercased. To make your life a bit easier, we have removed sentences longer than 25 words.\nThe next cell contains code that yields the sentences contained in a file as lists of strings:\n\ndef sentences(filename):\n    with open(filename) as source:\n        for line in source:\n            yield line.rstrip().split()"
  },
  {
    "objectID": "assignments/assignment2/Assignment2.html#problem-1-build-the-vocabularies",
    "href": "assignments/assignment2/Assignment2.html#problem-1-build-the-vocabularies",
    "title": "Assignment 2: Machine translation",
    "section": "Problem 1: Build the vocabularies",
    "text": "Problem 1: Build the vocabularies\nYour first task is to build the vocabularies for the data, one vocabulary for each language. Each vocabulary should contain the 10,000 most frequent words in the training data for the respective language.\n\ndef make_vocab(sentences, max_size):\n    # TODO: Replace the next line with your own code\n    raise NotImplementedError\n\nYour implementation must comply with the following specification:\nmake_vocab (sentences, max_size)\n\nReturns a dictionary that maps the most frequent words in the sentences to a contiguous range of integers starting at 0. The first four mappings in this dictionary are reserved for the pseudowords &lt;pad&gt; (padding, id 0), &lt;bos&gt; (beginning of sequence, id 1), &lt;eos&gt; (end of sequence, id 2), and &lt;unk&gt; (unknown word, id 3). The parameter max_size caps the size of the dictionary, including the pseudowords.\n\nWith this function, we can construct the vocabularies as follows:\n\nsrc_vocab = make_vocab(sentences('train-de.txt'), 10000)\ntgt_vocab = make_vocab(sentences('train-en.txt'), 10000)\n\n\n🤞 Test your code\nTo test you code, check that each vocabulary contains 10,000 words, including the pseudowords."
  },
  {
    "objectID": "assignments/assignment2/Assignment2.html#load-the-data",
    "href": "assignments/assignment2/Assignment2.html#load-the-data",
    "title": "Assignment 2: Machine translation",
    "section": "Load the data",
    "text": "Load the data\nThe next cell defines a class for the parallel dataset. We sub-class the abstract Dataset class, which represents map-style datasets in PyTorch. This will let us use standard infrastructure related to the loading and automatic batching of data.\n\nfrom torch.utils.data import Dataset\n\nclass TranslationDataset(Dataset):\n\n    def __init__(self, src_vocab, src_filename, tgt_vocab, tgt_filename):\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n\n        # We hard-wire the codes for &lt;bos&gt; (1), &lt;eos&gt; (2), and &lt;unk&gt; (3).\n        self.src = [[self.src_vocab.get(w, 3) for w in s] for s in sentences(src_filename)]\n        self.tgt = [[self.tgt_vocab.get(w, 3) for w in s] + [2] for s in sentences(tgt_filename)]\n\n    def __getitem__(self, idx):\n        return self.src[idx], self.tgt[idx]\n\n    def __len__(self):\n        return len(self.src)\n\nWe load the training data:\n\ntrain_dataset = TranslationDataset(src_vocab, 'train-de.txt', tgt_vocab, 'train-en.txt')\n\nThe following function will be helpful for debugging. It extracts a single source–target pair of sentences from the specified dataset and converts it into batches of size 1, which can be fed into the encoder–decoder model.\n\ndef example(dataset, i):\n    src, tgt = dataset[i]\n    return torch.LongTensor(src).unsqueeze(0), torch.LongTensor(tgt).unsqueeze(0)\n\n\nexample(train_dataset, 0)"
  },
  {
    "objectID": "assignments/assignment2/Assignment2.html#problem-2-the-encoderdecoder-architecture",
    "href": "assignments/assignment2/Assignment2.html#problem-2-the-encoderdecoder-architecture",
    "title": "Assignment 2: Machine translation",
    "section": "Problem 2: The encoder–decoder architecture",
    "text": "Problem 2: The encoder–decoder architecture\nIn this section, you will implement the encoder–decoder architecture, including the extension of that architecture by an attention mechanism. The implementation consists of four parts: the encoder, the attention mechanism, the decoder, and a class that wraps the complete architecture.\n\nProblem 2.1: Implement the encoder\nThe encoder is relatively straightforward. We look up word embeddings and unroll a bidirectional GRU over the embedding vectors to compute a representation at each token position. We then take the last hidden state of the forward GRU and the last hidden state of the backward GRU, concatenate them, and pass them through a linear layer. This produces a summary of the source sentence, which we will later feed into the decoder.\nTo solve this problem, complete the skeleton code in the next code cell:\n\nimport torch.nn as nn\n\nclass Encoder(nn.Module):\n\n    def __init__(self, num_words, embedding_dim=256, hidden_dim=512):\n        super().__init__()\n        # TODO: Add your code here\n\n    def forward(self, src):\n        # TODO: Replace the next line with your own code\n        raise NotImplementedError\n\nYour code must comply with the following specification:\ninit (num_words, embedding_dim = 256, hidden_dim = 512)\n\nInitialises the encoder. The encoder consists of an embedding layer that maps each of num_words words to an embedding vector of size embedding_dim, a bidirectional GRU that maps each embedding vector to a position-specific representation of size 2 × hidden_dim, and a final linear layer that projects the concatenation of the final hidden states of the GRU (the final hidden state of the forward direction and the final hidden state of the backward direction) to a single vector of size hidden_dim.\n\nforward (self, src)\n\nTakes a tensor src with source-language word ids and sends it through the encoder. The input tensor has shape (batch_size, src_len), where src_len is the length of the sentences in the batch. (We will make sure that all sentences in the same batch have the same length.) The method returns a pair of tensors (output, hidden), where output has shape (batch_size, src_len, 2 × hidden_dim), and hidden has shape (batch_size, hidden_dim).\n\n\n\n🤞 Test your code\nTo test your code, instantiate an encoder, feed it the first source sentence in the training data, and check that the tensors returned by the encoder have the expected shapes.\n\n\nProblem 2.2: Implement the attention mechanism\nYour next task is to implement the attention mechanism. Recall that the purpose of this mechanism is to inform the decoder when generating the translation of the next word. For this, attention has access to the previous hidden state of the decoder, as well as the complete output of the encoder. It returns the attention-weighted sum of the encoder output, the so-called context vector. For later usage, we also return the attention weights.\nAs mentioned in the lecture, attention can be implemented in various ways. One very simple implementation is uniform attention, which assigns equal weight to each position-specific representation in the output of the encoder, and completely ignores the hidden state of the decoder. This mechanism is implemented in the cell below.\n\nimport torch.nn.functional as F\n\nclass UniformAttention(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, decoder_hidden, encoder_output, src_mask):\n        batch_size, src_len, _ = encoder_output.shape\n\n        # Set all attention scores to the same constant value (0). After\n        # the softmax, we will have uniform weights.\n        scores = torch.zeros(batch_size, src_len, device=encoder_output.device)\n\n        # Mask out the attention scores for the padding tokens. We set\n        # them to -inf. After the softmax, we will have 0.\n        scores.data.masked_fill_(~src_mask, -float('inf'))\n\n        # Convert scores into weights\n        alpha = F.softmax(scores, dim=1)\n\n        # The context is the alpha-weighted sum of the encoder outputs.\n        context = torch.bmm(alpha.unsqueeze(1), encoder_output).squeeze(1)\n\n        return context, alpha\n\nOne technical detail in this code is our use of a mask src_mask to compute attention weights only for the ‘real’ tokens in the source sentences, but not for the padding tokens that we introduce to bring all sentences in a batch to the same length. Your task now is to implement the attention mechanism from the paper by Bahdanau et al. (2015). The relevant equation is in Section A.1.2.\nHere is the skeleton code for this problem. As you can see, your specific task is to initialise the required parameters and to compute the attention scores (scores); the rest of the code is the same as for the uniform attention.\n\nclass BahdanauAttention(nn.Module):\n\n    def __init__(self, hidden_dim=512):\n        super().__init__()\n        # TODO: Add your code here\n\n    def forward(self, decoder_hidden, encoder_output, src_mask):\n        batch_size, src_len, _ = encoder_output.shape\n\n        # TODO: Replace the next line with your own code\n        scores = torch.zeros(batch_size, src_len, device=encoder_output.device)\n\n        # The rest of the code is as in UniformAttention\n\n        # Mask out the attention scores for the padding tokens. We set\n        # them to -inf. After the softmax, we will have 0.\n        scores.data.masked_fill_(~src_mask, -float('inf'))\n\n        # Convert scores into weights\n        alpha = F.softmax(scores, dim=1)\n\n        # The context vector is the alpha-weighted sum of the encoder outputs.\n        context = torch.bmm(alpha.unsqueeze(1), encoder_output).squeeze(1)\n\n        return context, alpha\n\nYour code must comply with the following specification:\nforward (decoder_hidden, encoder_output, src_mask)\n\nTakes the previous hidden state of the decoder (decoder_hidden) and the encoder output (encoder_output) and returns a pair (context, alpha) where context is the context as computed as in Bahdanau et al. (2015), and alpha are the corresponding attention weights. The hidden state has shape (batch_size, hidden_dim), the encoder output has shape (batch_size, src_len, 2 × hidden_dim), the context has shape (batch_size, 2 × hidden_dim), and the attention weights have shape (batch_size, src_len).\n\n\n\n🤞 Test your code\nTo test your code, extend your test from Problem 2.1: Feed the output of your encoder into your attention class. As the previous hidden state of the decoder, you can use the hidden state returned by the encoder. You will also need to create a source mask; this can be done as follows:\nsrc_mask = (src != 0)\nCheck that the context tensor and the attention weights returned by the attention class have the expected shapes.\n\n\nProblem 2.3: Implement the decoder\nNow you are ready to implement the decoder. Like the encoder, the decoder is based on a GRU; but this time we use a unidirectional network, as we generate the target sentences left-to-right.\n⚠️ We expect that solving this problem will take you the longest time in this lab.\nBecause the decoder is an autoregressive model, we need to unroll the GRU ‘manually’: At each position, we take the previous hidden state as well as the new input, and apply the GRU for one step. The initial hidden state comes from the encoder. The new input is the embedding of the previous word, concatenated with the context vector from the attention model. To produce the final output, we take the output of the GRU, concatenate the embedding vector and the context vector (residual connection), and feed the result into a linear layer. Here is a graphical representation:\n\nWe need to implement this manual unrolling for two very similar tasks: When training, both the inputs to and the target outputs of the GRU come from the training data. When decoding, the outputs of the GRU are used to generate new target-side words, and these words become the inputs to the next step of the unrolling. We have implemented methods forward and decode for these two different modes of usage. Your task is to implement a method step that takes a single step with the GRU.\n\nclass Decoder(nn.Module):\n\n    def __init__(self, num_words, attention, embedding_dim=256, hidden_dim=512):\n        super().__init__()\n        self.embedding = nn.Embedding(num_words, embedding_dim)\n        self.attention = attention\n        # TODO: Add your own code\n\n    def forward(self, encoder_output, hidden, src_mask, tgt):\n        batch_size, tgt_len = tgt.shape\n\n        # Lookup the embeddings for the previous words\n        embedded = self.embedding(tgt)\n\n        # Initialise the list of outputs (in each sentence)\n        outputs = []\n\n        for i in range(tgt_len):\n            # Get the embedding for the previous word (in each sentence)\n            prev_embedded = embedded[:, i]\n\n            # Take one step with the RNN\n            output, hidden, alpha = self.step(encoder_output, hidden, src_mask, prev_embedded)\n\n            # Update the list of outputs (in each sentence)\n            outputs.append(output.unsqueeze(1))\n\n        return torch.cat(outputs, dim=1)\n\n    def decode(self, encoder_output, hidden, src_mask, max_len):\n        batch_size = encoder_output.size(0)\n\n        # Initialise the list of generated words and attention weights (in each sentence)\n        generated = [torch.ones(batch_size, dtype=torch.long, device=hidden.device)]\n        alphas = []\n\n        for i in range(max_len):\n            # Get the embedding for the previous word (in each sentence)\n            prev_embedded = self.embedding(generated[-1])\n\n            # Take one step with the RNN\n            output, hidden, alpha = self.step(encoder_output, hidden, src_mask, prev_embedded)\n\n            # Update the list of generated words and attention weights (in each sentence)\n            generated.append(output.argmax(-1))\n            alphas.append(alpha)\n\n        generated = [x.unsqueeze(1) for x in generated[1:]]\n        alphas = [x.unsqueeze(1) for x in alphas]\n            \n        return torch.cat(generated, dim=1), torch.cat(alphas, dim=1)\n\n    def step(self, encoder_output, hidden, src_mask, prev_embedded):\n        # TODO: Replace the next line with your own code\n        raise NotImplementedError\n\nYour implementation should comply with the following specification:\nstep (self, encoder_output, hidden, src_mask, prev_embedded)\n\nPerforms a single step in the manual unrolling of the decoder GRU. This takes the output of the encoder (encoder_output), the previous hidden state of the decoder (hidden), the source mask as described in Problem 2.2 (src_mask), and the embedding vector of the previous word (prev_embedded), and computes the output as described above.\nThe shape of encoder_output is (batch_size, src_len, 2 × hidden_dim); the shape of hidden is (batch_size, hidden_dim); the shape of src_mask is (batch_size, src_len); and the shape of prev_embedded is (batch_size, embedding_dim).\nThe method returns a triple of tensors (output, hidden, alpha) where output is the position-specific output of the GRU, of shape (batch_size, num_words); hidden is the new hidden state, of shape (batch_size, hidden_dim); and alpha are the attention weights that were used to compute the output, of shape (batch_size, src_len).\n\n\n💡 Hints on the implementation\nBatch first! Per default, the GRU implementation in PyTorch (just as the LSTM implementation) expects its input to be a three-dimensional tensor of the form (seq_len, batch_size, input_size). We find it conceptually easier to change this default behaviour and let the models take their input in the form (batch_size, seq_len, input_size). To do so, set batch_first=True when instantiating the GRU.\nUnsqueeze and squeeze. When doing the unrolling manually, we get the input in the form (batch_size, input_size). To convert between this representation and the (batch_size, seq_len, input_size) representation, you can use unsqueeze and squeeze.\n\n\n\n🤞 Test your code\nTo test your code, extend your test from the previous problems, and simulate a complete forward pass of the encoder–decoder architecture on the example sentence. Check the shapes of the resulting tensors.\n\n\nEncoder–decoder wrapper class\nThe last part of the implementation is a class that wraps the encoder and the decoder as a single model:\n\nclass EncoderDecoder(nn.Module):\n\n    def __init__(self, src_vocab_size, tgt_vocab_size, attention):\n        super().__init__()\n        self.encoder = Encoder(src_vocab_size)\n        self.decoder = Decoder(tgt_vocab_size, attention)\n\n    def forward(self, src, tgt):\n        encoder_output, hidden = self.encoder(src)\n        return self.decoder.forward(encoder_output, hidden, src != 0, tgt)\n\n    def decode(self, src, max_len):\n        encoder_output, hidden = self.encoder(src)\n        return self.decoder.decode(encoder_output, hidden, src != 0, max_len)\n\n\n\n🤞 Test your code\nAs a final test, instantiate an encoder–decoder model and use it to decode the example sentence. Check the shapes of the resulting tensors."
  },
  {
    "objectID": "assignments/assignment2/Assignment2.html#problem-3-train-a-translator",
    "href": "assignments/assignment2/Assignment2.html#problem-3-train-a-translator",
    "title": "Assignment 2: Machine translation",
    "section": "Problem 3: Train a translator",
    "text": "Problem 3: Train a translator\nWe now have all the pieces to build and train a complete translation system.\n\nTranslator class\nWe first define a class Translator that initialises an encoder–decoder model and uses it to translate sentences. It can also return the attention weights that were used to produce the translation of each sentence.\n\nclass Translator(object):\n\n    def __init__(self, src_vocab, tgt_vocab, attention, device=torch.device('cpu')):\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        self.device = device\n        self.model = EncoderDecoder(len(src_vocab), len(tgt_vocab), attention).to(device)\n\n    def translate_with_attention(self, sentences):\n        # Encode each sentence\n        encoded = [[self.src_vocab.get(w, 3) for w in s.split()] for s in sentences]\n\n        # Determine the maximal length of an encoded sentence\n        max_len = max(len(e) for e in encoded)\n\n        # Build the input tensor, padding all sequences to the same length\n        src = torch.LongTensor([e + [0] * (max_len - len(e)) for e in encoded]).to(self.device)\n\n        # Run the decoder and convert the result into nested lists\n        with torch.no_grad():\n            decoded, alphas = tuple(d.cpu().numpy().tolist() for d in self.model.decode(src, 2 * max_len))\n\n        # Prune each decoded sentence after the first &lt;eos&gt;\n        i2w = {i: w for w, i in self.tgt_vocab.items()}\n        result = []\n        for d, a in zip(decoded, alphas):\n            d = [i2w[i] for i in d]\n            try:\n                eos_index = d.index('&lt;eos&gt;')\n                del d[eos_index:]\n                del a[eos_index:]\n            except:\n                pass\n            result.append((' '.join(d), a))\n\n        return result\n\n    def translate(self, sentences):\n        translated, alphas = zip(*self.translate_with_attention(sentences))\n        return translated\n\nThe code below shows how this class is supposed to be used:\n\ntranslator = Translator(src_vocab, tgt_vocab, BahdanauAttention())\ntranslator.translate(['ich weiß nicht .', 'das haus ist klein .'])\n\n\n\nEvaluation function\nAs mentioned in the lectures, machine translation systems are typically evaluated using the BLEU metric. Here we use the implementation of this metric from the sacrebleu library.\n\n# If sacrebleu is not found, uncomment the next line:\n# !pip install sacrebleu\n\nimport sacrebleu\n\ndef bleu(translator, src, ref):\n    translated = translator.translate(src)\n    return sacrebleu.raw_corpus_bleu(translated, [ref], 0.01).score\n\nWe will report the BLEU score on the validation data:\n\nwith open('valid-de.txt') as src, open('valid-en.txt') as ref:\n    valid_src = [line.rstrip() for line in src]\n    valid_ref = [line.rstrip() for line in ref]\n\n\n\nBatcher class\nTo prepare the training, we next create a class that takes a batch of encoded parallel sentences (a pair of lists of integers) and transforms it into two tensors, one for the source side and one for the target side. Each tensor contains sequences padded to the length of the longest sequence.\n\nclass TranslationBatcher(object):\n\n    def __init__(self, device):\n        self.device = device\n\n    def __call__(self, batch):\n        srcs, tgts = zip(*batch)\n\n        # Determine the maximal length of a source/target sequence\n        max_src_len = max(len(s) for s in srcs)\n        max_tgt_len = max(len(t) for t in tgts)\n\n        # Create the source/target tensors\n        S = torch.LongTensor([s + [0] * (max_src_len - len(s)) for s in srcs])\n        T = torch.LongTensor([t + [0] * (max_tgt_len - len(t)) for t in tgts])\n\n        return S.to(self.device), T.to(self.device)\n\n\n\nTraining loop\nThe training loop is pretty standard. We use a few new utilities from the PyTorch ecosystem.\n\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndef train(n_epochs=2, batch_size=128, lr=5e-4):\n    # Build the vocabularies\n    vocab_src = make_vocab(sentences('train-de.txt'), 10000)\n    vocab_tgt = make_vocab(sentences('train-en.txt'), 10000)\n\n    # Prepare the dataset\n    train_dataset = TranslationDataset(vocab_src, 'train-de.txt', vocab_tgt, 'train-en.txt')\n\n    # Prepare the data loaders\n    batcher = TranslationBatcher(device)\n    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=batcher)\n\n    # Build the translator\n    translator = Translator(src_vocab, tgt_vocab, BahdanauAttention(), device=device)\n\n    # Initialise the optimiser\n    optimizer = torch.optim.Adam(translator.model.parameters(), lr=lr)\n\n    # Make it possible to interrupt the training\n    try:\n        for epoch in range(n_epochs):\n            losses = []\n            bleu_valid = 0\n            sample = '&lt;none&gt;'\n            with tqdm(total=len(train_dataset)) as pbar:\n                for i, (src_batch, tgt_batch) in enumerate(train_loader):\n                    # Create a shifted version of tgt_batch containing the previous words\n                    batch_size, tgt_len = tgt_batch.shape\n                    bos = torch.ones(batch_size, 1, dtype=torch.long, device=tgt_batch.device)\n                    tgt_batch_shifted = torch.cat((bos, tgt_batch[:, :-1]), dim=1)\n\n                    translator.model.train()\n\n                    # Forward pass\n                    scores = translator.model(src_batch, tgt_batch_shifted)\n                    scores = scores.view(-1, len(tgt_vocab))\n\n                    # Backward pass\n                    optimizer.zero_grad()\n                    loss = F.cross_entropy(scores, tgt_batch.view(-1), ignore_index=0)\n                    loss.backward()\n                    optimizer.step()\n\n                    # Update the diagnostics\n                    losses.append(loss.item())\n                    pbar.set_postfix(loss=(sum(losses) / len(losses)), bleu_valid=bleu_valid, sample=sample)\n                    pbar.update(len(src_batch))\n\n                    if i % 50 == 0:\n                        translator.model.eval()\n                        bleu_valid = int(bleu(translator, valid_src, valid_ref))\n                        sample = translator.translate(['das haus ist klein .'])[0]\n\n    except KeyboardInterrupt:\n        pass\n\n    return translator\n\nNow it is time to train the system. During training, two diagnostics will be printed periodically: the running average of the training loss, the BLEU score on the validation data, and the translation of a sample sentence, das haus ist klein (which should translate into the house is small).\nAs mentioned before, training the translator takes quite a bit of compute power and time. Even with a GPU, you should expect training times per epoch of about 5–6 minutes. The default number of epochs is 2; however, you may want to interrupt the training prematurely and use a partially trained model in case you run out of time.\n\ntranslator = train()\n\n⚠️ Your submitted notebook must contain output demonstrating at least 16 BLEU points on the validation data."
  },
  {
    "objectID": "assignments/assignment2/Assignment2.html#problem-4-visualising-attention-reflection",
    "href": "assignments/assignment2/Assignment2.html#problem-4-visualising-attention-reflection",
    "title": "Assignment 2: Machine translation",
    "section": "Problem 4: Visualising attention (reflection)",
    "text": "Problem 4: Visualising attention (reflection)\nFigure 3 in the paper by Bahdanau et al. (2015) shows some heatmaps of attention weights in selected sentences. In the last problem of this lab, we ask you to inspect attention weights for your trained translation system. We define a function plot_attention that visualizes the attention weights. The x axis corresponds to the words in the source sentence (German) and the y axis to the generated target sentence (English). The heatmap colors represent the strengths of the attention weights.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%config InlineBackend.figure_format = 'svg'\n\nplt.style.use('seaborn')\n\ndef plot_attention(translator, sentence):\n    translation, weights = translator.translate_with_attention([sentence])[0]\n    weights = np.array(weights)\n\n    fig, ax = plt.subplots()\n    heatmap = ax.pcolor(weights, cmap='Blues_r')\n\n    ax.set_xticklabels(sentence.split(), minor=False, rotation='vertical')\n    ax.set_yticklabels(translation.split(), minor=False)\n\n    ax.xaxis.tick_top()\n    ax.set_xticks(np.arange(weights.shape[1]) + 0.5, minor=False)\n    ax.set_yticks(np.arange(weights.shape[0]) + 0.5, minor=False)\n    ax.invert_yaxis()\n\n    plt.colorbar(heatmap)\n\nHere is an example:\n\nplot_attention(translator, 'das haus ist klein .')\n\nUse these heatmaps to inspect the attention patterns for selected German sentences. Try to find sentences for which the model produces reasonably good English translations. If your German is a bit rusty (or non-existent), use sentences from the validation data. It might be interesting to look at examples where the German and the English word order differ substantially. Document your exploration in a short reflection piece (ca. 150 words). Respond to the following prompts:\n\nWhat sentences did you try out? What patterns did you spot? Include example heatmaps in your notebook.\nBased on what you know about attention, did you expect your results? Was there anything surprising in them?"
  },
  {
    "objectID": "assignments/assignment1/index.html",
    "href": "assignments/assignment1/index.html",
    "title": "Assignment 1",
    "section": "",
    "text": "This is a placeholder for the first assignment."
  },
  {
    "objectID": "assignments/assignment3/index.html",
    "href": "assignments/assignment3/index.html",
    "title": "Assignment 3",
    "section": "",
    "text": "This is a placeholder for the third assignment."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning for Natural Language Processing",
    "section": "",
    "text": "This is the website for the WASP course “Deep Learning for Natural Language Processing”, taught by Marco Kuhlmann (Linköping University) and Richard Johansson (Chalmers University of Technology)."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Deep Learning for Natural Language Processing",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nNatural Language Processing (NLP) develops methods for making human language accessible to computers. The goal of this course is to provide students with a theoretical understanding of and practical experience with the advanced algorithms that power modern NLP. The course focuses on methods based on deep neural networks.\nOn completion of the course, you will be able to\n\nexplain and analyze state-of-the-art deep learning architectures for NLP\nimplement such architectures and apply them to practical problems\ndesign and carry out evaluations of deep learning architectures for NLP\nuse current approaches to NLP in your own field of research"
  },
  {
    "objectID": "index.html#course-literature",
    "href": "index.html#course-literature",
    "title": "Deep Learning for Natural Language Processing",
    "section": "Course literature",
    "text": "Course literature\nMuch of the content of the course is not described in a book, so we will mostly give pointers to research papers and survey articles when needed. If you prefer textbook-style introductions, you can have a look\nJacob Eisenstein, Natural Language Processing. MIT Press, 2019. Pre-print version"
  },
  {
    "objectID": "index.html#teaching",
    "href": "index.html#teaching",
    "title": "Deep Learning for Natural Language Processing",
    "section": "Teaching",
    "text": "Teaching\nThe course uses a flipped classroom format where the central concepts and methods are presented in pre-recorded video lectures. The physical meetings will focus on discussions of the important points in the recorded material, practical exercises, and reflections.\nThe teaching is divided into three two-day meetings, each of which covers one module:\n\nMeeting 1: 11–12 April (Linköping)\nMeeting 2: 16–17 May (Gothenburg)\nMeeting 3: 3–4 June (Linköping)\n\nEach meeting will start with a lunch at 12:00 and end at 15:00 on the second day."
  },
  {
    "objectID": "index.html#modules",
    "href": "index.html#modules",
    "title": "Deep Learning for Natural Language Processing",
    "section": "Modules",
    "text": "Modules\nThe course consists of three thematic modules and a final project. The content of each module consists of\n\nvideo lectures introducing the important topics\npointers to literature, some of which is fundamental and some optional\na set of programming exercises\na set of discussion tasks\nan assignment where you implement a model\n\nDetailed information about the modules is available on the module pages, which you can find in the menu at the top of this page."
  },
  {
    "objectID": "index.html#project",
    "href": "index.html#project",
    "title": "Deep Learning for Natural Language Processing",
    "section": "Project",
    "text": "Project\nIn the project, you apply your learning in the course to your own field of research."
  },
  {
    "objectID": "assignments/assignment3/Assignment3.html",
    "href": "assignments/assignment3/Assignment3.html",
    "title": "Assignment 3: Graph-based dependency parsing",
    "section": "",
    "text": "In this assignment, you will implement a simplified version of the dependency parser used by Glavaš and Vulić (2021) (Figure 1). This parser consists of a transformer encoder followed by a bi-affine layer that computes arc scores for all pairs of words. These scores are then used as logits in a classifier that predicts the syntactic head of each word. In contrast to the parser described in the paper, your parser will only support unlabelled parsing, i.e., you will implement an arc classifier but no relation classifier. As the encoder, you will use the uncased BERT base model from the Transformers library.\nWe start by importing PyTorch and setting the device we will use for training and evaluating.\nimport torch\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
  },
  {
    "objectID": "assignments/assignment3/Assignment3.html#dataset",
    "href": "assignments/assignment3/Assignment3.html#dataset",
    "title": "Assignment 3: Graph-based dependency parsing",
    "section": "Dataset",
    "text": "Dataset\nThe data for this lab comes from the English Web Treebank from the Universal Dependencies Project; we distribute it here in the form of two JSON files. The code in the next cell below defines a PyTorch Dataset wrapper for the data.\n\nimport json\n\nfrom torch.utils.data import Dataset\n\nclass ParserDataset(Dataset):\n\n    def __init__(self, filename):\n        super().__init__()\n        with open(filename, 'rt', encoding='utf-8') as fp:\n            self.items = [[tuple(x) for x in json.loads(l)] for l in fp]\n\n    def __len__(self):\n        return len(self.items)\n\n    def __getitem__(self, idx):\n        return self.items[idx]\n\nWe can now load the training data:\n\nTRAIN_DATA = ParserDataset('en_ewt-ud-train.jsonl')\n\nA data set consists of parsed sentences. A parsed sentence is represented as a list of pairs. The first component of each pair (a string) represents a word. The second component (an integer) specifies the position of the word’s syntactic head, i.e., its parent in the dependency tree. Note that word positions are numbered starting at 1. The special head position 0 marks the root of the tree.\nRun the following code cell to see an example sentence:\n\nEXAMPLE_SENTENCE = TRAIN_DATA[531]\n\nEXAMPLE_SENTENCE\n\nIn this example the head of the pronoun I is the word at position 2 – the verb like. The dependents of like are I (position 1) and the noun blog (position 4), as well as the final punctuation mark. Note that the pronoun your (position 3) is misspelled as yuor."
  },
  {
    "objectID": "assignments/assignment3/Assignment3.html#problem-1-tokenisation",
    "href": "assignments/assignment3/Assignment3.html#problem-1-tokenisation",
    "title": "Assignment 3: Graph-based dependency parsing",
    "section": "Problem 1: Tokenisation",
    "text": "Problem 1: Tokenisation\nTo feed parsed sentences to BERT, we need to tokenise them and encode the resulting tokens as integers in the model vocabulary. We start by loading the BERT tokeniser using the Auto classes:\n\nfrom transformers import AutoTokenizer\n\nTOKENIZER = AutoTokenizer.from_pretrained('bert-base-uncased')\n\nWe can call the tokeniser on the example sentence as follows:\n\nTOKENIZER([w for w, _ in EXAMPLE_SENTENCE], is_split_into_words=True)\n\nNote that we use the is_split_into_words keyword argument to indicate that the input is already pre-tokenised (split on whitespace).\nThe BERT tokeniser segments each word (pre-token) into one or several subword tokens, and we want to keep track of which of these were introduced by which word. To this end, for each actual word in a parsed sentence we compute the corresponding span in the tokens list.\nTo illustrate this, consider again the tokenisation of the example sentence. Note that in order to match the default behaviour when calling the tokeniser, we explicitly add the special tokens at the beginning and at the end of the sentence.\n\nTOKENIZER.tokenize([w for w, _ in EXAMPLE_SENTENCE], add_special_tokens=True, is_split_into_words=True)\n\nFor this sentence, we would like to compute the following token spans:\n\n[(1, 2), (2, 3), (3, 5), (5, 6), (6, 7)]\n\nEach of these spans covers a single token, except for the span (3, 5) which covers the tokens yu and ##or. Note that token indices start at 1, as position 0 is occupied by the special [CLS] token.\nThe next cell contains skeleton code for a function encode that takes a tokeniser and a batch of sentences and returns the tokeniser’s encoded input as well as the corresponding token spans.\n\ndef encode(tokenizer, sentences):\n    # TODO: Replace the next line with your own code\n    raise NotImplementedError\n\nImplement this function to match the following specification:\nencode (tokenizer, sentences):\n\nUses the specified tokenizer to encode a batch of parsed sentences (sentences). This returns a pair consisting of a BatchEncoding and a matching batch of token spans (as explained above). The BatchEncoding is the standard batch encoding, including the token ids to be fed to a model. Its inner content have been converted to PyTorch tensors. Token indexes start at 1.\n\n\n🤞 Test your code\nTo test you code, call encode on the example sentence and check that output matches your expectations.\n\n\nProblem 2: Merging tokens\nBERT gives us a representation for each token in the input sequence. To compute scores between pairs of words, we need to combine the token representations that correspond to each word. A standard strategy for this is to take their element-wise mean.\nThe next cell contains skeleton code for a function merge_tokens that implements this strategy.\n\ndef merge_tokens(tokens, token_spans):\n    # TODO: Replace the next line with your own code\n    raise NotImplementedError\n\nImplement this function to match the following specification:\nmerge_tokens (tokens, token_spans)\n\nTakes a batch of token vectors (tokens) and a batch of token spans (token_spans) and returns a new batch of word-level representations, computed as explained above. The token vectors are a tensor of shape (batch_size, num_tokens, hidden_dim). The token spans are a nested list of integer pairs as computed in Problem 1. The result is a tensor of shape (batch_size, max_num_words, hidden_dim), where max_num_words denotes the maximum number of words in any sentence in the batch. Entries corresponding to padding are represented by the zero vector of size hidden_dim.\n\n\n\n🤞 Test your code\nTo test you code, create a sample input to merge_tokens and check that the output matches your expectations"
  },
  {
    "objectID": "assignments/assignment3/Assignment3.html#problem-3-biaffine-layer",
    "href": "assignments/assignment3/Assignment3.html#problem-3-biaffine-layer",
    "title": "Assignment 3: Graph-based dependency parsing",
    "section": "Problem 3: Biaffine layer",
    "text": "Problem 3: Biaffine layer\nYour next task is to implement the bi-affine layer. Given matrices \\(X \\in \\mathbb{R}^{m \\times h}\\) and \\(X' \\in \\mathbb{R}^{n \\times h}\\), this layer computes a matrix \\(Y \\in \\mathbb{R}^{m \\times n}\\) as\n\\[\nY = X W X'{}^\\top + b\n\\]\nwhere \\(W \\in \\mathbb{R}^{h \\times h}\\) and \\(b \\in \\mathbb{R}\\) are learnable weight and bias parameters. In the context of the dependency parser, the matrices \\(X\\) and \\(X'\\) hold the encodings of all words in the input sentence, and the entries of the matrix \\(Y\\) are interpreted as scores of dependency arcs between words. More specifically, the entry \\(Y_{ij}\\) represents the score of an arc from a head word at position \\(j\\) to a dependent at position \\(i\\).\nThe following cell contains skeleton code for the implementation of the bi-affine layer. Implement this layer according to the specification above.\n\nimport torch.nn as nn\n\nclass Biaffine(nn.Module):\n\n    def __init__(self, in_features):\n        super().__init__()\n        # TODO: Replace the next line with your own code\n        raise NotImplementedError\n\n    def forward(self, x1, x2):\n        # TODO: Replace the next line with your own code\n        raise NotImplementedError\n\n⚠️ Note that your implementation should be able to handle batches of input sentences.\n\n🤞 Test your code\nTo test you code, create a sample input to the bi-affine layer as well as suitable weights and biases and check that the output of the forward method matches your expectations"
  },
  {
    "objectID": "assignments/assignment3/Assignment3.html#problem-4-parser",
    "href": "assignments/assignment3/Assignment3.html#problem-4-parser",
    "title": "Assignment 3: Graph-based dependency parsing",
    "section": "Problem 4: Parser",
    "text": "Problem 4: Parser\nWe are now ready to put the two main components of the parser together: the encoder (BERT) and the bi-affine layer that computes the arc scores. We also add a dropout layer between the two components. The following code cell contains skeleton code for the parsing model with the init method already complete. Your task is to implement the forward method. Have another look at the paper to understand how things need to be wired up.\n\nimport torch.nn as nn\nimport torch\n\nfrom transformers import BertConfig, BertModel, BertPreTrainedModel\n\nclass BertForParsing(BertPreTrainedModel):\n\n    config_class = BertConfig\n\n    def __init__(self, config, dropout=0.1):\n        super().__init__(config)\n        self.encoder = BertModel(config)\n        self.dropout = nn.Dropout(dropout)\n        self.biaffine = Biaffine(config.hidden_size)\n\n    def forward(self, encoded_input, token_spans):\n        # TODO: Replace the next line with your own code\n        raise NotImplementedError\n\nImplement the forward method to match the following specification:\nforward (encoded_input, token_spans)\n\nTakes a tokeniser-encoded batch of sentences (of type BatchEncoding) and a corresponding batch of token spans and returns a tensor with scores between all words in the input. More specifically, the output tensor \\(Y\\) has shape (batch_size, num_words, num_words+1), where the entry \\(Y_{bij}\\) represents the score of an arc from a head word at position \\(j\\) to a dependent at position \\(i\\) in the \\(b\\)th sentence of the batch. Note that the number of possible heads is one greater than the number of possible dependents because the heads include the special token [CLS] (at position 0), which the paper uses to represent the root vertex.\n\n\n🤞 Test your code\nTo test you code, instantiate the parsing model and feed it the tokenised example sentence."
  },
  {
    "objectID": "assignments/assignment3/Assignment3.html#batching-the-data",
    "href": "assignments/assignment3/Assignment3.html#batching-the-data",
    "title": "Assignment 3: Graph-based dependency parsing",
    "section": "Batching the data",
    "text": "Batching the data\nWe are now almost ready to train the parser. The missing piece is a data collator that prepares a batch of parsed sentences:\n\ntokenises the sentences and extracts token spans using encode (Problem 1)\nconstructs the ground-truth head tensor needed to compute the loss (Problem 2)\n\nThe code in the next cell implements these two steps. For pseudo-words introduced through padding, we assign a head index of −100. This value is ignored by PyTorch’s cross-entropy loss function.\n\nimport torch\n\nclass ParserBatcher(object):\n\n    def __init__(self, tokenizer, device=None):\n        self.tokenizer = tokenizer\n        self.device = device\n\n    def __call__(self, parser_inputs):\n        encoded_input, start_indices = encode(self.tokenizer, parser_inputs)\n\n        # Get the maximal number of words, for padding\n        max_num_words = max(len(s) for s in parser_inputs)\n\n        # Construct tensor containing the ground-truth heads\n        all_heads = []\n        for parser_input in parser_inputs:\n            words, heads = zip(*parser_input)\n            heads = list(heads)\n            heads.extend([-100] * (max_num_words - len(heads)))  # -100 will be ignored\n            all_heads.append(heads)\n        all_heads = torch.LongTensor(all_heads)\n\n        # Send all data to the specified device\n        if self.device:\n            encoded_input = encoded_input.to(self.device)\n            all_heads = all_heads.to(self.device)\n\n        return encoded_input, start_indices, all_heads"
  },
  {
    "objectID": "assignments/assignment3/Assignment3.html#training-loop",
    "href": "assignments/assignment3/Assignment3.html#training-loop",
    "title": "Assignment 3: Graph-based dependency parsing",
    "section": "Training loop",
    "text": "Training loop\nFinally, here is the training loop of the parser. Most of it is quite standard. The training loss of the parser is the cross-entropy between the head scores and the ground truth head positions. In other words, the parser is trained as a classifier that predicts the position of each word’s head.\n\nimport torch.nn.functional as F\n\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndef train(dataset, n_epochs=1, lr=1e-5, batch_size=8):\n    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n    model = BertForParsing.from_pretrained('bert-base-uncased').to(DEVICE)\n    data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=ParserBatcher(tokenizer, device=DEVICE))\n    optimizer = Adam(model.parameters(), lr=lr)\n    for epoch in range(n_epochs):\n        model.train()\n        running_loss = 0\n        n_batches = 0\n        with tqdm(total=len(dataset)) as pbar:\n            pbar.set_description(f'Epoch {epoch+1}')\n            for encoded_input, token_spans, gold_heads in data_loader:\n                optimizer.zero_grad()\n                head_scores = model.forward(encoded_input, token_spans)\n                loss = F.cross_entropy(head_scores.flatten(0, -2), gold_heads.view(-1))\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item()\n                n_batches += 1\n                pbar.set_postfix(loss=running_loss/n_batches)\n                pbar.update(len(token_spans))\n    return model\n\nWe are now ready to train the parser. With a GPU, you should expect training times of approximately 3 minutes per epoch.\n\nPARSING_MODEL = train(TRAIN_DATA, n_epochs=1)"
  },
  {
    "objectID": "assignments/assignment3/Assignment3.html#evaluation",
    "href": "assignments/assignment3/Assignment3.html#evaluation",
    "title": "Assignment 3: Graph-based dependency parsing",
    "section": "Evaluation",
    "text": "Evaluation\nThe parser is evaluated using unlabelled attachment score (UAS), which is the percentage of words that have been assigned their correct heads. Note that pseudo-words corresponding to padding (which we marked with the special head index −100 above) must be excluded from this calculation.\n\nDEV_DATA = ParserDataset('en_ewt-ud-dev.jsonl')\n\n\nimport torch\n\ndef evaluate(model, dataset, batch_size=8):\n    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n    data_loader = DataLoader(DEV_DATA, batch_size=batch_size, collate_fn=ParserBatcher(tokenizer, device=DEVICE))\n    n_correct = 0\n    n_total = 0\n    model.eval()\n    with tqdm(total=len(dataset)) as pbar:\n        for encoded_input, token_spans, gold_heads in data_loader:\n            with torch.no_grad():\n                head_scores = model.forward(encoded_input, token_spans)\n                pred_heads = torch.argmax(head_scores, dim=-1)\n            mask = gold_heads.ne(-100)\n            n_correct += torch.sum(pred_heads[mask] == gold_heads[mask])\n            n_total += torch.sum(mask)\n            pbar.update(len(token_spans))\n    return n_correct / n_total\n\n\nevaluate(PARSING_MODEL, DEV_DATA)\n\nYour notebook must contain output demonstrating at least 88% UAS on the development data.\nThat’s it! Congratulations on finishing the last assignment of this course! 🥳"
  },
  {
    "objectID": "assignments/assignment2/index.html",
    "href": "assignments/assignment2/index.html",
    "title": "Assignment 2",
    "section": "",
    "text": "This is a placeholder for the second assignment."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "modules/module2/index.html",
    "href": "modules/module2/index.html",
    "title": "Website",
    "section": "",
    "text": "This is a Quarto website. Test.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "modules/module0/Introduction_to_PyTorch.html",
    "href": "modules/module0/Introduction_to_PyTorch.html",
    "title": "Introduction to PyTorch",
    "section": "",
    "text": "The purpose of this notebook is to introduce you to the basics of PyTorch, the deep learning framework that we will be using for the labs.\nMany good introductions to PyTorch are available online, including the 60 Minute Blitz on the official PyTorch website. This notebook is designed to put focus on those basics that you will encounter in the labs. Beyond the notebook, you will also need to get comfortable with the PyTorch documentation.\nWe start by importing the PyTorch module:\nimport torch\nThe following code prints the current version of the module:\nprint(torch.__version__)\nThe version of PyTorch at the time of writing this notebook was 1.10.1."
  },
  {
    "objectID": "modules/module0/Introduction_to_PyTorch.html#tensors",
    "href": "modules/module0/Introduction_to_PyTorch.html#tensors",
    "title": "Introduction to PyTorch",
    "section": "Tensors",
    "text": "Tensors\nThe fundamental data structure in PyTorch is the tensor, a multi-dimensional matrix containing elements of a single numerical data type. Tensors are similar to arrays as you may know them from NumPy or MATLAB.\n\nCreating tensors\nOne way to create a tensor is to call the function torch.tensor() on a Python list or NumPy array.\nThe code in the following cell creates a 2-dimensional tensor with 4 elements.\n\nx = torch.tensor([[0, 1], [2, 3]])\nx\n\nEach tensor has a shape, which specifies the number and sizes of its dimensions:\n\nx.shape\n\nEach tensor also has a data type for its elements. More information about data types\n\nx.dtype\n\nWhen creating a tensor, you can explicitly pass the intended data type as a keyword argument:\n\ny = torch.tensor([[0, 1], [2, 3]], dtype=torch.float)\ny.dtype\n\nFor many data types, there also exists a specialised constructor:\n\nz = torch.FloatTensor([[0, 1], [2, 3]])\nz.dtype\n\n\n\nMore creation operations\nCreate a 3D-tensor of the specified shape filled with the scalar value zero:\n\nx = torch.zeros(2, 3, 5)\nx\n\nCreate a 3D-tensor filled with random values:\n\nx = torch.rand(2, 3, 5)\nx\n\nCreate a tensor with the same shape as another one, but filled with ones:\n\ny = torch.ones_like(x)\ny    # shape: [2, 3, 5]\n\nFor a complete list of tensor-creating operations, see Creation ops.\n\n\nEmbrace vectorisation!\nIteration is of one the most useful techniques for processing data in Python. However, you should not loop over tensors. Instead, you should be looking at vectorising any operations. This is because looping over tensors is slow, while vectorised operations on tensors are fast (and can be made even faster when the code is run on a GPU). To illustrate this point, let us create a 1D-tensor containing the first 1M integers:\n\nx = torch.arange(1000000)\nx\n\nSumming up the elements of the tensor using a loop is relatively slow:\n\nsum(i for i in x)\n\nDoing the same thing using a tensor operation is much faster:\n\nx.sum()\n\n\n\nIndexing and slicing\nTo access the contents of a tensor, you can use an extended version of Python’s syntax for indexing and slicing. Essentially the same syntax is used by NumPy. For more information, see Indexing on ndarrays.\nTo illustrate this, we create a 3D-tensor with random numbers:\n\nx = torch.rand(2, 3, 5)\nx\n\nIndex an element by a 3D-coordinate; this gives a 0D-tensor:\n\nx[0,1,2]\n\n(If you want the result as a non-tensor, use the method item().)\nIndex the second element; this gives a 2D-tensor:\n\nx[1]\n\nIndex the second-to-last element:\n\nx[-2]\n\nSlice out the sub-tensor with elements from index 1 onwards; this gives a 3D-tensor:\n\nx[1:]\n\nHere is a more complex example of slicing. As in Python, the colon : selects all indices of a dimension.\n\nx[:,:,2:4]\n\nThe syntax for indexing and slicing is very powerful. For example, the same effect as in the previous cell can be obtained with the following code, which uses the ellipsis (...) to match all dimensions but the ones explicitly mentioned:\n\nx[...,2:4]\n\n\n\nCreating views\nYou will sometimes want to use a tensor with a different shape than its initial shape. In these situations, you can re-shape the tensor or create a view of the tensor. The latter is preferable because views can share the same data as their base tensors and thus do not require copying.\nWe create a 3D-tensor of 12 random values:\n\nx = torch.rand(2, 3, 2)\nx\n\nCreate a view of this tensor as a 2D-tensor:\n\nx.view(3, 4)\n\nWhen creating a view, the special size -1 is inferred from the other sizes:\n\nx.view(3, -1)\n\nModifying a view affects the data in the base tensor:\n\ny = torch.rand(2, 3, 2)\nz = y.view(3, 4)\nz[2, 3] = 42\ny\n\n\n\nMore viewing operations\nThere are a few other useful methods that create views. More information about views\n\nx = torch.rand(2, 3, 5)\nx\n\nThe permute() method returns a view of the base tensor with some of its dimensions permuted. In the example, we maintain the first dimension but swap the second and the third dimension:\n\ny = x.permute(0, 2, 1)\nprint(y)\ny.shape\n\nThe unsqueeze() method returns a tensor with a dimension of size one inserted at the specified position. This is useful e.g. in the training of neural networks when you want to create a batch with just one example.\n\ny = x.unsqueeze(0)\nprint(y)\ny.shape\n\nThe inverse operation to unsqueeze() is squeeze():\n\ny = y.squeeze(0)\nprint(y)\ny.shape\n\n\n\nRe-shaping a tensor\nThere are some cases where you cannot create a view and need to explicitly re-shape a tensor. In particular, this happens when the data in the base tensor and the view are not in contiguous regions of memory.\n\nx = torch.rand(2, 3, 5)\nx\n\nWe permute the tensor x to create a new tensor y in which the data is no longer consecutive in memory:\n\ny = x.permute(0, 2, 1)\n# y = y.view(-1)    # raises a runtime error\ny\n\nIn such a case, you can explicitly re-shape the tensor, which will copy the data if necessary:\n\ny = x.permute(0, 2, 1)\ny = y.reshape(-1)\ny\n\nModifying a reshaped tensor will not necessarily change the data in the base tensor. This depends on whether the reshaped tensor is able to share the data with the base tensor.\n\ny = torch.rand(2, 3, 2)\ny = y.permute(0, 2, 1)    # if commented out, data can be shared\nz = y.reshape(-1)\nz[0] = 42\ny"
  },
  {
    "objectID": "modules/module0/Introduction_to_PyTorch.html#computing-with-tensors",
    "href": "modules/module0/Introduction_to_PyTorch.html#computing-with-tensors",
    "title": "Introduction to PyTorch",
    "section": "Computing with tensors",
    "text": "Computing with tensors\n\nElement-wise operations\nUnary mathematical operations defined on numbers can be ‘lifted’ to tensors by applying them element-wise. This includes multiplication by a constant, exponentiation (**), taking roots (torch.sqrt()), and the logarithm (torch.log()).\n\nx = torch.rand(2, 3)\nprint(x)\nx * 2    # element-wise multiplication with 2\n\nSimilarly, we can do binary mathematical operations on tensors with the same shape. For example, the Hadamard product of two tensors \\(X\\) and \\(Y\\) is the tensor \\(X \\odot Y\\) obtained by the element-wise multiplication of the elements of \\(X\\) and \\(Y\\).\n\nx = torch.rand(2, 3)\ny = torch.rand(2, 3)\ntorch.mul(x, y)    # shape: [2, 3]\n\nThe Hadamard product can be written more succinctly as follows:\n\nx * y\n\n\n\nMatrix product\nWhen computing the matrix product between two tensors \\(X\\) and \\(Y\\), the sizes of the last dimension of \\(X\\) and the first dimension of \\(Y\\) must match. The shape of the resulting tensor is the concatenation of the shapes of \\(X\\) and \\(Y\\), with the last dimension of \\(X\\) and the first dimension of \\(Y\\) removed.\n\nx = torch.rand(2, 3)\ny = torch.rand(3, 5)\ntorch.matmul(x, y)    # shape: [2, 5]\n\nThe matrix product can be written more succinctly as follows:\n\nx @ y\n\n\n\nSum and argmax\nLet us define a tensor of random numbers:\n\nx = torch.rand(2, 3, 5)\nx\n\nYou have already seen that we can compute the sum of a tensor:\n\ntorch.sum(x)\n\nThere is a second form of the sum operation where we can specify the dimension along which the sum should be computed. This will return a tensor with the specified dimension removed.\n\ntorch.sum(x, dim=0)    # shape: [3, 5]\n\n\ntorch.sum(x, dim=1)   # shape: [2, 5]\n\nThe same idea also applies to the operation argmax() which returns the index of the component with the maximal value along the specified dimension.\n\ntorch.argmax(x)    # index of the highest component across all dimensions, numbered in consecutive order\n\n\ntorch.argmax(x, dim=0)   # index of the highest component across the first dimension\n\n\n\nConcatenating tensors\nA list of tensors can be combined into one long tensor by concatenation.\n\nx = torch.rand(2, 3)\ny = torch.rand(3, 3)\nz = torch.cat([x, y])\nprint(z)\nz.shape\n\nYou can also concatenate along a specific dimension:\n\nx = torch.rand(2, 2)\ny = torch.rand(2, 2)\nprint(x)\nprint(y)\nprint(torch.cat([x, y], dim=0))\nprint(torch.cat([x, y], dim=1))\n\n\n\nBroadcasting\nThe term broadcasting describes how PyTorch treats tensors with different shapes. Subject to certain constraints, the ‘smaller’ tensor is ‘broadcast’ across the larger tensor so that they have compatible shapes. Broadcasting is a way to avoid looping. In short, if a PyTorch operation supports broadcasting, then its Tensor arguments can be automatically expanded to be of equal sizes (without making copies of the data).\nIn the simplest case, two tensors have the same shapes. This is the case for the matrix x @ W and the bias vector b in the linear model below:\n\nx = torch.rand(1, 2)\nW = torch.rand(2, 3)\nb = torch.rand(1, 3)\nz = x @ W    # shape: [1, 3]\nz = z + b    # shape: [1, 3]\nprint(z)\nz.shape\n\nNow suppose that we have a whole batch of inputs. Watch what happens when adding the bias vector b:\n\nX = torch.rand(5, 2)\nZ = X @ W    # shape: [5, 3]\nZ = Z + b    # shape: [5, 3]    Broadcasting happens here!\nprint(Z)\nZ.shape\n\nIn the example, broadcasting expands the shape of b from \\([1, 3]\\) into \\([5, 3]\\). The matrix Z is formed by effectively adding b to each row of X. However, this is not implemented by a Python loop but happens implicitly through broadcasting.\nPyTorch uses the same broadcasting semantics as NumPy. More information about broadcasting\nTo be expanded!"
  },
  {
    "objectID": "modules/module1/meeting1.html",
    "href": "modules/module1/meeting1.html",
    "title": "Meeting 1",
    "section": "",
    "text": "The first meeting will take place 11–12 April in Linköping.\nStart and end. The meeting will start with lunch at 12:00 on Thursday, 11 April and end at 15:00 on Friday, 12 April. We will send out a detailed schedule shortly before the meeting.\nMeeting venue. We will be at Linköping University, in B-huset, Campus Valla. Most sessions will be in lecture hall Nobel. For fikas we will walk over to Ljusgården. Lunches will be in Kårallen and Universitetsklubben.\nPublic transport. You can choose between two bus lines to get to B-huset from the city center using public transport (Östgötatrafiken). The specified travel times include a final walk from the bus stop to the meeting room.\n\nBus no. 4 to Lambohov, get off at Nobeltorget (25 minutes)\nBus no. 12 to Lambohov, get off at Universitetet or Mästar Mattias väg (20 minutes)\n\nDinner. On the first day of the meeting (11 April), we will have a joint dinner at Olympia Da Vinci. You registered for the dinner when registering for the course. Address: Bielkegatan 1, 582 21 Linköping (map)\nHotels. We can recommend Scandic Frimurarhotellet and Scandic Linköping City. Both of them are located close to the city centre and train station."
  }
]