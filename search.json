[
  {
    "objectID": "modules/module1/meeting1.html",
    "href": "modules/module1/meeting1.html",
    "title": "Meeting 1",
    "section": "",
    "text": "The first meeting will take place 11–12 April in Linköping."
  },
  {
    "objectID": "modules/module1/meeting1.html#schedule",
    "href": "modules/module1/meeting1.html#schedule",
    "title": "Meeting 1",
    "section": "Schedule",
    "text": "Schedule\nThe meeting will start with lunch at 12:00 on Thursday, 11 April and end at 15:00 on Friday, 12 April. Here is a rough schedule:\n\n\n\nTime\nActivity\n\n\n\n\nThursday\n\n\n\n12:00–13:00\nLunch at Kårallen, Gästmatsalen\n\n\n13:00–13:30\nWelcome. Course logistics\n\n\n13:30–14:30\nDiscussion: Word and document representations\n\n\n14:30–15:00\nFika in Ljusgården\n\n\n15:00–17:00\nExercise 1-1: Simple neural NLP models from scratch (notebook, solution)\n\n\n18:30–21:00\nDinner at Olympia Da Vinci\n\n\nFriday\n\n\n\n09:00–09:30\nDiscussion: Attention (blog post)\n\n\n09:30–10:00\nDiscussion: Bias in NLP models (paper by Garg et al.)\n\n\n10:00–10:30\nFika in Ljusgården\n\n\n10:30–12:00\nExercise 1-2: Using pre-trained NLP models from Hugging Face (notebook, solution)\n\n\n12:00–13:00\nLunch at Universitetsklubben\n\n\n13:00–14:00\nIntroduction to the project\n\n\n14:00–15:00\nIntroduction to Assignment 1: Language models"
  },
  {
    "objectID": "modules/module1/meeting1.html#practical-information",
    "href": "modules/module1/meeting1.html#practical-information",
    "title": "Meeting 1",
    "section": "Practical information",
    "text": "Practical information\nMeeting venue. We will be at Linköping University, in B-huset, Campus Valla. Most sessions will be in lecture hall Nobel. Friday afternoon, we will be in lecture hall Ada Lovelace. For fikas we will walk over to Ljusgården. Lunches will be in Kårallen (Thursday) and Universitetsklubben (Friday).\nPublic transport. You can choose between two bus lines to get to B-huset from the city center using public transport (Östgötatrafiken). The specified travel times include a final walk from the bus stop to the meeting room.\n\nBus no. 4 to Lambohov, get off at Nobeltorget (25 minutes)\nBus no. 12 to Lambohov, get off at Universitetet or Mästar Mattias väg (20 minutes)\n\nDinner. On the first day of the meeting (11 April), we will have a joint dinner at Olympia Da Vinci. You registered for the dinner when registering for the course. Address: Bielkegatan 1, 582 21 Linköping (map)\nHotels. We can recommend Scandic Frimurarhotellet and Scandic Linköping City. Both of them are located close to the city centre and train station."
  },
  {
    "objectID": "modules/module0/Introduction_to_PyTorch.html",
    "href": "modules/module0/Introduction_to_PyTorch.html",
    "title": "Introduction to PyTorch",
    "section": "",
    "text": "The purpose of this notebook is to introduce you to the basics of PyTorch, the deep learning framework that we will be using for the labs.\nMany good introductions to PyTorch are available online, including the 60 Minute Blitz on the official PyTorch website. This notebook is designed to put focus on those basics that you will encounter in the labs. Beyond the notebook, you will also need to get comfortable with the PyTorch documentation.\nWe start by importing the PyTorch module:\nimport torch\nThe following code prints the current version of the module:\nprint(torch.__version__)\nThe version of PyTorch at the time of writing this notebook was 1.10.1."
  },
  {
    "objectID": "modules/module0/Introduction_to_PyTorch.html#tensors",
    "href": "modules/module0/Introduction_to_PyTorch.html#tensors",
    "title": "Introduction to PyTorch",
    "section": "Tensors",
    "text": "Tensors\nThe fundamental data structure in PyTorch is the tensor, a multi-dimensional matrix containing elements of a single numerical data type. Tensors are similar to arrays as you may know them from NumPy or MATLAB.\n\nCreating tensors\nOne way to create a tensor is to call the function torch.tensor() on a Python list or NumPy array.\nThe code in the following cell creates a 2-dimensional tensor with 4 elements.\n\nx = torch.tensor([[0, 1], [2, 3]])\nx\n\nEach tensor has a shape, which specifies the number and sizes of its dimensions:\n\nx.shape\n\nEach tensor also has a data type for its elements. More information about data types\n\nx.dtype\n\nWhen creating a tensor, you can explicitly pass the intended data type as a keyword argument:\n\ny = torch.tensor([[0, 1], [2, 3]], dtype=torch.float)\ny.dtype\n\nFor many data types, there also exists a specialised constructor:\n\nz = torch.FloatTensor([[0, 1], [2, 3]])\nz.dtype\n\n\n\nMore creation operations\nCreate a 3D-tensor of the specified shape filled with the scalar value zero:\n\nx = torch.zeros(2, 3, 5)\nx\n\nCreate a 3D-tensor filled with random values:\n\nx = torch.rand(2, 3, 5)\nx\n\nCreate a tensor with the same shape as another one, but filled with ones:\n\ny = torch.ones_like(x)\ny    # shape: [2, 3, 5]\n\nFor a complete list of tensor-creating operations, see Creation ops.\n\n\nEmbrace vectorisation!\nIteration is of one the most useful techniques for processing data in Python. However, you should not loop over tensors. Instead, you should be looking at vectorising any operations. This is because looping over tensors is slow, while vectorised operations on tensors are fast (and can be made even faster when the code is run on a GPU). To illustrate this point, let us create a 1D-tensor containing the first 1M integers:\n\nx = torch.arange(1000000)\nx\n\nSumming up the elements of the tensor using a loop is relatively slow:\n\nsum(i for i in x)\n\nDoing the same thing using a tensor operation is much faster:\n\nx.sum()\n\n\n\nIndexing and slicing\nTo access the contents of a tensor, you can use an extended version of Python’s syntax for indexing and slicing. Essentially the same syntax is used by NumPy. For more information, see Indexing on ndarrays.\nTo illustrate this, we create a 3D-tensor with random numbers:\n\nx = torch.rand(2, 3, 5)\nx\n\nIndex an element by a 3D-coordinate; this gives a 0D-tensor:\n\nx[0,1,2]\n\n(If you want the result as a non-tensor, use the method item().)\nIndex the second element; this gives a 2D-tensor:\n\nx[1]\n\nIndex the second-to-last element:\n\nx[-2]\n\nSlice out the sub-tensor with elements from index 1 onwards; this gives a 3D-tensor:\n\nx[1:]\n\nHere is a more complex example of slicing. As in Python, the colon : selects all indices of a dimension.\n\nx[:,:,2:4]\n\nThe syntax for indexing and slicing is very powerful. For example, the same effect as in the previous cell can be obtained with the following code, which uses the ellipsis (...) to match all dimensions but the ones explicitly mentioned:\n\nx[...,2:4]\n\n\n\nCreating views\nYou will sometimes want to use a tensor with a different shape than its initial shape. In these situations, you can re-shape the tensor or create a view of the tensor. The latter is preferable because views can share the same data as their base tensors and thus do not require copying.\nWe create a 3D-tensor of 12 random values:\n\nx = torch.rand(2, 3, 2)\nx\n\nCreate a view of this tensor as a 2D-tensor:\n\nx.view(3, 4)\n\nWhen creating a view, the special size -1 is inferred from the other sizes:\n\nx.view(3, -1)\n\nModifying a view affects the data in the base tensor:\n\ny = torch.rand(2, 3, 2)\nz = y.view(3, 4)\nz[2, 3] = 42\ny\n\n\n\nMore viewing operations\nThere are a few other useful methods that create views. More information about views\n\nx = torch.rand(2, 3, 5)\nx\n\nThe permute() method returns a view of the base tensor with some of its dimensions permuted. In the example, we maintain the first dimension but swap the second and the third dimension:\n\ny = x.permute(0, 2, 1)\nprint(y)\ny.shape\n\nThe unsqueeze() method returns a tensor with a dimension of size one inserted at the specified position. This is useful e.g. in the training of neural networks when you want to create a batch with just one example.\n\ny = x.unsqueeze(0)\nprint(y)\ny.shape\n\nThe inverse operation to unsqueeze() is squeeze():\n\ny = y.squeeze(0)\nprint(y)\ny.shape\n\n\n\nRe-shaping a tensor\nThere are some cases where you cannot create a view and need to explicitly re-shape a tensor. In particular, this happens when the data in the base tensor and the view are not in contiguous regions of memory.\n\nx = torch.rand(2, 3, 5)\nx\n\nWe permute the tensor x to create a new tensor y in which the data is no longer consecutive in memory:\n\ny = x.permute(0, 2, 1)\n# y = y.view(-1)    # raises a runtime error\ny\n\nIn such a case, you can explicitly re-shape the tensor, which will copy the data if necessary:\n\ny = x.permute(0, 2, 1)\ny = y.reshape(-1)\ny\n\nModifying a reshaped tensor will not necessarily change the data in the base tensor. This depends on whether the reshaped tensor is able to share the data with the base tensor.\n\ny = torch.rand(2, 3, 2)\ny = y.permute(0, 2, 1)    # if commented out, data can be shared\nz = y.reshape(-1)\nz[0] = 42\ny"
  },
  {
    "objectID": "modules/module0/Introduction_to_PyTorch.html#computing-with-tensors",
    "href": "modules/module0/Introduction_to_PyTorch.html#computing-with-tensors",
    "title": "Introduction to PyTorch",
    "section": "Computing with tensors",
    "text": "Computing with tensors\n\nElement-wise operations\nUnary mathematical operations defined on numbers can be ‘lifted’ to tensors by applying them element-wise. This includes multiplication by a constant, exponentiation (**), taking roots (torch.sqrt()), and the logarithm (torch.log()).\n\nx = torch.rand(2, 3)\nprint(x)\nx * 2    # element-wise multiplication with 2\n\nSimilarly, we can do binary mathematical operations on tensors with the same shape. For example, the Hadamard product of two tensors \\(X\\) and \\(Y\\) is the tensor \\(X \\odot Y\\) obtained by the element-wise multiplication of the elements of \\(X\\) and \\(Y\\).\n\nx = torch.rand(2, 3)\ny = torch.rand(2, 3)\ntorch.mul(x, y)    # shape: [2, 3]\n\nThe Hadamard product can be written more succinctly as follows:\n\nx * y\n\n\n\nMatrix product\nWhen computing the matrix product between two tensors \\(X\\) and \\(Y\\), the sizes of the last dimension of \\(X\\) and the first dimension of \\(Y\\) must match. The shape of the resulting tensor is the concatenation of the shapes of \\(X\\) and \\(Y\\), with the last dimension of \\(X\\) and the first dimension of \\(Y\\) removed.\n\nx = torch.rand(2, 3)\ny = torch.rand(3, 5)\ntorch.matmul(x, y)    # shape: [2, 5]\n\nThe matrix product can be written more succinctly as follows:\n\nx @ y\n\n\n\nSum and argmax\nLet us define a tensor of random numbers:\n\nx = torch.rand(2, 3, 5)\nx\n\nYou have already seen that we can compute the sum of a tensor:\n\ntorch.sum(x)\n\nThere is a second form of the sum operation where we can specify the dimension along which the sum should be computed. This will return a tensor with the specified dimension removed.\n\ntorch.sum(x, dim=0)    # shape: [3, 5]\n\n\ntorch.sum(x, dim=1)   # shape: [2, 5]\n\nThe same idea also applies to the operation argmax() which returns the index of the component with the maximal value along the specified dimension.\n\ntorch.argmax(x)    # index of the highest component across all dimensions, numbered in consecutive order\n\n\ntorch.argmax(x, dim=0)   # index of the highest component across the first dimension\n\n\n\nConcatenating tensors\nA list of tensors can be combined into one long tensor by concatenation.\n\nx = torch.rand(2, 3)\ny = torch.rand(3, 3)\nz = torch.cat([x, y])\nprint(z)\nz.shape\n\nYou can also concatenate along a specific dimension:\n\nx = torch.rand(2, 2)\ny = torch.rand(2, 2)\nprint(x)\nprint(y)\nprint(torch.cat([x, y], dim=0))\nprint(torch.cat([x, y], dim=1))\n\n\n\nBroadcasting\nThe term broadcasting describes how PyTorch treats tensors with different shapes. Subject to certain constraints, the ‘smaller’ tensor is ‘broadcast’ across the larger tensor so that they have compatible shapes. Broadcasting is a way to avoid looping. In short, if a PyTorch operation supports broadcasting, then its Tensor arguments can be automatically expanded to be of equal sizes (without making copies of the data).\nIn the simplest case, two tensors have the same shapes. This is the case for the matrix x @ W and the bias vector b in the linear model below:\n\nx = torch.rand(1, 2)\nW = torch.rand(2, 3)\nb = torch.rand(1, 3)\nz = x @ W    # shape: [1, 3]\nz = z + b    # shape: [1, 3]\nprint(z)\nz.shape\n\nNow suppose that we have a whole batch of inputs. Watch what happens when adding the bias vector b:\n\nX = torch.rand(5, 2)\nZ = X @ W    # shape: [5, 3]\nZ = Z + b    # shape: [5, 3]    Broadcasting happens here!\nprint(Z)\nZ.shape\n\nIn the example, broadcasting expands the shape of b from \\([1, 3]\\) into \\([5, 3]\\). The matrix Z is formed by effectively adding b to each row of X. However, this is not implemented by a Python loop but happens implicitly through broadcasting.\nPyTorch uses the same broadcasting semantics as NumPy. More information about broadcasting\nTo be expanded!"
  },
  {
    "objectID": "modules/module2/meeting2.html",
    "href": "modules/module2/meeting2.html",
    "title": "Meeting 2",
    "section": "",
    "text": "The second meeting will take place 16–17 May at Chalmers, Gothenburg."
  },
  {
    "objectID": "modules/module2/meeting2.html#schedule",
    "href": "modules/module2/meeting2.html#schedule",
    "title": "Meeting 2",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nActivity\n\n\n\n\nThursday\n\n\n\n11:30–13:00\nLunch at Waste\n\n\n13:00–13:30\nComments about Assignment 1\n\n\n13:30–14:30\nDiscussion: Large language models\n\n\n14:30–15:00\nFika\n\n\n15:00–17:00\nExercise 2-1: Efficient fine-tuning (repo)\n\n\nFriday\n\n\n\n09:00–10:00\nDiscussion: Summarization use case\n\n\n10:00–10:30\nFika\n\n\n10:30–11:30\nExercise 2-2: Using open LLMs (notebook, solution)\n\n\n11:30–12:30\nLunch at Waste\n\n\n12:30–15:00\nIntroduction to Assignment 2 (local RAG demo)\n\n\n13:45–15:00\nProject feedback"
  },
  {
    "objectID": "modules/module2/meeting2.html#practical-information",
    "href": "modules/module2/meeting2.html#practical-information",
    "title": "Meeting 2",
    "section": "Practical information",
    "text": "Practical information\nVenue. For the sessions, we will use the room SB-L200, located at Sven Hultins gata 6. At the entrance, walk right and take the first staircase to SB-L200.\nLunch both days will be at Waste, Sven Hultins Plats 5.\nPublic transport. The Chalmers tram and bus stop is located a short distance from the classroom and the lunch restaurant.\nFrom the train station, you can walk a short distance to Brunnsparken, from which the following two lines can be taken directly to Chalmers.\n\nTram 10 (towards Guldheden)\nBus 16 (towards Högsbohöjd)\n\nYou can use the Västtrafik app or pay with a credit card for a single ticket."
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html",
    "href": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "",
    "text": "Fine-tuning all parameters of pre-trained language models can be resource-intensive. Because of this, current research in natural language processing is looking into developing methods for adapting models to downstream tasks without full fine-tuning. These methods only tune a small number of model parameters while yielding performance comparable to that of a fully fine-tuned model.\nIn this exercise, you will implement LoRA, one of the most well-known methods for parameter-efficient fine-tuning. LoRA stands for “Low-Rank Adaptation of Large Language Models” and was originally described in a research article by Hu et al. (2021)."
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#dataset",
    "href": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#dataset",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Dataset",
    "text": "Dataset\nThe data for this lab comes from the Large Movie Review Dataset. The full dataset consists of 50,000 highly polar movie reviews collected from the Internet Movie Database (IMDB). Here, we use a random sample consisting of 2,000 reviews for training and 500 reviews for evaluation.\nTo load the dataset, we use the Hugging Face Datasets library.\n\nfrom datasets import load_dataset\n\nimdb_dataset = load_dataset('csv', data_files = {'train': 'train.csv', 'eval': 'eval.csv'})\n\nimdb_dataset\n\nAs we can see, each sample in the dataset is a record with three fields: an internal index (index, an integer), the text of the review (review, a string), and the sentiment label (label, an integer – 1 for “positive” and 0 for “negative” sentiment).\nHere is an example record:\n\nimdb_dataset['train'][645]"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#tokeniser",
    "href": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#tokeniser",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Tokeniser",
    "text": "Tokeniser\nAs our pre-trained language model, we will use DistilBERT, a compact encoder model with 40% less parameters than BERT base. DistilBERT is not actually a large language model by modern standards and thus does not benefit as much from parameter-efficient fine-tuning as other models. However, it has the benefit of being light and fast, and can be run even on consumer hardware.\nTo feed the movie reviews to DistilBERT, we need to tokenise them and encode the resulting tokens as integers in the model vocabulary. We start by loading the DistilBERT tokeniser using the Auto classes:\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n\nWe then create a tokenised version of the dataset:\n\ndef tokenize_function(batch):\n    return tokenizer(batch['review'], padding=True, truncation=True)\n\ntokenized_imdb_dataset = imdb_dataset.map(tokenize_function, batched=True)\n\ntokenized_imdb_dataset\n\nAs we can see, tokenising adds two additional fields to each review: input_ids is the list of token ids corresponding to the review, and attention_mask is the list of indices specifying which tokens the encoder should attend to.\nTo avoid trouble when fine-tuning the model later, the next cell disables tokeniser parallelism.\n\nimport os\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#trainer",
    "href": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#trainer",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Trainer",
    "text": "Trainer\nIn this section, we will set up our workflow for training and evaluating DistilBERT models. The central component in this workflow is the Trainer, which provides extensive configuration options. Here, we leave most of these options at their default value. Two changes we do make are to enable evaluation of the trained model after each epoch, and to log the training and evaluation loss after every 5 training steps (the default is 500).\n\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='tmp_trainer',\n    evaluation_strategy='epoch',\n    logging_steps=5,\n)\n\nIn addition to the loss, we also track classification accuracy. For this we import the Hugging Face Evaluate library and define a small helper function compute_metrics() that the trainer will call after each epoch.\n\nimport evaluate\n\naccuracy = evaluate.load('accuracy')\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    return accuracy.compute(predictions=predictions, references=labels)\n\nIn the next cell we define a convenience function make_trainer() that creates a readily-configured trainer for a specified model (model). We will use this trainer both to train the model on the training section of the tokenised review dataset, and to evaluate it on the evaluation section.\n\nfrom transformers import Trainer\n\ndef make_trainer(model):\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_imdb_dataset['train'],\n        eval_dataset=tokenized_imdb_dataset['eval'],\n        compute_metrics=compute_metrics,\n    )\n    return trainer"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#problem-1-full-fine-tuning",
    "href": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#problem-1-full-fine-tuning",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Problem 1: Full fine-tuning",
    "text": "Problem 1: Full fine-tuning\nIn the rest of this notebook, we will work our way to the implementation of LoRA, and compare LoRA to traditional fine-tuning methods. Our first point of reference is a fully fine-tuned DistilBERT model.\nWe start by loading the pre-trained model:\n\nfrom transformers import AutoModelForSequenceClassification\n\npretrained_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n\npretrained_model\n\nThe architecture of DistilBERT is that of a standard Transformer encoder with an embedding layer (embeddings) followed by a stack of six Transformer blocks (transformer) and a feedforward network with two linear layers (pre_classifier and classifier) and a final dropout layer (dropout).\n\nCount the number of trainable parameters\nOne relevant measure in the context of parameter-efficient fine-tuning is the number of parameters that need to be changed when training a model. Your first task in this lab is to write a function num_trainable_parameters() that calculates this number for a given model.\n\ndef num_trainable_parameters(model):\n    # TODO: Replace the next line with your own code\n    return 0\n\nThe function should implement the following specification:\n\nnum_trainable_parameters (model)\nReturns the number of float-valued trainable parameters in the specified model as an integer.\n\n\n👍 Hint\nThe term parameter can refer to either complete tensors or the individual elements of these tensors. For example, a linear layer created by nn.Linear(3, 5) has 2 tensor-valued parameters (a weight matrix and a bias vector) and 20 float-valued parameters (the elements of these tensors). To get the tensor-valued parameters of a model, you can use the parameters() method. A parameter is trainable if it requires gradient.\n\ndef num_trainable_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n\n🤞 Test your code\nTo test your code, apply your function to the pre-trained model. The correct number of float-valued trainable parameters for this model is 66,955,010.\n\nprint('Number of trainable parameters:', num_trainable_parameters(pretrained_model))\n\n\n\n\nFine-tuning\nWhen we load the pre-trained model, the Hugging Face Transformers library warns us that the weights of the feedforward network have not yet been trained. To do so, we pass the pre-trained model to a trainer and initiate the fine-tuning process. Because full fine-tuning is so resource-intensive, we save the fine-tuned model to disk. Alternatively, we load an already fine-tuned version of the model (provided on the course website).\n⚠️ Please note that fine-tuning the model will take some time! ⚠️\n\n# Alternative A: Fine-tuning\n\n# finetuned_trainer = make_trainer(pretrained_model)\n# finetuned_trainer.train()\n# finetuned_trainer.save_model('finetuned')\n\n# Alternative B: Load a fine-tuned model\n\nfinetuned_model = AutoModelForSequenceClassification.from_pretrained('finetuned')\n\n\n\nConvenience functions\nBecause we will repeat the steps we just took to fine-tune the pre-trained model several times in this notebook, we define two convenience functions:\n\ndef train(model):\n    print('Number of trainable parameters:', num_trainable_parameters(model))\n    trainer = make_trainer(model)\n    trainer.train()\n    return model\n\n\ndef evaluate(model):\n    trainer = make_trainer(model)\n    return trainer.evaluate()"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#problem-2-tuning-the-final-layers-only",
    "href": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#problem-2-tuning-the-final-layers-only",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Problem 2: Tuning the final layers only",
    "text": "Problem 2: Tuning the final layers only\nIf full fine-tuning marks one end of the complexity spectrum, the other end is marked by only tuning the final layers of the transformer – the head of the model. In the case of DistilBERT, the head consists of the pre_classifier and classifier layers.\nImplement the head-tuning strategy by coding the following function:\n\ndef make_headtuned_model():\n    # TODO: Replace the next line with your own code\n    raise NotImplementedError\n\nHere is the specification of this function:\n\nmake_headtuned_model ()\nReturns a model that is identical to the pre-trained model, except that the head layers have been trained on the sentiment data. (The other parameters of the pre-trained model are left untouched.)\n\n\n👍 Hint\nYou freeze a parameter by setting its requires_grad-attribute to False.\n\ndef make_headtuned_model():\n    model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n\n    for param in model.distilbert.parameters():\n        param.requires_grad = False\n\n    return train(model)\n\nOnce you have an implementation of the head-tuning strategy, evaluate it on the evaluation data. How much accuracy do we lose when only training the final layers of the pre-trained model, compared to full fine-tuning?\n\nheadtuned_model = make_headtuned_model()\n\n\n\n🤞 Test your code\nIf you configured your model correctly, num_trainable_parameters() should show 592,130 trainable parameters.\nFor future reference, we also save the head-tuned model:\n\nmake_trainer(headtuned_model).save_model('headtuned')"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#problem-3-layer-surgery",
    "href": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#problem-3-layer-surgery",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Problem 3: Layer surgery",
    "text": "Problem 3: Layer surgery\nLoRA works by “wrapping” frozen layers from the pre-trained Transformer model inside adapter modules. Conventionally, this wrapping is only applied to the linear layers that transform the queries and values in the self-attention mechanism. To implement the wrapping, we need functions to extract and replace layers in a model. Your task in this section is to code these functions.\n\nExtracting layers\nCode a function that extracts the query and value linear layers from a DistilBERT model:\n\ndef extract(model):\n    # TODO: Replace the next line with your own code\n    return {}\n\nImplement this function to match the following specification:\n\nextract (model)\nTakes a DistilBERT model (model) and extracts the query and value linear layers from each block of the Transformer. Returns a dictionary mapping the DistilBERT module names of these layers to the layers themselves (instances of nn.Linear).\n\n\n👍 Hint\nAs we saw earlier, the DistilBERT model consists of a hierarchy of nested submodules. Each of these can be addressed by a fully-qualified string name. Use get_submodule() to retrieve a layer by name. You can hard-wire the names of the layers you want to extract.\n\nSUBMODULES = \"\"\"\ndistilbert.transformer.layer.0.attention.q_lin\ndistilbert.transformer.layer.0.attention.v_lin\ndistilbert.transformer.layer.1.attention.q_lin\ndistilbert.transformer.layer.1.attention.v_lin\ndistilbert.transformer.layer.2.attention.q_lin\ndistilbert.transformer.layer.2.attention.v_lin\ndistilbert.transformer.layer.3.attention.q_lin\ndistilbert.transformer.layer.3.attention.v_lin\ndistilbert.transformer.layer.4.attention.q_lin\ndistilbert.transformer.layer.4.attention.v_lin\ndistilbert.transformer.layer.5.attention.q_lin\ndistilbert.transformer.layer.5.attention.v_lin\n\"\"\"\n\ndef extract(model):\n    result = {}\n    for name in SUBMODULES.split():\n        result[name] = model.get_submodule(name)\n    return result\n\n\n\n🤞 Test your code\nTo test your code, check the number of trainable float-valued parameters in the extracted layers. This number should be 7,087,104.\n\nsum(p.numel() for m in extract(pretrained_model).values() for p in m.parameters())\n\n\n\n\nReplacing layers\nThe inverse of the extract() function replaces selected layers of a module using a dictionary of named layers.\n\ndef replace(model, named_layers):\n    for name, layer in named_layers.items():\n        components = name.split('.')\n        submodule = model\n        for component in components[:-1]:\n            submodule = getattr(submodule, component)\n        setattr(submodule, components[-1], layer)\n    return model\n\nThis function has the following specification:\n\nreplace (model, named_layers)\nTakes a DistilBERT model (model) and a dictionary in the format returned by extract() (named_layers) and injects the extracted layers into the model. More specifically, suppose that named_layers contains a key–value pair (name, layer). Then the function replaces the submodule of model addressed by the fully-qualified string name name by the layer layer. Returns the modified model.\n\n\n🤞 Test your code\nTo test your implementation, write code that (1) extracts the query and value linear layers from the fine-tuned model; (2) replaces these layers with clones with random weights; and (3) replaces these layers again with the original versions. Evaluating the modified model after step (2) should yield a near-random accuracy. Evaluating it again after step (3) should yield the original accuracy.\nThe following function should be helpful. It clones a linear layer, copying the weights and the bias from the original.\n\nimport torch.nn as nn\n\ndef clone_linear(original):\n    out_features, in_features = original.weight.shape\n    copy = nn.Linear(in_features, out_features)\n    copy.load_state_dict(original.state_dict())\n    return copy\n\n\nimport torch\n\ndef test_extract_and_replace():\n    finetuned_model = AutoModelForSequenceClassification.from_pretrained('finetuned')\n    finetuned_layers = extract(finetuned_model)\n\n    modified_layers = {}\n    for name in finetuned_layers:\n        clone = clone_linear(finetuned_layers[name])\n        with torch.no_grad():\n            nn.init.normal_(clone.weight)\n        modified_layers[name] = clone\n\n    modified_model = replace(finetuned_model, modified_layers)\n    print('After intervention:', evaluate(modified_model))\n\n    modified_model = replace(finetuned_model, finetuned_layers)\n    print('After restoration:', evaluate(modified_model))\n\n\ntest_extract_and_replace()"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#problem-4-lora",
    "href": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#problem-4-lora",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Problem 4: LoRA",
    "text": "Problem 4: LoRA\nIn this section, you will implement the LoRA adapters and fine-tune the adapted model.\nRecall that the basic idea behind LoRA is to conceptualise fine-tuned weights as a sum \\(W_0 + \\Delta W\\) of the weights from the pre-trained model, \\(W_0\\), and a low-rank update matrix \\(\\Delta W\\). The goal of fine-tuning, then, is to learn the update matrix; this happens in the adapter layers.\n\nImplement the adapter\nA LoRA adapter implements the forward function\n\\[\ny = x W_0 + x \\Delta W = x W_0 + x A B\n\\]\nwhere \\(W_0\\) is a linear transformation from the pre-trained model and \\(\\Delta W\\) is a learned update matrix, deconstructed into the product \\(AB\\) of two rank-\\(r\\) matrices \\(A\\) and \\(B\\). LoRA scales the update matrix \\(\\Delta W\\) by a factor of \\(\\alpha / r\\), where \\(\\alpha\\) is a hyperparameter. (To keep the formula tidy, we ignore the fact that the linear transformation in the pre-trained model may additionally include a bias.)\n\nimport torch.nn as nn\n\nclass LoRA(nn.Module):\n\n    def __init__(self, pretrained, rank=6, alpha=12):\n        super().__init__()\n        # TODO: Add your code here\n\n    def forward(self, x):\n        # TODO: Replace the next line with your own code\n        raise NotImplementedError\n\nYour code must comply with the following specification:\ninit (self, pretrained, rank = 6, alpha = 12)\n\nInitialises the LoRA adapter. This sets up the matrices \\(A\\) and \\(B\\) from the equation above. The matrix \\(A\\) is initialised with random weights from a standard normal distribution; the matrix \\(B\\) is initialised with zeros. The argument pretrained is the linear layer from the pre-trained model that should be adapted. The arguments rank and alpha are the rank \\(r\\) and the hyperparameter \\(\\alpha\\) in the equation above.\n\nforward (self, x)\n\nSends an input x through the adapter, implementing the equation above.\n\n\nimport torch.nn as nn\n\nclass LoRA(nn.Module):\n\n    def __init__(self, pretrained, rank=6, alpha=12):\n        super().__init__()\n\n        # Wrap the pretrained layer\n        self.pretrained = pretrained\n\n        # Create the low-rank matrices\n        out_dim, in_dim = pretrained.weight.shape\n        self.A = nn.Linear(in_dim, rank, bias=False)\n        nn.init.normal_(self.A.weight)\n        self.B = nn.Linear(rank, out_dim, bias=False)\n        nn.init.zeros_(self.B.weight)\n\n        # Set the scaling constant\n        self.scaling = alpha / rank\n\n    def forward(self, x):\n        return self.pretrained(x) + self.B(self.A(x)) * self.scaling\n\n\n\nInject the adapter into the pre-trained model\nThe final step is to construct an adapted model by injecting the LoRA adapters into the pre-trained model.\n\ndef make_lora_model(rank):\n    # TODO: Replace the next line with your own code\n    raise NotImplementedError\n\nImplement the function to match the following specification:\n\nmake_lora_model (rank)\nReturns a model that is identical to the pre-trained model, except that the query and value linear layers have been wrapped in LoRA adapters, and the LoRA adapters and the head layers of the pre-trained model have been trained on the sentiment data. (The other parameters of the pre-trained model are left untouched.) The rank of the adapters is specified by the argument rank. The alpha value of the adapters is set to twice the rank (a common rule of thumb).\n\n\ndef make_lora_model(rank):\n    pretrained_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n\n    for param in pretrained_model.distilbert.parameters():\n        param.requires_grad = False\n\n    tmp = {}\n    for name, layer in extract(pretrained_model).items():\n        tmp[name] = LoRA(layer, rank=rank, alpha=2*rank)\n\n    lora_model = replace(pretrained_model, tmp)\n\n    return train(lora_model)\n\nRun the next cell to evaluate your model for \\(r = 6\\) and \\(\\alpha = 12\\). How many trainable parameters does the adapted model have? What accuracy do you get? How do these value relate to the number of trainable parameters and accuracy of the fully fine-tuned model, in terms of percentages?\n\nlora_model = make_lora_model(6)\n\nevaluate(lora_model)"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#problem-5-alternatives-to-transformer-based-models",
    "href": "modules/module2/peft/parameter-efficient-finetuning-with-solutions.html#problem-5-alternatives-to-transformer-based-models",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Problem 5: Alternatives to Transformer-based models",
    "text": "Problem 5: Alternatives to Transformer-based models\nEven with methods for parameter-efficient fine-tuning, applying DistilBERT and other Transformer-based models comes at a significant cost – an investment that does not always pay off. In the final problem of this lab, we ask you to explore a more traditional approach to classification and contrast it with the pre-training/fine-tuning approach of neural language models.\nBrowse the web to find a tutorial on how to apply a classifier from the scikit-learn library to the problem of sentiment classification and implement the method here in this notebook. We suggest you use multinomial Naive Bayes or logistic regression. (Once you have code for one method, it is easy to switch to the other.) Evaluate your chosen classifier on the IMDB dataset.\n\nWhich classifier did you try? What results did you get? How long did it take you to train and run the classifier?\nWhat is your perspective on the trade-off between accuracy and resource requirements between the two approaches?\n\n\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n\ntrain_data = pd.read_csv('train.csv')\neval_data = pd.read_csv('eval.csv')\n\n# MultinomialNB: accuracy 0.83\n# nb_pipe = Pipeline([\n#     ('vec', CountVectorizer()),\n#     ('cls', MultinomialNB()),\n# ])\n\n# LogisticRegression: accuracy 0.84\nnb_pipe = Pipeline([\n    ('vec', CountVectorizer()),\n    ('cls', LogisticRegression(max_iter=1000)),\n])\n\nmodel = nb_pipe.fit(train_data['review'], train_data['label'])\n\nprint(classification_report(eval_data['label'], model.predict(eval_data['review'])))"
  },
  {
    "objectID": "modules/module2/local-rag.html",
    "href": "modules/module2/local-rag.html",
    "title": "Local RAG",
    "section": "",
    "text": "This is the local RAG system that Marco showcased during the last session of Meeting 2. It requires you to install Ollama and download the nomic-text-embed (for querying) and llama-3 (for generating queries and answering questions) models.\n\n!pip install --q unstructured langchain\n!pip install --q \"unstructured[all-docs]\"\n\n\nfrom langchain_community.document_loaders import UnstructuredPDFLoader\nfrom langchain_community.document_loaders import OnlinePDFLoader\n\n\nlocal_path = \"some.pdf\"    # insert your PDF filename here\n\n\nloader = UnstructuredPDFLoader(file_path=local_path)\ndata = loader.load()\n\n\ndata[0].page_content    # shows the raw text content of the first page\n\n\n!pip install --q chromadb\n!pip install --q langchain-text-splitters\n\n\nfrom langchain_community.embeddings import OllamaEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\n\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\nchunks = text_splitter.split_documents(data)\n\n\nvector_db = Chroma.from_documents(\n    documents=chunks,\n    embedding=OllamaEmbeddings(model='nomic-embed-text', show_progress=True),\n    collection_name='local-rag',\n)\n\nOllamaEmbeddings: 100%|███████████████████████████| 4/4 [00:01&lt;00:00,  2.73it/s]\n\n\n\nfrom langchain.prompts import ChatPromptTemplate, PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_core. runnables import RunnablePassthrough\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\n\n\nlocal_model = 'llama3'\nllm = ChatOllama(model=local_model)\n\n\n# Based on the prompt format for Llama 3; see their documentation for details\n\ndef make_prompt(message):\n    return f'&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n{message}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;'\n\n\nQUERY_PROMPT = PromptTemplate(\n    input_variables = ['question'],\n    template = make_prompt('You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines.\\n\\nOriginal question: {question}'),\n)\n\n\nretriever = MultiQueryRetriever.from_llm(\n    vector_db.as_retriever(),\n    llm,\n    prompt=QUERY_PROMPT,\n)\n\n\ntemplate = make_prompt(\"Answer the question based ONLY on the following context:\\n\\n{context}\\n\\nQuestion: {question}\")\n\nprompt = ChatPromptTemplate.from_template(template)\n\n\nchain = (\n    {'context': retriever, 'question': RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\n\nchain.invoke('What is this document about?')\n\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00,  1.54it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 81.55it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 67.02it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 93.30it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 59.18it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 95.18it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 64.52it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 92.63it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 64.02it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 95.38it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 62.01it/s]\n\n\n'Based on the provided context, this document appears to be a thesis or research paper in the field of Natural Language Processing (NLP). Specifically, it discusses the application of machine translation models, particularly transformer models, to translate text from Swedish to Northern Sámi. The document also touches on topics such as preprocessing data, attention mechanisms in transformers, and the challenges of working with low-resource language settings.'\n\n\n\nchain.invoke('What languages are mentioned in the paper?')\n\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00,  1.65it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 86.80it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 63.46it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 91.68it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 63.27it/s]\nOllamaEmbeddings: 100%|██████████████████████████| 1/1 [00:00&lt;00:00, 101.10it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 60.05it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 93.73it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 53.69it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 94.35it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 46.71it/s]\n\n\n'According to the context, the following languages are mentioned:\\n\\n1. Swedish\\n2. Northern Sámi\\n3. Norwegian\\n4. Finnish'\n\n\n\nchain.invoke('What is the BLEU score of the final Swedish-Sámi model?')\n\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00,  1.37it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 73.37it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 62.36it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 47.21it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 95.07it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 60.57it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 59.58it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 60.27it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 59.50it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 60.70it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 81.88it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 51.44it/s]\n\n\n\"According to the text, the results from the evaluation for each language pair's baseline and final model are presented in Table 1. For the Swedish-Sámi model, the BLEU score of the final model is:\\n\\n24.35\"\n\n\n\nchain.invoke('List the main learnings articulated by the author.')\n\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00,  1.48it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 99.41it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 56.55it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 97.99it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 66.19it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 84.92it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 61.73it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 93.36it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 55.96it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 82.73it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 42.92it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 58.08it/s]\nOllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00&lt;00:00, 40.87it/s]\n\n\n\"Based on the provided context, the main learnings articulated by the author are:\\n\\n1. The importance of quality in machine translation, as highlighted by the BLEU scores and the difficulty in evaluating the performance of the model.\\n2. The value of preprocessing data to prepare it for training, including techniques such as removing duplicate sentences and using byte-pair-encoding and stemming.\\n3. The transformer architecture's ability to utilize attention mechanisms to allow models to consider how words relate to each other in a sentence during translation.\\n4. The complexity and resource requirements of the transformer model, including its large number of parameters and need for long training times.\\n\\nThese learnings were gained through hands-on experience with the openNMT framework and by reading papers on machine translation, such as Stenlund et al. (2023) and Vaswani et al. (2023).\""
  },
  {
    "objectID": "modules/module3/index.html",
    "href": "modules/module3/index.html",
    "title": "Module 3",
    "section": "",
    "text": "The topic of this module is structured prediction, an umbrella term for tasks that involve predicting structured outputs, rather than atomic values. We will covers two such tasks: sequence labelling, the task of mapping an input sequence to an output sequence, and dependency parsing, the task of mapping a sentence to a representation of its syntactic structure in the form of a dependency tree. The lectures introduce several technical approaches and concrete algorithms for these tasks.\nWe will discuss the material during the third course meeting in Linköping. Please see the meeting page for details."
  },
  {
    "objectID": "modules/module3/index.html#unit-3-1-sequence-labelling",
    "href": "modules/module3/index.html#unit-3-1-sequence-labelling",
    "title": "Module 3",
    "section": "Unit 3-1: Sequence labelling",
    "text": "Unit 3-1: Sequence labelling\n\n\n\nTitle\nSlides\nVideo\n\n\n\n\nIntroduction to sequence labelling\n[slides]\n[video]\n\n\nApproaches to sequence labelling\n[slides]\n[video]\n\n\nThe Viterbi algorithm\n[slides]\n[video]\n\n\n\n\nReading\n\nEisenstein (2019), chapters 7–8, sections 2.3.1–2.3.2"
  },
  {
    "objectID": "modules/module3/index.html#unit-3-2-dependency-parsing",
    "href": "modules/module3/index.html#unit-3-2-dependency-parsing",
    "title": "Module 3",
    "section": "Unit 3-2: Dependency parsing",
    "text": "Unit 3-2: Dependency parsing\nThis unit introduces dependency parsing, the task of mapping a natural language sentence into a formal representation of its syntactic structure in the form of a dependency tree.\n\nLecture videos\n\n\n\nTitle\nSlides\nVideo\n\n\n\n\nIntroduction to dependency parsing\n[slides]\n[video]\n\n\nThe arc-standard algorithm\n[slides]\n[video]\n\n\nNeural architectures for dependency parsing\n[slides]\n[video]\n\n\n\n\n\nReading\n\nEisenstein (2019), chapter 11\nGlavaš and Vulić (2021)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "assignments/assignment3/assignment3.html",
    "href": "assignments/assignment3/assignment3.html",
    "title": "Assignment 3: Dependency parsing",
    "section": "",
    "text": "In this assignment, you will implement a simplified version of the dependency parser presented by Glavaš and Vulić (2021). This parser consists of a transformer encoder followed by a bi-affine layer that computes arc scores for all pairs of words. These scores are then used as logits in a classifier that predicts the position of the head of each word. In contrast to the parser described in the paper, yours will only support unlabelled parsing, that is, you will not implement what the authors call a relation classifier. As the encoder, you will use the uncased DistilBERT base model from the Transformers library, even though every other BERT-based encoder should work equally well."
  },
  {
    "objectID": "assignments/assignment3/assignment3.html#dataset",
    "href": "assignments/assignment3/assignment3.html#dataset",
    "title": "Assignment 3: Dependency parsing",
    "section": "Dataset",
    "text": "Dataset\nThe data for this lab comes from the English Web Treebank from the Universal Dependencies Project. To read the data, we use the CoNLL-U Parser library. The code in the next cell defines a PyTorch Dataset wrapper for the data.\n\nimport conllu\n\nfrom torch.utils.data import Dataset\n\nclass ParserDataset(Dataset):\n\n    def __init__(self, filename):\n        super().__init__()\n        self.items = []\n        with open(filename, 'rt', encoding='utf-8') as fp:\n            for tokens in conllu.parse_incr(fp):\n                self.items.append([(t['form'], t['head']) for t in tokens if isinstance(t['id'], int)])\n\n    def __len__(self):\n        return len(self.items)\n\n    def __getitem__(self, idx):\n        return self.items[idx]\n\nWe can now load the training data:\n\nTRAIN_DATA = ParserDataset('en_ewt-ud-train.conllu')\n\nOur data consists of parsed sentences. A parsed sentence is represented as a list of pairs. The first component of each pair (a string) represents a word; the second component (an integer) specifies the position of the word’s head, i.e., its parent in the dependency tree. Note that word positions are numbered starting at 1; the head position 0 represents the root of the tree.\nRun the next cell to see an example sentence:\n\nEXAMPLE_WORDS, EXAMPLE_HEADS = zip(*TRAIN_DATA[531])\n\nEXAMPLE_WORDS, EXAMPLE_HEADS\n\nThe example sentence consists of five whitespace-separated words, including the final punctuation mark. The head of the pronoun I is the word at position 2 – the verb like. The head of the word like is the root of the tree (position 0). The dependents of like are I (position 1), the noun blog (position 4), and the final punctuation mark. Note that the pronoun your (position 3) is misspelled as yuor."
  },
  {
    "objectID": "assignments/assignment3/assignment3.html#problem-1-tokenisation",
    "href": "assignments/assignment3/assignment3.html#problem-1-tokenisation",
    "title": "Assignment 3: Dependency parsing",
    "section": "Problem 1: Tokenisation",
    "text": "Problem 1: Tokenisation\nTo feed parsed sentences to DistilBERT, we need to tokenise them and encode the resulting tokens as integers in the model vocabulary. We start by loading the DistilBERT tokeniser using the Auto classes:\n\nfrom transformers import AutoTokenizer\n\nTOKENIZER = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n\nWe can call the tokeniser on the example sentence as shown in the next cell. Note that we use the is_split_into_words keyword argument to indicate that the input is already pre-tokenised (split on whitespace).\n\nEXAMPLE_TOKENS = TOKENIZER(EXAMPLE_WORDS, is_split_into_words=True)\n\nThe output of the tokeniser is an object of class BatchEncoding. The code in the following cell shows the list of tokens:\n\n[TOKENIZER.decode(i) for i in EXAMPLE_TOKENS.input_ids]\n\nAs you can see, the tokeniser adds the special tokens [CLS] and [SEP] and splits unknown words (here: the misspelled word yuor) into sub-word tokens. We will need to keep track of which tokens correspond to which word. To achieve that, we can use the method word_to_tokens(), which gets us the encoded token span (an object of class TokenSpan) corresponding to a word in a sequence of the input batch:\n\nfor i, word in enumerate(EXAMPLE_WORDS):\n    print(EXAMPLE_TOKENS.word_to_tokens(i), '-&gt;', word)\n\nYour first task is to code a function encode() that takes a tokeniser and a list of sentences and returns the tokeniser’s encoded input as well as the corresponding token spans. The following cell contains skeleton code for this function.\n\ndef encode(tokenizer, sentences):\n    # TODO: Replace the next line with your own code\n    raise NotImplementedError\n\nImplement this function to match the following specification:\nencode (tokenizer, sentences):\n\nUses the specified tokenizer to encode a batch of parsed sentences (sentences). Returns a pair consisting of a BatchEncoding and a matching batch of token spans (as explained above). The BatchEncoding is the standard batch encoding, but with tensors instead of lists of Python integers. Sentences have been truncated to the maximum acceptable input length.\n\n\n🤞 Test your code\nTo test you code, call encode() on a small number of sentences and check that output matches your expectations."
  },
  {
    "objectID": "assignments/assignment3/assignment3.html#problem-2-merging-tokens",
    "href": "assignments/assignment3/assignment3.html#problem-2-merging-tokens",
    "title": "Assignment 3: Dependency parsing",
    "section": "Problem 2: Merging tokens",
    "text": "Problem 2: Merging tokens\nDistilBERT gives us a representation for each token in a sentence. To compute scores between pairs of words, we will need to combine the token representations that correspond to each word. A standard strategy for this is to take their element-wise mean. The next cell contains skeleton code for a function merge_tokens() that implements this strategy.\n\ndef merge_tokens(tokens, token_spans):\n    # TODO: Replace the next line with your own code\n    raise NotImplementedError\n\nImplement this function to match the following specification:\nmerge_tokens (tokens, token_spans)\n\nTakes a batch of token vectors (tokens) and a list of matching token spans (token_spans) and returns a batch of word-level representations, computed using the element-wise mean. The token vectors are a tensor of shape (batch_size, num_tokens, hidden_dim), where hidden_dim is the dimensionality of the DistilBERT representations. The token spans are a nested list containing integer pairs, as computed in Problem 1. The result is a tensor of shape (batch_size, max_num_words, hidden_dim), where max_num_words denotes the maximum number of words in any sentence in the batch. Entries corresponding to padding are represented by the zero vector of size hidden_dim.\n\n\n🤞 Test your code\nTo test you code, create a sample input to merge_tokens() and check that the output matches your expectations."
  },
  {
    "objectID": "assignments/assignment3/assignment3.html#problem-3-biaffine-layer",
    "href": "assignments/assignment3/assignment3.html#problem-3-biaffine-layer",
    "title": "Assignment 3: Dependency parsing",
    "section": "Problem 3: Biaffine layer",
    "text": "Problem 3: Biaffine layer\nYour next task is to implement the bi-affine layer. Given matrices \\(X \\in \\mathbb{R}^{m \\times d}\\) and \\(X' \\in \\mathbb{R}^{n \\times d}\\), this layer computes a matrix \\(Y \\in \\mathbb{R}^{m \\times n}\\) as\n\\[\nY = X W X'{}^\\top + b\n\\]\nwhere \\(W \\in \\mathbb{R}^{d \\times d}\\) and \\(b \\in \\mathbb{R}\\) are learnable weight and bias parameters. In the context of the dependency parser, the matrices \\(X\\) and \\(X'\\) hold the word representations of all dependents and all heads in the input sentence, and the entries of the matrix \\(Y\\) are interpreted as scores of possible dependency arcs. More specifically, the entry \\(Y_{ij}\\) represents the score of an arc from a head word at position \\(j\\) to a dependent at position \\(i\\).\nThe following cell contains skeleton code for the implementation of the bi-affine layer. Implement this layer according to the specification above.\n\nimport torch.nn as nn\n\nclass Biaffine(nn.Module):\n\n    def __init__(self, encoder_dim):\n        super().__init__()\n        # TODO: Replace the next line with your own code\n        raise NotImplementedError\n\n    def forward(self, x1, x2):\n        # TODO: Replace the next line with your own code\n        raise NotImplementedError\n\n⚠️ Note that your implementation should be able to handle batches of input sentences.\n\n🤞 Test your code\nTo test you code, create a sample input to the bi-affine layer as well as suitable weights and biases and check that the output of the forward method matches your expectations"
  },
  {
    "objectID": "assignments/assignment3/assignment3.html#problem-4-parser",
    "href": "assignments/assignment3/assignment3.html#problem-4-parser",
    "title": "Assignment 3: Dependency parsing",
    "section": "Problem 4: Parser",
    "text": "Problem 4: Parser\nWe are now ready to put the two main components of the parser together: the encoder (DistilBert) and the bi-affine layer that computes the arc scores. We also add a dropout layer between the two components.\nThe following code cell contains skeleton code for the parsing model with the init() method already complete. Your task is to implement the forward() method. If you are unsure how things should be wired up, have another look at the slides.\n\nimport torch.nn as nn\nimport torch\n\nfrom transformers import DistilBertModel, DistilBertPreTrainedModel\n\nclass DistilBertForParsing(DistilBertPreTrainedModel):\n\n    def __init__(self, config, dropout=0.1):\n        super().__init__(config)\n        self.config = config\n        self.distilbert = DistilBertModel(config)\n        self.dropout = nn.Dropout(dropout)\n        self.biaffine = Biaffine(config.hidden_size)\n\n    def forward(self, encoded_input, token_spans):\n        # TODO: Replace the next line with your own code\n        raise NotImplementedError\n\nImplement the forward() method to match the following specification:\nforward (encoded_input, token_spans)\n\nTakes a tokeniser-encoded batch of sentences (encoded_input, of type BatchEncoding) and a corresponding nested list of token spans (token_spans) and returns a tensor with scores for each pair of words. More specifically, the output tensor \\(Y\\) has shape (batch_size, num_words, num_words+1), where the entry \\(Y_{bij}\\) represents the score of an arc from a head word at position \\(j\\) to a dependent at position \\(i\\) in the \\(b\\)th sentence of the batch. Note that the number of possible heads is one greater than the number of possible dependents because the possible heads include the root of the dependency tree, which we represent using the special token [CLS] (at position 0).\n\n\n🤞 Test your code\nTo test you code, instantiate the parsing model and feed it a small batch of sentences."
  },
  {
    "objectID": "assignments/assignment3/assignment3.html#data-loader",
    "href": "assignments/assignment3/assignment3.html#data-loader",
    "title": "Assignment 3: Dependency parsing",
    "section": "Data loader",
    "text": "Data loader\nWe are now almost ready to train the parser. The missing piece is a data collator that prepares a batch of parsed sentences:\n\ntokenises the sentences and extracts token spans using encode() (Problem 1)\nconstructs the ground-truth head tensor needed to compute the loss (Problem 2)\n\nThe code in the next cell implements these two steps. For pseudo-words introduced through padding, we assign a head position of −100. This value is ignored by PyTorch’s cross-entropy loss function.\n\nimport torch\n\nclass ParserBatcher(object):\n\n    def __init__(self, tokenizer, device=None):\n        self.tokenizer = tokenizer\n        self.device = device\n\n    def __call__(self, parser_inputs):\n        encoded_input, start_indices = encode(self.tokenizer, parser_inputs)\n\n        # Get the maximal number of words, for padding\n        max_num_words = max(len(s) for s in parser_inputs)\n\n        # Construct tensor containing the ground-truth heads\n        all_heads = []\n        for parser_input in parser_inputs:\n            words, heads = zip(*parser_input)\n            heads = list(heads)\n            heads.extend([-100] * (max_num_words - len(heads)))  # -100 will be ignored\n            all_heads.append(heads)\n        all_heads = torch.LongTensor(all_heads)\n\n        # Send all data to the specified device\n        if self.device:\n            encoded_input = encoded_input.to(self.device)\n            all_heads = all_heads.to(self.device)\n\n        return encoded_input, start_indices, all_heads"
  },
  {
    "objectID": "assignments/assignment3/assignment3.html#training-loop",
    "href": "assignments/assignment3/assignment3.html#training-loop",
    "title": "Assignment 3: Dependency parsing",
    "section": "Training loop",
    "text": "Training loop\nFinally, here is the training loop of the parser. Most of it is quite standard. The training loss of the parser is the cross-entropy between the head scores and the ground truth head positions. In other words, the parser is trained as a classifier that predicts the position of each word’s head.\n\nimport torch.nn.functional as F\n\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# DEVICE = torch.device('mps')\n\ndef train(dataset, n_epochs=1, lr=1e-5, batch_size=8):\n    # Initialise the tokeniser\n    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n\n    # Initialise the encoder\n    model = DistilBertForParsing.from_pretrained('distilbert-base-uncased').to(DEVICE)\n\n    # Initialise the data loader\n    data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=ParserBatcher(tokenizer, device=DEVICE))\n\n    # Initialise the optimiser\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    # Train for the specified number of epochs\n    for epoch in range(n_epochs):\n        model.train()\n\n        # We keep track of the running loss\n        running_loss = 0\n        n_batches = 0\n        with tqdm(total=len(dataset)) as pbar:\n            pbar.set_description(f'Epoch {epoch+1}')\n\n            # Process a batch of samples\n            for encoded_input, token_spans, gold_heads in data_loader:\n                optimizer.zero_grad()\n\n                # Compute the arc scores\n                arc_scores = model.forward(encoded_input, token_spans)\n                # shape: [batch_size, num_words, num_words+1]\n\n                # Flatten arc_scores and gold_heads for cross_entropy\n                loss = F.cross_entropy(arc_scores.flatten(0, -2), gold_heads.view(-1))\n                # shape of the flattened arc_scores: [batch_size * num_words, num_words+1]\n                # shape of the flattened gold_heads: [batch_size * num_words]\n\n                # Backward pass\n                loss.backward()\n                optimizer.step()\n\n                # Update the loss\n                running_loss += loss.item()\n                n_batches += 1\n                pbar.set_postfix(loss=running_loss/n_batches)\n                pbar.update(len(token_spans))\n\n    return model\n\nWe are now ready to train the parser. With a GPU, you should expect training times of approximately 3 minutes per epoch.\n\nPARSING_MODEL = train(TRAIN_DATA, n_epochs=1)"
  },
  {
    "objectID": "assignments/assignment3/assignment3.html#problem-5-evaluation",
    "href": "assignments/assignment3/assignment3.html#problem-5-evaluation",
    "title": "Assignment 3: Dependency parsing",
    "section": "Problem 5: Evaluation",
    "text": "Problem 5: Evaluation\nDependency parsers are commonly evaluated using unlabelled attachment score (UAS), which is the percentage of (non-root) words that have been assigned their correct heads. The following cell contains skeleton code for a function uas() that computes this score on a given dataset.\n\ndef uas(tokenizer, model, filename, batch_size=8):\n    # TODO: Replace the following line with your own code\n    return 0.0\n\nImplement the uas() function to match the following specification:\nuas (tokenizer, model, filename)\n\nTakes a tokenizer (tokenizer), a trained parsing model (model), and the filename of a dataset in the CoNLLU format (filename) and returns the unlabelled attachment score of the model on the tokenised dataset.\n\n⚠️ Note that pseudo-words corresponding to padding must be excluded from the calculation of the UAS.\nThe code in the following cell evaluates the trained parser on the development section of the data:\n\nuas(TOKENIZER, PARSING_MODEL, ParserDataset('en_ewt-ud-dev.conllu'))\n\nYour notebook must contain output demonstrating at least 86% UAS on the development data."
  },
  {
    "objectID": "assignments/assignment3/assignment3.html#problem-6-counting-trees",
    "href": "assignments/assignment3/assignment3.html#problem-6-counting-trees",
    "title": "Assignment 3: Dependency parsing",
    "section": "Problem 6: Counting trees",
    "text": "Problem 6: Counting trees\nIn the last problem of this assignment, we ask you to take a closer look at the graph-theoretic properties of the outputs of your trained model.\nThe following cell contains code that will use the tokeniser and your trained model to parse the development data and write the gold-standard dependency trees as well as the dependency trees predicted by the parser to a JSON file:\n\nimport json\n\ndataset = ParserDataset('en_ewt-ud-dev.conllu')\ndata_loader = DataLoader(dataset, collate_fn=ParserBatcher(TOKENIZER, device=DEVICE))\n\nwith open('graphs.jsonl', 'wt') as fp:\n    for encoded_input, token_spans, gold_heads in data_loader:\n        with torch.no_grad():\n            head_scores = PARSING_MODEL.forward(encoded_input, token_spans)\n            pred_heads = torch.argmax(head_scores, dim=-1)\n        for gold, pred in zip(gold_heads, pred_heads):\n            mask = gold.ne(-100)\n            print(json.dumps((gold[mask].tolist(), pred[mask].tolist())), file=fp)\n\nThe repo for this assignment contains a script analyze.py that you can use to compute statistics on various graph-theoretic properties: total number of graphs, number of acyclic graphs, connected graphs, trees, and projective trees. Document your exploration in a short reflection piece (ca. 150 words). Respond to the following prompts\n\nWhat are the results of this exploration? Summarise the statistics you obtained.\nBased on what you know about the parser, did you expect your results? Was there anything surprising in them?\nWhat did you learn? How, exactly, did you learn it? Why does this learning matter?"
  },
  {
    "objectID": "assignments/assignment2/Assignment2.html",
    "href": "assignments/assignment2/Assignment2.html",
    "title": "Assignment 2: Machine translation",
    "section": "",
    "text": "In this lab, you will implement the encoder–decoder architecture of Sutskever et al. (2014), including the attention-based extension of Bahdanau et al. (2015), and evaluate this architecture on a machine translation task.\nimport torch\nTraining the models in this notebook requires significant compute power, and we strongly recommend using a GPU.\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
  },
  {
    "objectID": "assignments/assignment2/Assignment2.html#the-data",
    "href": "assignments/assignment2/Assignment2.html#the-data",
    "title": "Assignment 2: Machine translation",
    "section": "The data",
    "text": "The data\nWe will build a system that translates from German (our source language) to English (our target language). The dataset is a collection of parallel English–German sentences taken from translations of subtitles for TED talks. It was derived from the TED2013 dataset, which is available in the OPUS collection. The code cell below prints the first lines in the training data:\n\nwith open('train-de.txt') as src, open('train-en.txt') as tgt:\n    for i, src_sentence, tgt_sentence in zip(range(5), src, tgt):\n        print(f'{i}: {src_sentence.rstrip()} / {tgt_sentence.rstrip()}')\n\nAs you can see, some ‘sentences’ are actually sequences of sentences, but we will use the term sentence nevertheless. All sentences are whitespace-tokenised and lowercased. To make your life a bit easier, we have removed sentences longer than 25 words.\nThe next cell contains code that yields the sentences contained in a file as lists of strings:\n\ndef sentences(filename):\n    with open(filename) as source:\n        for line in source:\n            yield line.rstrip().split()"
  },
  {
    "objectID": "assignments/assignment2/Assignment2.html#problem-1-build-the-vocabularies",
    "href": "assignments/assignment2/Assignment2.html#problem-1-build-the-vocabularies",
    "title": "Assignment 2: Machine translation",
    "section": "Problem 1: Build the vocabularies",
    "text": "Problem 1: Build the vocabularies\nYour first task is to build the vocabularies for the data, one vocabulary for each language. Each vocabulary should contain the 10,000 most frequent words in the training data for the respective language.\n\ndef make_vocab(sentences, max_size):\n    # TODO: Replace the next line with your own code\n    raise NotImplementedError\n\nYour implementation must comply with the following specification:\nmake_vocab (sentences, max_size)\n\nReturns a dictionary that maps the most frequent words in the sentences to a contiguous range of integers starting at 0. The first four mappings in this dictionary are reserved for the pseudowords &lt;pad&gt; (padding, id 0), &lt;bos&gt; (beginning of sequence, id 1), &lt;eos&gt; (end of sequence, id 2), and &lt;unk&gt; (unknown word, id 3). The parameter max_size caps the size of the dictionary, including the pseudowords.\n\nWith this function, we can construct the vocabularies as follows:\n\nsrc_vocab = make_vocab(sentences('train-de.txt'), 10000)\ntgt_vocab = make_vocab(sentences('train-en.txt'), 10000)\n\n\n🤞 Test your code\nTo test you code, check that each vocabulary contains 10,000 words, including the pseudowords."
  },
  {
    "objectID": "assignments/assignment2/Assignment2.html#load-the-data",
    "href": "assignments/assignment2/Assignment2.html#load-the-data",
    "title": "Assignment 2: Machine translation",
    "section": "Load the data",
    "text": "Load the data\nThe next cell defines a class for the parallel dataset. We sub-class the abstract Dataset class, which represents map-style datasets in PyTorch. This will let us use standard infrastructure related to the loading and automatic batching of data.\n\nfrom torch.utils.data import Dataset\n\nclass TranslationDataset(Dataset):\n\n    def __init__(self, src_vocab, src_filename, tgt_vocab, tgt_filename):\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n\n        # We hard-wire the codes for &lt;bos&gt; (1), &lt;eos&gt; (2), and &lt;unk&gt; (3).\n        self.src = [[self.src_vocab.get(w, 3) for w in s] for s in sentences(src_filename)]\n        self.tgt = [[self.tgt_vocab.get(w, 3) for w in s] + [2] for s in sentences(tgt_filename)]\n\n    def __getitem__(self, idx):\n        return self.src[idx], self.tgt[idx]\n\n    def __len__(self):\n        return len(self.src)\n\nWe load the training data:\n\ntrain_dataset = TranslationDataset(src_vocab, 'train-de.txt', tgt_vocab, 'train-en.txt')\n\nThe following function will be helpful for debugging. It extracts a single source–target pair of sentences from the specified dataset and converts it into batches of size 1, which can be fed into the encoder–decoder model.\n\ndef example(dataset, i):\n    src, tgt = dataset[i]\n    return torch.LongTensor(src).unsqueeze(0), torch.LongTensor(tgt).unsqueeze(0)\n\n\nexample(train_dataset, 0)"
  },
  {
    "objectID": "assignments/assignment2/Assignment2.html#problem-2-the-encoderdecoder-architecture",
    "href": "assignments/assignment2/Assignment2.html#problem-2-the-encoderdecoder-architecture",
    "title": "Assignment 2: Machine translation",
    "section": "Problem 2: The encoder–decoder architecture",
    "text": "Problem 2: The encoder–decoder architecture\nIn this section, you will implement the encoder–decoder architecture, including the extension of that architecture by an attention mechanism. The implementation consists of four parts: the encoder, the attention mechanism, the decoder, and a class that wraps the complete architecture.\n\nProblem 2.1: Implement the encoder\nThe encoder is relatively straightforward. We look up word embeddings and unroll a bidirectional GRU over the embedding vectors to compute a representation at each token position. We then take the last hidden state of the forward GRU and the last hidden state of the backward GRU, concatenate them, and pass them through a linear layer. This produces a summary of the source sentence, which we will later feed into the decoder.\nTo solve this problem, complete the skeleton code in the next code cell:\n\nimport torch.nn as nn\n\nclass Encoder(nn.Module):\n\n    def __init__(self, num_words, embedding_dim=256, hidden_dim=512):\n        super().__init__()\n        # TODO: Add your code here\n\n    def forward(self, src):\n        # TODO: Replace the next line with your own code\n        raise NotImplementedError\n\nYour code must comply with the following specification:\ninit (num_words, embedding_dim = 256, hidden_dim = 512)\n\nInitialises the encoder. The encoder consists of an embedding layer that maps each of num_words words to an embedding vector of size embedding_dim, a bidirectional GRU that maps each embedding vector to a position-specific representation of size 2 × hidden_dim, and a final linear layer that projects the concatenation of the final hidden states of the GRU (the final hidden state of the forward direction and the final hidden state of the backward direction) to a single vector of size hidden_dim.\n\nforward (self, src)\n\nTakes a tensor src with source-language word ids and sends it through the encoder. The input tensor has shape (batch_size, src_len), where src_len is the length of the sentences in the batch. (We will make sure that all sentences in the same batch have the same length.) The method returns a pair of tensors (output, hidden), where output has shape (batch_size, src_len, 2 × hidden_dim), and hidden has shape (batch_size, hidden_dim).\n\n\n\n🤞 Test your code\nTo test your code, instantiate an encoder, feed it the first source sentence in the training data, and check that the tensors returned by the encoder have the expected shapes.\n\n\nProblem 2.2: Implement the attention mechanism\nYour next task is to implement the attention mechanism. Recall that the purpose of this mechanism is to inform the decoder when generating the translation of the next word. For this, attention has access to the previous hidden state of the decoder, as well as the complete output of the encoder. It returns the attention-weighted sum of the encoder output, the so-called context vector. For later usage, we also return the attention weights.\nAs mentioned in the lecture, attention can be implemented in various ways. One very simple implementation is uniform attention, which assigns equal weight to each position-specific representation in the output of the encoder, and completely ignores the hidden state of the decoder. This mechanism is implemented in the cell below.\n\nimport torch.nn.functional as F\n\nclass UniformAttention(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, decoder_hidden, encoder_output, src_mask):\n        batch_size, src_len, _ = encoder_output.shape\n\n        # Set all attention scores to the same constant value (0). After\n        # the softmax, we will have uniform weights.\n        scores = torch.zeros(batch_size, src_len, device=encoder_output.device)\n\n        # Mask out the attention scores for the padding tokens. We set\n        # them to -inf. After the softmax, we will have 0.\n        scores.data.masked_fill_(~src_mask, -float('inf'))\n\n        # Convert scores into weights\n        alpha = F.softmax(scores, dim=1)\n\n        # The context is the alpha-weighted sum of the encoder outputs.\n        context = torch.bmm(alpha.unsqueeze(1), encoder_output).squeeze(1)\n\n        return context, alpha\n\nOne technical detail in this code is our use of a mask src_mask to compute attention weights only for the ‘real’ tokens in the source sentences, but not for the padding tokens that we introduce to bring all sentences in a batch to the same length. Your task now is to implement the attention mechanism from the paper by Bahdanau et al. (2015). The relevant equation is in Section A.1.2.\nHere is the skeleton code for this problem. As you can see, your specific task is to initialise the required parameters and to compute the attention scores (scores); the rest of the code is the same as for the uniform attention.\n\nclass BahdanauAttention(nn.Module):\n\n    def __init__(self, hidden_dim=512):\n        super().__init__()\n        # TODO: Add your code here\n\n    def forward(self, decoder_hidden, encoder_output, src_mask):\n        batch_size, src_len, _ = encoder_output.shape\n\n        # TODO: Replace the next line with your own code\n        scores = torch.zeros(batch_size, src_len, device=encoder_output.device)\n\n        # The rest of the code is as in UniformAttention\n\n        # Mask out the attention scores for the padding tokens. We set\n        # them to -inf. After the softmax, we will have 0.\n        scores.data.masked_fill_(~src_mask, -float('inf'))\n\n        # Convert scores into weights\n        alpha = F.softmax(scores, dim=1)\n\n        # The context vector is the alpha-weighted sum of the encoder outputs.\n        context = torch.bmm(alpha.unsqueeze(1), encoder_output).squeeze(1)\n\n        return context, alpha\n\nYour code must comply with the following specification:\nforward (decoder_hidden, encoder_output, src_mask)\n\nTakes the previous hidden state of the decoder (decoder_hidden) and the encoder output (encoder_output) and returns a pair (context, alpha) where context is the context as computed as in Bahdanau et al. (2015), and alpha are the corresponding attention weights. The hidden state has shape (batch_size, hidden_dim), the encoder output has shape (batch_size, src_len, 2 × hidden_dim), the context has shape (batch_size, 2 × hidden_dim), and the attention weights have shape (batch_size, src_len).\n\n\n\n🤞 Test your code\nTo test your code, extend your test from Problem 2.1: Feed the output of your encoder into your attention class. As the previous hidden state of the decoder, you can use the hidden state returned by the encoder. You will also need to create a source mask; this can be done as follows:\nsrc_mask = (src != 0)\nCheck that the context tensor and the attention weights returned by the attention class have the expected shapes.\n\n\nProblem 2.3: Implement the decoder\nNow you are ready to implement the decoder. Like the encoder, the decoder is based on a GRU; but this time we use a unidirectional network, as we generate the target sentences left-to-right.\n⚠️ We expect that solving this problem will take you the longest time in this lab.\nBecause the decoder is an autoregressive model, we need to unroll the GRU ‘manually’: At each position, we take the previous hidden state as well as the new input, and apply the GRU for one step. The initial hidden state comes from the encoder. The new input is the embedding of the previous word, concatenated with the context vector from the attention model. To produce the final output, we take the output of the GRU, concatenate the embedding vector and the context vector (residual connection), and feed the result into a linear layer. Here is a graphical representation:\n\nWe need to implement this manual unrolling for two very similar tasks: When training, both the inputs to and the target outputs of the GRU come from the training data. When decoding, the outputs of the GRU are used to generate new target-side words, and these words become the inputs to the next step of the unrolling. We have implemented methods forward and decode for these two different modes of usage. Your task is to implement a method step that takes a single step with the GRU.\n\nclass Decoder(nn.Module):\n\n    def __init__(self, num_words, attention, embedding_dim=256, hidden_dim=512):\n        super().__init__()\n        self.embedding = nn.Embedding(num_words, embedding_dim)\n        self.attention = attention\n        # TODO: Add your own code\n\n    def forward(self, encoder_output, hidden, src_mask, tgt):\n        batch_size, tgt_len = tgt.shape\n\n        # Lookup the embeddings for the previous words\n        embedded = self.embedding(tgt)\n\n        # Initialise the list of outputs (in each sentence)\n        outputs = []\n\n        for i in range(tgt_len):\n            # Get the embedding for the previous word (in each sentence)\n            prev_embedded = embedded[:, i]\n\n            # Take one step with the RNN\n            output, hidden, alpha = self.step(encoder_output, hidden, src_mask, prev_embedded)\n\n            # Update the list of outputs (in each sentence)\n            outputs.append(output.unsqueeze(1))\n\n        return torch.cat(outputs, dim=1)\n\n    def decode(self, encoder_output, hidden, src_mask, max_len):\n        batch_size = encoder_output.size(0)\n\n        # Initialise the list of generated words and attention weights (in each sentence)\n        generated = [torch.ones(batch_size, dtype=torch.long, device=hidden.device)]\n        alphas = []\n\n        for i in range(max_len):\n            # Get the embedding for the previous word (in each sentence)\n            prev_embedded = self.embedding(generated[-1])\n\n            # Take one step with the RNN\n            output, hidden, alpha = self.step(encoder_output, hidden, src_mask, prev_embedded)\n\n            # Update the list of generated words and attention weights (in each sentence)\n            generated.append(output.argmax(-1))\n            alphas.append(alpha)\n\n        generated = [x.unsqueeze(1) for x in generated[1:]]\n        alphas = [x.unsqueeze(1) for x in alphas]\n            \n        return torch.cat(generated, dim=1), torch.cat(alphas, dim=1)\n\n    def step(self, encoder_output, hidden, src_mask, prev_embedded):\n        # TODO: Replace the next line with your own code\n        raise NotImplementedError\n\nYour implementation should comply with the following specification:\nstep (self, encoder_output, hidden, src_mask, prev_embedded)\n\nPerforms a single step in the manual unrolling of the decoder GRU. This takes the output of the encoder (encoder_output), the previous hidden state of the decoder (hidden), the source mask as described in Problem 2.2 (src_mask), and the embedding vector of the previous word (prev_embedded), and computes the output as described above.\nThe shape of encoder_output is (batch_size, src_len, 2 × hidden_dim); the shape of hidden is (batch_size, hidden_dim); the shape of src_mask is (batch_size, src_len); and the shape of prev_embedded is (batch_size, embedding_dim).\nThe method returns a triple of tensors (output, hidden, alpha) where output is the position-specific output of the GRU, of shape (batch_size, num_words); hidden is the new hidden state, of shape (batch_size, hidden_dim); and alpha are the attention weights that were used to compute the output, of shape (batch_size, src_len).\n\n\n💡 Hints on the implementation\nBatch first! Per default, the GRU implementation in PyTorch (just as the LSTM implementation) expects its input to be a three-dimensional tensor of the form (seq_len, batch_size, input_size). We find it conceptually easier to change this default behaviour and let the models take their input in the form (batch_size, seq_len, input_size). To do so, set batch_first=True when instantiating the GRU.\nUnsqueeze and squeeze. When doing the unrolling manually, we get the input in the form (batch_size, input_size). To convert between this representation and the (batch_size, seq_len, input_size) representation, you can use unsqueeze and squeeze.\n\n\n\n🤞 Test your code\nTo test your code, extend your test from the previous problems, and simulate a complete forward pass of the encoder–decoder architecture on the example sentence. Check the shapes of the resulting tensors.\n\n\nEncoder–decoder wrapper class\nThe last part of the implementation is a class that wraps the encoder and the decoder as a single model:\n\nclass EncoderDecoder(nn.Module):\n\n    def __init__(self, src_vocab_size, tgt_vocab_size, attention):\n        super().__init__()\n        self.encoder = Encoder(src_vocab_size)\n        self.decoder = Decoder(tgt_vocab_size, attention)\n\n    def forward(self, src, tgt):\n        encoder_output, hidden = self.encoder(src)\n        return self.decoder.forward(encoder_output, hidden, src != 0, tgt)\n\n    def decode(self, src, max_len):\n        encoder_output, hidden = self.encoder(src)\n        return self.decoder.decode(encoder_output, hidden, src != 0, max_len)\n\n\n\n🤞 Test your code\nAs a final test, instantiate an encoder–decoder model and use it to decode the example sentence. Check the shapes of the resulting tensors."
  },
  {
    "objectID": "assignments/assignment2/Assignment2.html#problem-3-train-a-translator",
    "href": "assignments/assignment2/Assignment2.html#problem-3-train-a-translator",
    "title": "Assignment 2: Machine translation",
    "section": "Problem 3: Train a translator",
    "text": "Problem 3: Train a translator\nWe now have all the pieces to build and train a complete translation system.\n\nTranslator class\nWe first define a class Translator that initialises an encoder–decoder model and uses it to translate sentences. It can also return the attention weights that were used to produce the translation of each sentence.\n\nclass Translator(object):\n\n    def __init__(self, src_vocab, tgt_vocab, attention, device=torch.device('cpu')):\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n        self.device = device\n        self.model = EncoderDecoder(len(src_vocab), len(tgt_vocab), attention).to(device)\n\n    def translate_with_attention(self, sentences):\n        # Encode each sentence\n        encoded = [[self.src_vocab.get(w, 3) for w in s.split()] for s in sentences]\n\n        # Determine the maximal length of an encoded sentence\n        max_len = max(len(e) for e in encoded)\n\n        # Build the input tensor, padding all sequences to the same length\n        src = torch.LongTensor([e + [0] * (max_len - len(e)) for e in encoded]).to(self.device)\n\n        # Run the decoder and convert the result into nested lists\n        with torch.no_grad():\n            decoded, alphas = tuple(d.cpu().numpy().tolist() for d in self.model.decode(src, 2 * max_len))\n\n        # Prune each decoded sentence after the first &lt;eos&gt;\n        i2w = {i: w for w, i in self.tgt_vocab.items()}\n        result = []\n        for d, a in zip(decoded, alphas):\n            d = [i2w[i] for i in d]\n            try:\n                eos_index = d.index('&lt;eos&gt;')\n                del d[eos_index:]\n                del a[eos_index:]\n            except:\n                pass\n            result.append((' '.join(d), a))\n\n        return result\n\n    def translate(self, sentences):\n        translated, alphas = zip(*self.translate_with_attention(sentences))\n        return translated\n\nThe code below shows how this class is supposed to be used:\n\ntranslator = Translator(src_vocab, tgt_vocab, BahdanauAttention())\ntranslator.translate(['ich weiß nicht .', 'das haus ist klein .'])\n\n\n\nEvaluation function\nAs mentioned in the lectures, machine translation systems are typically evaluated using the BLEU metric. Here we use the implementation of this metric from the sacrebleu library.\n\n# If sacrebleu is not found, uncomment the next line:\n# !pip install sacrebleu\n\nimport sacrebleu\n\ndef bleu(translator, src, ref):\n    translated = translator.translate(src)\n    return sacrebleu.raw_corpus_bleu(translated, [ref], 0.01).score\n\nWe will report the BLEU score on the validation data:\n\nwith open('valid-de.txt') as src, open('valid-en.txt') as ref:\n    valid_src = [line.rstrip() for line in src]\n    valid_ref = [line.rstrip() for line in ref]\n\n\n\nBatcher class\nTo prepare the training, we next create a class that takes a batch of encoded parallel sentences (a pair of lists of integers) and transforms it into two tensors, one for the source side and one for the target side. Each tensor contains sequences padded to the length of the longest sequence.\n\nclass TranslationBatcher(object):\n\n    def __init__(self, device):\n        self.device = device\n\n    def __call__(self, batch):\n        srcs, tgts = zip(*batch)\n\n        # Determine the maximal length of a source/target sequence\n        max_src_len = max(len(s) for s in srcs)\n        max_tgt_len = max(len(t) for t in tgts)\n\n        # Create the source/target tensors\n        S = torch.LongTensor([s + [0] * (max_src_len - len(s)) for s in srcs])\n        T = torch.LongTensor([t + [0] * (max_tgt_len - len(t)) for t in tgts])\n\n        return S.to(self.device), T.to(self.device)\n\n\n\nTraining loop\nThe training loop is pretty standard. We use a few new utilities from the PyTorch ecosystem.\n\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndef train(n_epochs=2, batch_size=128, lr=5e-4):\n    # Build the vocabularies\n    vocab_src = make_vocab(sentences('train-de.txt'), 10000)\n    vocab_tgt = make_vocab(sentences('train-en.txt'), 10000)\n\n    # Prepare the dataset\n    train_dataset = TranslationDataset(vocab_src, 'train-de.txt', vocab_tgt, 'train-en.txt')\n\n    # Prepare the data loaders\n    batcher = TranslationBatcher(device)\n    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=batcher)\n\n    # Build the translator\n    translator = Translator(src_vocab, tgt_vocab, BahdanauAttention(), device=device)\n\n    # Initialise the optimiser\n    optimizer = torch.optim.Adam(translator.model.parameters(), lr=lr)\n\n    # Make it possible to interrupt the training\n    try:\n        for epoch in range(n_epochs):\n            losses = []\n            bleu_valid = 0\n            sample = '&lt;none&gt;'\n            with tqdm(total=len(train_dataset)) as pbar:\n                for i, (src_batch, tgt_batch) in enumerate(train_loader):\n                    # Create a shifted version of tgt_batch containing the previous words\n                    batch_size, tgt_len = tgt_batch.shape\n                    bos = torch.ones(batch_size, 1, dtype=torch.long, device=tgt_batch.device)\n                    tgt_batch_shifted = torch.cat((bos, tgt_batch[:, :-1]), dim=1)\n\n                    translator.model.train()\n\n                    # Forward pass\n                    scores = translator.model(src_batch, tgt_batch_shifted)\n                    scores = scores.view(-1, len(tgt_vocab))\n\n                    # Backward pass\n                    optimizer.zero_grad()\n                    loss = F.cross_entropy(scores, tgt_batch.view(-1), ignore_index=0)\n                    loss.backward()\n                    optimizer.step()\n\n                    # Update the diagnostics\n                    losses.append(loss.item())\n                    pbar.set_postfix(loss=(sum(losses) / len(losses)), bleu_valid=bleu_valid, sample=sample)\n                    pbar.update(len(src_batch))\n\n                    if i % 50 == 0:\n                        translator.model.eval()\n                        bleu_valid = int(bleu(translator, valid_src, valid_ref))\n                        sample = translator.translate(['das haus ist klein .'])[0]\n\n    except KeyboardInterrupt:\n        pass\n\n    return translator\n\nNow it is time to train the system. During training, two diagnostics will be printed periodically: the running average of the training loss, the BLEU score on the validation data, and the translation of a sample sentence, das haus ist klein (which should translate into the house is small).\nAs mentioned before, training the translator takes quite a bit of compute power and time. Even with a GPU, you should expect training times per epoch of about 5–6 minutes. The default number of epochs is 2; however, you may want to interrupt the training prematurely and use a partially trained model in case you run out of time.\n\ntranslator = train()\n\n⚠️ Your submitted notebook must contain output demonstrating at least 16 BLEU points on the validation data."
  },
  {
    "objectID": "assignments/assignment2/Assignment2.html#problem-4-visualising-attention-reflection",
    "href": "assignments/assignment2/Assignment2.html#problem-4-visualising-attention-reflection",
    "title": "Assignment 2: Machine translation",
    "section": "Problem 4: Visualising attention (reflection)",
    "text": "Problem 4: Visualising attention (reflection)\nFigure 3 in the paper by Bahdanau et al. (2015) shows some heatmaps of attention weights in selected sentences. In the last problem of this lab, we ask you to inspect attention weights for your trained translation system. We define a function plot_attention that visualizes the attention weights. The x axis corresponds to the words in the source sentence (German) and the y axis to the generated target sentence (English). The heatmap colors represent the strengths of the attention weights.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%config InlineBackend.figure_format = 'svg'\n\nplt.style.use('seaborn')\n\ndef plot_attention(translator, sentence):\n    translation, weights = translator.translate_with_attention([sentence])[0]\n    weights = np.array(weights)\n\n    fig, ax = plt.subplots()\n    heatmap = ax.pcolor(weights, cmap='Blues_r')\n\n    ax.set_xticklabels(sentence.split(), minor=False, rotation='vertical')\n    ax.set_yticklabels(translation.split(), minor=False)\n\n    ax.xaxis.tick_top()\n    ax.set_xticks(np.arange(weights.shape[1]) + 0.5, minor=False)\n    ax.set_yticks(np.arange(weights.shape[0]) + 0.5, minor=False)\n    ax.invert_yaxis()\n\n    plt.colorbar(heatmap)\n\nHere is an example:\n\nplot_attention(translator, 'das haus ist klein .')\n\nUse these heatmaps to inspect the attention patterns for selected German sentences. Try to find sentences for which the model produces reasonably good English translations. If your German is a bit rusty (or non-existent), use sentences from the validation data. It might be interesting to look at examples where the German and the English word order differ substantially. Document your exploration in a short reflection piece (ca. 150 words). Respond to the following prompts:\n\nWhat sentences did you try out? What patterns did you spot? Include example heatmaps in your notebook.\nBased on what you know about attention, did you expect your results? Was there anything surprising in them?"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html",
    "href": "assignments/assignment1/assignment1.html",
    "title": "Assignment 1: Language modelling",
    "section": "",
    "text": "In this assignment you will implement and train two or three neural language models: the fixed-window model, the recurrent neural network model from Unit 1-2, and optionally a model based on the Transformer architecture from Unit 1-3. You will evaluate these models by computing their perplexity on a benchmark dataset.\nimport torch\nFor this lab, you should use the GPU if you have one:\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    # NVIDIA\n# device = torch.device('mps')    # Apple Silicon"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#data",
    "href": "assignments/assignment1/assignment1.html#data",
    "title": "Assignment 1: Language modelling",
    "section": "Data",
    "text": "Data\nThe data for this assignment is WikiText, a collection of more than 100 million tokens extracted from the “Good” and “Featured” articles on Wikipedia. We will use the small version of the dataset, which contains slightly more than 2.5 million tokens.\nThe next cell contains code for an object that will act as a container for the “training” and the “validation” section of the data. We fill this container by reading the corresponding text files. The only processing we do is to whitespace-tokenise and to replace each newline with an end-of-sentence token.\n\nclass WikiText(object):\n\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n        self.train = self.read_data('wiki.train.tokens')\n        self.valid = self.read_data('wiki.valid.tokens')\n\n    def read_data(self, path):\n        ids = []\n        with open(path, encoding='utf-8') as source:\n            for line in source:\n                for word in line.split() + ['&lt;eos&gt;']:\n                    if word not in self.word2idx:\n                        self.word2idx[word] = len(self.word2idx)\n                        self.idx2word.append(word)\n                    ids.append(self.word2idx[word])\n        return ids\n\nThe cell below loads the data and prints the total number of tokens and the size of the vocabulary.\n\nwikitext = WikiText()\n\nprint('Tokens in train:', len(wikitext.train))\nprint('Tokens in valid:', len(wikitext.valid))\nprint('Vocabulary size:', len(wikitext.word2idx))\n\nTokens in train: 2088628\nTokens in valid: 217646\nVocabulary size: 33278"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#problem-1-fixed-window-model",
    "href": "assignments/assignment1/assignment1.html#problem-1-fixed-window-model",
    "title": "Assignment 1: Language modelling",
    "section": "Problem 1: Fixed-window model",
    "text": "Problem 1: Fixed-window model\nIn this section, you will implement and train the fixed-window neural language model proposed by Bengio et al. (2003) and presented in the lectures. Recall that an input to the network takes the form of a vector of \\(n-1\\) integers representing the preceding words. Each integer is mapped to a vector via an embedding layer. (All positions share the same embedding.) The embedding vectors are then concatenated and sent through a two-layer feed-forward network with a non-linearity in the form of a rectified linear unit (ReLU) and a final softmax layer.\n\nProblem 1.1: Vectorise the data\nYour first task is to write code for transforming the data in the WikiText container into a vectorised form that can be fed to the fixed-window model. Concretely, you will implement a collate function in the form of a callable vectoriser object. Complete the skeleton code in the cell below:\n\nclass FixedWindowVectorizer(object):\n    def __init__(self, n):\n        self.n = n\n\n    def __call__(self, data):\n        # TODO: Replace the following line with your own code\n        return None, None\n\nYour code should implement the following specification:\ninit (self, n)\n\nCreates a new vectoriser with n-gram order \\(n\\). Your code should be able to handle arbitrary n-gram orders \\(n \\geq 1\\).\n\ncall (self, data)\n\nTransforms WikiText data (a list of word ids) into a pair of tensors \\(\\mathbf{X}\\), \\(\\mathbf{y}\\) that can be used to train the fixed-window model. Let \\(N\\) be the total number of \\(n\\)-grams from the token list; then \\(\\mathbf{X}\\) is a matrix with shape \\((N, n-1)\\) and \\(\\mathbf{y}\\) is a vector with length \\(N\\).\n\n\n🤞 Test your code\nTest your implementation by running the code in the next cell. Does the output match your expectation?\n\nvalid_x, valid_y = FixedWindowVectorizer(3)(wikitext.valid)\n\nprint(valid_x.size(), valid_y.size())\n\ntorch.Size([217644, 2]) torch.Size([217644])\n\n\n\n\n\nProblem 1.2: Implement the model\nYour next task is to implement the fixed-window model based on the graphical specification given in the lecture.\n\nimport torch.nn as nn\n\nclass FixedWindowModel(nn.Module):\n\n    def __init__(self, n, n_words, embedding_dim=64, hidden_dim=64):\n        super().__init__()\n        # TODO: Add your own code\n\n    def forward(self, x):\n        # TODO: Replace the next line with your own code\n        raise NotImplemented\n\nHere is the specification of the two methods:\ninit (self, n, n_words, embedding_dim=64, hidden_dim=64)\n\nCreates a new fixed-window neural language model. The argument n specifies the model’s \\(n\\)-gram order. The argument n_words is the number of words in the vocabulary. The arguments embedding_dim and hidden_dim specify the dimensionalities of the embedding layer and the hidden layer of the feedforward network, respectively; their default value is 64.\n\nforward (self, x)\n\nComputes the network output on an input batch x. The shape of x is \\((B, n-1)\\), where \\(B\\) is the batch size. The output of the forward pass is a tensor of shape \\((B, V)\\) where \\(V\\) is the number of words in the vocabulary.\n\n\n🤞 Test your code\nTest your code by instantiating the model and feeding it a batch of examples from the training data.\n\n\n\nProblem 1.3: Train the model\nNext, write code to train the fixed-window model using minibatch gradient descent and the cross-entropy loss function. This should be a straightforward generalisation of the training loops that you have seen so far. Complete the skeleton code in the cell below:\n\ndef train_fixed_window(n, n_epochs=2, batch_size=3072, lr=1e-2):\n    # TODO: Replace the following line with your own code\n    return None\n\nHere is the specification of the training function:\ntrain_fixed_window (n, n_epochs = 2, batch_size = 3072, lr = 0.01)\n\nTrains a fixed-window neural language model of order n using minibatch gradient descent and returns it. The parameters n_epochs and batch_size specify the number of training epochs and the minibatch size, respectively. Training uses the cross-entropy loss function and the Adam optimizer with learning rate lr. After each epoch, prints the perplexity of the model on the validation data.\n\nThe code in the cell below trains a trigram model.\n\nmodel_fixed_window = train_fixed_window(3, n_epochs=1)\n\nEpoch 1: 100%|█| 2087268/2087268 [00:41&lt;00:00, 49905.93it/s, loss=5.89, ppl=363]\n\n\nPerplexity after epoch 1: 321\n\n\n\nPerformance goal\nYour submitted notebook must contain output demonstrating a validation perplexity of at most 360 after training for two epochs with the default parameters.\n⚠️ Computing the validation perplexity in one go (for the full validation set) will most probably exhaust your computer’s memory and/or take a lot of time. Instead, do the computation at the minibatch level and aggregate the results.\n\n\n🤞 Test your code\nTo see whether your network is learning something, print or plot the loss and/or the perplexity on the training data. If the two values do not decrease during training, try to find the problem before wasting time (and electricity) on useless computation.\nTraining and even evaluation will take some time – on a CPU, you should expect several minutes per epoch, depending on hardware. Our reference implementation uses a GPU and runs in 45 seconds on a MacBook Pro (2023)."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#problem-2-recurrent-neural-network-model",
    "href": "assignments/assignment1/assignment1.html#problem-2-recurrent-neural-network-model",
    "title": "Assignment 1: Language modelling",
    "section": "Problem 2: Recurrent neural network model",
    "text": "Problem 2: Recurrent neural network model\nIn this section, you will implement the recurrent neural network language model. Recall that an input to this model is a vector of word ids. Each integer is mapped to an embedding vector. The sequence of embedded vectors is then fed into an unrolled LSTM. At each position \\(i\\) in the sequence, the hidden state of the LSTM at that position is sent through a linear transformation into a final softmax layer representing the probability distribution over the words at position \\(i+1\\). In theory, the input vector could represent the complete training data; for practical reasons, however, we will truncate the input to some fixed value bptt_len. This length is called the backpropagation-through-time horizon.\n\nProblem 2.1: Vectorise the data\nAs in the previous problem, your first task is to transform the data in the WikiText container into a vectorised form that can be fed to the model.\n\nclass RNNVectorizer(object):\n    def __init__(self, bptt_len):\n        self.bptt_len = bptt_len\n\n    def __call__(self, data):\n        # TODO: Replace the following line with your own code\n        return None, None\n\nYour vectoriser should meet the following specification:\ninit (self, bptt_len)\n\nCreates a new vectoriser. The parameter bptt_len specifies the backpropagation-through-time horizon.\n\ncall (self, data)\n\nTransforms a list of token indexes data into a pair of tensors \\(\\mathbf{X}\\), \\(\\mathbf{Y}\\) that can be used to train the recurrent neural language model. The rows of both tensors represent contiguous subsequences of token indexes of length bptt_len. Compared to the sequences in \\(\\mathbf{X}\\), the corresponding sequences in \\(\\mathbf{Y}\\) are shifted one position to the right. More precisely, if the \\(i\\)th row of \\(\\mathbf{X}\\) is the sequence that starts at token position \\(j\\), then the same row of \\(\\mathbf{Y}\\) is the sequence that starts at position \\(j+1\\).\n\n\n🤞 Test your code\nTest your implementation by running the following code:\n\nvalid_x, valid_y = RNNVectorizer(32)(wikitext.valid)\n\nprint(valid_x.size(), valid_y.size())\n\ntorch.Size([6801, 32]) torch.Size([6801, 32])\n\n\n\n\n\nProblem 2.2: Implement the model\nYour next task is to implement the recurrent neural network model based on the graphical specification.\n\nimport torch.nn as nn\n\nclass RNNModel(nn.Module):\n    \n    def __init__(self, n_words, embedding_dim=64, hidden_dim=64):\n        super().__init__()\n        # TODO: Add your own code\n\n    def forward(self, x):\n        # TODO: Replace the next line with your own code\n        raise NotImplemented\n\nYour implementation should follow this specification:\ninit (self, n_words, embedding_dim = 64, hidden_dim = 64)\n\nCreates a new recurrent neural network language model based on an LSTM. The argument n_words is the number of words in the vocabulary. The arguments embedding_dim and hidden_dim specify the dimensionalities of the embedding layer and the LSTM hidden layer, respectively; their default value is 64.\n\nforward (self, x)\n\nComputes the network output on an input batch x. The shape of x is \\((B, H)\\), where \\(B\\) is the batch size and \\(H\\) is the length of each input sequence. The shape of the output tensor is \\((B, H, V)\\), where \\(V\\) is the size of the vocabulary.\n\n\n🤞 Test your code\nTest your code by instantiating the model and feeding it a batch of examples from the training data.\n\n\n\nProblem 2.3: Train the model\nThe training loop for the recurrent neural network model is essentially identical to the loop that you wrote for the feed-forward model. The only thing to note is that the cross-entropy loss function expects its input to be a two-dimensional tensor; you will therefore have to re-shape the output tensor from the LSTM as well as the gold-standard output tensor in a suitable way. The most efficient way to do so is to use the view() method.\n\ndef train_rnn(n_epochs=2, batch_size=3072, bptt_len=32, lr=1e-2):\n    # TODO: Replace the next line with your own code\n    return None\n\nHere is the specification of the training function:\ntrain_rnn (n_epochs = 2, batch_size = 3072, bptt_len = 32, lr = 0.01)\n\nTrains a recurrent neural network language model on the WikiText data using minibatch gradient descent and returns it. The parameters n_epochs and batch_size specify the number of training epochs and the minibatch size, respectively. The parameter bptt_len specifies the length of the backpropagation-through-time horizon, that is, the length of the input and output sequences. Training uses the cross-entropy loss function and the Adam optimizer with learning rate lr. After each epoch, prints the perplexity of the model on the validation data.\n\nEvaluate your model by running the following code cell:\n\nmodel_rnn = train_rnn(n_epochs=1)\n\nEpoch 1: 100%|████| 127820/127820 [00:57&lt;00:00, 2241.17it/s, loss=5.91, ppl=368]\n\n\nPerplexity after epoch 1: 288\n\n\n\nPerformance goal\nYour submitted notebook must contain output demonstrating a validation perplexity of at most 280 after training for two epochs with the default hyperparameters."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#problem-3-transformer-model-optional",
    "href": "assignments/assignment1/assignment1.html#problem-3-transformer-model-optional",
    "title": "Assignment 1: Language modelling",
    "section": "Problem 3: Transformer model (optional)",
    "text": "Problem 3: Transformer model (optional)\nIf you are up for a challenge, try implementing a Transformer-based language model. The required vectoriser is identical to the vectoriser for the RNN model. For the model itself, you can use the Pytorch modules nn.TransformerEncoder and nn.TransformerEncoderLayer. To represent positional information, follow the approach from the original Transformer paper and use sine and cosine functions of different frequencies (details), or learn position-specific embeddings. Can you get a lower perplexity than for the RNN model?"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#problem-4-generation",
    "href": "assignments/assignment1/assignment1.html#problem-4-generation",
    "title": "Assignment 1: Language modelling",
    "section": "Problem 4: Generation",
    "text": "Problem 4: Generation\nIn this section, you will implement a simple generation mechanism for the language models you have implemented.\nRecall that one way to generate text with a language model is to repeatedly sample from the model’s output distribution, conditioning on some context. More specifically, this involves treating the softmax-normalised logits of the model as a multinomial distribution. The “creativeness” of the generation can be controlled with the temperature parameter of the softmax distribution.\nTo implement this recipe, we first ask you to extend each model with a generate method according to the following specification:\ngenerate (self, context, n_tokens = 10, temperature = 1.0)\n\nTakes a batch of context tokens context and extends it by sampling n_tokens new tokens from the model’s output distribution, scaled with the temperature temperature. Returns the extended context.\n\nIn a second stage, you should implement a convenience function generate that allows you to easily generate text with different models, like this:\ngenerate(model_fixed_window, 'i like', max_tokens=10, temperature=1.5)\n\ndef generate(model, context, max_tokens=3, temperature=1.0):\n    # TODO: Replace the next line with your own code\n    return context\n\nHere is the specification of the convenience function:\ngenerate (model, context, max_tokens = 10, temperature = 1.0)\n\nTakes a context sentence context, tokenises and vectorises it, and passes it to the specified model to generate new text. The new text consists of at most max_tokens, but is cut off at the first &lt;eos&gt; token. Returns the generated text (including the context)."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#problem-5-parameter-initialisation",
    "href": "assignments/assignment1/assignment1.html#problem-5-parameter-initialisation",
    "title": "Assignment 1: Language modelling",
    "section": "Problem 5: Parameter initialisation",
    "text": "Problem 5: Parameter initialisation\nThe error surfaces explored when training neural networks can be very complex. Because of this, it is important to choose “good” initial values for the parameters. In PyTorch, the weights of the embedding layer are initialised by sampling from the standard normal distribution \\(\\mathcal{N}(0, 1)\\). Test how changing the initialisation affects the perplexity of your language models. Find research articles that propose different initialisation strategies.\nWrite a short (150 words) report about your experiments and literature search. Use the following prompts:\n\nWhat different initialisation did you try? What results did you get?\nHow do your results compare to what was suggested by the research articles?\nWhat did you learn? How, exactly, did you learn it? Why does this learning matter?\n\nTODO: Enter your text here"
  },
  {
    "objectID": "assignments/assignment1/index.html",
    "href": "assignments/assignment1/index.html",
    "title": "Assignment 1",
    "section": "",
    "text": "In this assignment you will implement and train two or three neural language models: the fixed-window model, the recurrent neural network model from Unit 1-2, and optionally a model based on the Transformer architecture from Unit 1-3. You will evaluate these models by computing their perplexity on a benchmark dataset.\nLink to the assignment\nDeadline: 2024-05-03 (submit via Canvas)"
  },
  {
    "objectID": "assignments/assignment2/index.html",
    "href": "assignments/assignment2/index.html",
    "title": "Assignment 2",
    "section": "",
    "text": "In the second assignment, we are going to use a large language model in a retrieval-augmented setup. As an application, we are going to consider a question answering task.\nYou can use any LLMs you want in this assignment, but your solution must consider at least one open model (e.g. Mistral or one of the Llama models). Optionally, you may compare to a commercial model.\nPlease implement the steps described below in a Jupyter notebook. The notebook should contain text cells with brief explanations of what you are doing and why. You should also include your evaluation scores in the text cells. Deadline: 31 May.\n\nThe dataset\nThe dataset we will use in this assignment is a simplified version of Natural Questions, which was compiled by Google and consists of real search engine queries about factual questions.\nDownload the assignment dataset here. Then load the file using Pandas as follows:\nnq_data = pd.read_csv('nq_simplified.val.tsv', sep='\\t', header=None, names=['question', 'answer', 'gold_context'], quoting=3)\nThere are three columns: the question, the answer, and part of a Wikipedia page.\n\n\nStep 1: Evaluating an LLM on Natural Questions\nLoad an LLM and explore different prompting strategies to try to make it answer the questions in the dataset. As a benchmark, you can use the ROUGE-1 precision/recall/F1 scores.\ndef rouge1(gold, predicted):\n  assert(len(gold) == len(predicted))\n  n_p = 0\n  n_g = 0\n  n_c = 0\n  for g, p in zip(gold, predicted):\n    g = set(cleanup(g).strip().split())\n    p = set(cleanup(p).strip().split())\n    n_g += len(g)\n    n_p += len(p)\n    n_c += len(p.intersection(g))\n  pr = n_c / n_p\n  re = n_c / n_g\n  if pr &gt; 0 and re &gt; 0:\n    f1 = 2*pr*re/(pr + re)\n  else:\n    f1 = 0.0\n  return pr, re, f1\n\ndef cleanup(text):\n  text = text.replace(',', ' ')\n  text = text.replace('.', ' ')\n  return text\nWhile developing, you should probably just use a small subset of the dataset.\n\n\nStep 2: An idealized retrieval-augmented LLM\nThe third column in the dataset (called gold_context above) contains a text fragment from a Wikipedia page, from which the answer can be deduced. Try out new prompts where you include this relevant context. How does this change the evaluation scores?\n\n\nStep 3: Setting up the retriever\nThe setup in Step 2 is idealized, because we provided a context from Wikipedia where we know that the answer is avaialable. In real-world settings, this is not going to be the case.\nTo make this assignment work in Colab, we are going to work with a rather small set of passages. You can download these texts from here. For a given question, we are going to search among these passages to find the best-matching passage.\n\nRepresenting the passages as vectors\nSet up a representation model that maps a text passage to a numerical vector.\nFor instance, some model from SentenceTransformers, such as all-MiniLM-L6-v2 could be a good choice.\nApply this model to all text passages.\n\n\nStoring the passage vectors in a database\nWe now create a vector database that allows us to search efficiently for the neareast neighbors in the vector space of a given query vector. We recommend the FAISS library for this purpose. You can install it as follows.\n!pip install faiss-gpu\nTo create the vector database, you can use the following code:\nimport faiss\nindex = faiss.IndexFlatL2(embedded_passages.shape[1])\nindex.add(embedded_passages)\nTo search for the nearest neighbor, we simply call search on the previously created database.\n_, ix = index.search(embedded_question, 1)\nAs an example, which is the passage that most closely matches the question where did the first african american air force unit train?\n\n\n\nStep 4: Putting the pieces together\nFor each of the questions in the dataset we used in Steps 1–2, retrieve the best-matching passage from the vector database. Use this passage instead of the gold-standard passages you used in Step 2. Evaluate again.\nHow does your result compare to those in Steps 1 and 2?\nHint. While you are developing, it might be useful to run the retriever once and for all and store the result."
  },
  {
    "objectID": "assignments/assignment3/index.html",
    "href": "assignments/assignment3/index.html",
    "title": "Assignment 3",
    "section": "",
    "text": "The the third programming assignment is dedicated to the task of dependency parsing. More specifically, you will implement a simplified version of the dependency parser used by Glavaš and Vulić (2021).\nLink to the assignment\nDeadline: 2024-06-14 (submit via Canvas)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning for Natural Language Processing",
    "section": "",
    "text": "This is the website for the WASP course “Deep Learning for Natural Language Processing”, taught by Marco Kuhlmann (Linköping University) and Richard Johansson (Chalmers University of Technology)."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Deep Learning for Natural Language Processing",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nNatural Language Processing (NLP) develops methods for making human language accessible to computers. The goal of this course is to provide students with a theoretical understanding of and practical experience with the advanced algorithms that power modern NLP. The course focuses on methods based on deep neural networks.\nOn completion of the course, you will be able to\n\nexplain and analyze state-of-the-art deep learning architectures for NLP\nimplement such architectures and apply them to practical problems\ndesign and carry out evaluations of deep learning architectures for NLP\nuse current approaches to NLP in your own field of research"
  },
  {
    "objectID": "index.html#course-literature",
    "href": "index.html#course-literature",
    "title": "Deep Learning for Natural Language Processing",
    "section": "Course literature",
    "text": "Course literature\nMuch of the content of the course is not described in a book, so we will mostly give pointers to research papers and survey articles when needed. If you prefer textbook-style introductions, you can have a look\nJacob Eisenstein, Natural Language Processing. MIT Press, 2019. Pre-print version"
  },
  {
    "objectID": "index.html#teaching",
    "href": "index.html#teaching",
    "title": "Deep Learning for Natural Language Processing",
    "section": "Teaching",
    "text": "Teaching\nThe course uses a flipped classroom format where the central concepts and methods are presented in pre-recorded video lectures. The physical meetings will focus on discussions of the important points in the recorded material, practical exercises, and reflections.\nThe teaching is divided into three two-day meetings, each of which covers one module:\n\nMeeting 1: 11–12 April (Linköping)\nMeeting 2: 16–17 May (Gothenburg)\nMeeting 3: 3–4 June (Linköping)\n\nEach meeting will start with a lunch at 12:00 and end at 15:00 on the second day."
  },
  {
    "objectID": "index.html#modules",
    "href": "index.html#modules",
    "title": "Deep Learning for Natural Language Processing",
    "section": "Modules",
    "text": "Modules\nThe course consists of three thematic modules and a final project. The content of each module consists of\n\nvideo lectures introducing the important topics\npointers to literature, some of which is fundamental and some optional\na set of programming exercises\na set of discussion tasks\nan assignment where you implement a model\n\nDetailed information about the modules is available on the module pages, which you can find in the menu at the top of this page."
  },
  {
    "objectID": "index.html#project",
    "href": "index.html#project",
    "title": "Deep Learning for Natural Language Processing",
    "section": "Project",
    "text": "Project\nIn the project, you apply your learning in the course to your own field of research."
  },
  {
    "objectID": "index.html#deadlines",
    "href": "index.html#deadlines",
    "title": "Deep Learning for Natural Language Processing",
    "section": "Deadlines",
    "text": "Deadlines\n\n2024-05-03 Assignment 1 due\n2024-05-10 Project pitch due\n2024-05-31 Assignment 2 due\n2024-06-14 Assignment 3 due\n2024-06-14 Project report due\n2024-08-31 Last day to submit late assignments and project reports"
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Project",
    "section": "",
    "text": "In the project work, your task will be to\nThe projects can be carried out individually or in groups of 2–4 students."
  },
  {
    "objectID": "project/index.html#preliminary-project-description",
    "href": "project/index.html#preliminary-project-description",
    "title": "Project",
    "section": "Preliminary project description",
    "text": "Preliminary project description\nPlease submit a short text that describes what you intend to work on. For suggestions of topics, see below.\nThe text should include a project title and about 1 page giving a rough outline of how you intend to work. If you have found any related work or relevant dataset, this is useful information to include. The deadline for this preliminary description is 2024-05-10; we will give you comments on your description before the second meeting (16–17 May)."
  },
  {
    "objectID": "project/index.html#oral-presentation",
    "href": "project/index.html#oral-presentation",
    "title": "Project",
    "section": "Oral presentation",
    "text": "Oral presentation\nFor the final meeting of the course, we ask you to prepare a short (no more than 7 minutes) oral presentation. Depending on how far you have come in the project, you can focus on your project idea, method, or preliminary results. After your presentation, there will be time for feedback.\nIf you are unable to attend the third meeting, we request you to record your presentation as a video (in mp4 format) and submit it to us by 2024-06-03, 10:00. We will then present your video at the meeting and share any feedback received with you."
  },
  {
    "objectID": "project/index.html#the-report",
    "href": "project/index.html#the-report",
    "title": "Project",
    "section": "The report",
    "text": "The report\nThe text should be structured as a typical technical report, including an abstract, statement of problem, method description, experiments and results, and a conclusion. The deadline for submitting the report is 2024-06-14. If you do not manage to submit by that date, there is a second chance on 2024-08-31."
  },
  {
    "objectID": "project/index.html#finding-a-topic",
    "href": "project/index.html#finding-a-topic",
    "title": "Project",
    "section": "Finding a topic",
    "text": "Finding a topic\nYou can work on any project that is small and manageable enough for a short project of this kind. Ideally, try to find a topic that is relevant to your own research interests. It’s OK if you end up with preliminary results, as long as you have some tidbit to present at the end.\nIf you need help deciding on a topic, here are a few suggestions. Some of these are very easy and some more challenging, and this list is not sorted by the difficulty level.\n\nTry out a benchmark. There are some popular benchmark sets such as GLUE and the more recently released Super GLUE that are used to evaluate language understanding systems. Build a model and evaluate it on one of these benchmarks.\nCrosslingual applications. For some application, e.g. some categorization or tagging task we’ve seen in the course, investigate how well crosslingual representations such as multilingual BERT allows us to train with one language and evaluate with other languages.\nShared tasks. Every year, several NLP competitions are arranged at the SemEval conference. Get the data from some task from previous years. Additionally, there are shared tasks organized by CoNLL, BioNLP and other conferences, where datasets can often be accessed easily.\n\nAlternatively, if you don’t have an idea and you don’t like the suggested topics, just talk to Marco or Richard."
  },
  {
    "objectID": "modules/module3/meeting3.html",
    "href": "modules/module3/meeting3.html",
    "title": "Meeting 3",
    "section": "",
    "text": "The third and final meeting will take place 3–4 June in Linköping."
  },
  {
    "objectID": "modules/module3/meeting3.html#schedule",
    "href": "modules/module3/meeting3.html#schedule",
    "title": "Meeting 3",
    "section": "Schedule",
    "text": "Schedule\nThe meeting will start with lunch at 12:00 on Monday, 3 June and end at 15:00 on Tuesday, 4 June. Here is a rough schedule:\n\n\n\nTime\nActivity\n\n\n\n\nMonday\n\n\n\n12:00–13:00\nLunch at Kårallen, Gästmatsalen\n\n\n13:00–14:30\nReview and discussion of the video material\n\n\n14:30–15:00\nFika\n\n\n15:00–17:00\nProject presentations, part 1\n\n\nTuesday\n\n\n\n09:00–10:00\nExercise on structured prediction (TBD)\n\n\n10:00–12:00\nProject presentations, part 2 (including fika)\n\n\n12:00–13:00\nLunch at Universitetsklubben\n\n\n13:00–14:00\nIntroduction to Assignment 3: Dependency parsing\n\n\n14:00–15:00\nClosing session"
  },
  {
    "objectID": "modules/module3/meeting3.html#practical-information",
    "href": "modules/module3/meeting3.html#practical-information",
    "title": "Meeting 3",
    "section": "Practical information",
    "text": "Practical information\nMeeting venue. We will be at Linköping University, in C-huset, Campus Valla. Plenary sessions will be in lecture hall S26, group sessions in lecture halls S2 and S6. Lunches will be in Kårallen (Monday) and Universitetsklubben (Tuesday).\nPublic transport. You can choose between two bus lines to get to C-huset from the city center using public transport (Östgötatrafiken). The specified travel times include a final walk from the bus stop to the meeting room.\n\nBus no. 4 to Lambohov, get off at Nobeltorget (25 minutes)\nBus no. 12 to Lambohov, get off at Universitetet or Mästar Mattias väg (20 minutes)\n\nHotels. We can recommend Scandic Frimurarhotellet and Scandic Linköping City. Both of them are located close to the city centre and train station."
  },
  {
    "objectID": "modules/module2/index.html",
    "href": "modules/module2/index.html",
    "title": "Module 2",
    "section": "",
    "text": "This module will dive deeper in the techniques used to train modern language models. In addition, we will discuss applications and methodologies in text generation.\nWhen comparing the material for this module with that for Module 1, you will notice an increased share of material from external sources, such as video recordings and research articles. We have selected these to provide you with an up-to-date overview of this fast-developing field. If you want to know more, feel free to search for more detailed video lectures and articles from other sources.\nWe will discuss this module during the second course meeting in Gothenburg. Please see the meeting page for details."
  },
  {
    "objectID": "modules/module2/index.html#unit-2-1-modern-large-language-models",
    "href": "modules/module2/index.html#unit-2-1-modern-large-language-models",
    "title": "Module 2",
    "section": "Unit 2-1: Modern large language models",
    "text": "Unit 2-1: Modern large language models\nThis unit reviews some of the central topics related to modern language models (LLMs), notably from the GPT family. We examine their emergent capabilities like zero-shot learning and in-context learning and explore methods for aligning LLMs with human instructions and preferences. Finally, the lectures address the crucial aspect of evaluating general-purpose language models, offering insights into their effectiveness and applicability across various tasks and domains.\n\n\n\nTitle\nSlides\nVideo\n\n\n\n\nIntroduction to modern language models\n[slides]\n[video]\n\n\nEmergent abilities of LLMs\n[slides]\n[video]\n\n\nLLM alignment\n[slides]\n[video]\n\n\nEvaluating LLMs\n[slides]\n[video]\n\n\n\n\nReading\n\nBrown et al. (2020): Language Models are Few-Shot Learners\nOuyang et al. (2022): Aligning language models to follow instructions\n\n\n\nSurveys and other optional material\n\nKaddour et al. (2023): Challenges and Applications of Large Language Models\nLiu et al. (2023): Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing\nMinaee et al. (2024): Large Language Models: A Survey\nZheng et al. (2023): Secrets of RLHF in Large Language Models Part I: PPO\n\n\n\nSoftware resources\n\nTRL – Transformer Reinforcement Learning\nOpenAI Python API library"
  },
  {
    "objectID": "modules/module2/index.html#unit-2-2-working-with-open-large-language-models",
    "href": "modules/module2/index.html#unit-2-2-working-with-open-large-language-models",
    "title": "Module 2",
    "section": "Unit 2-2: Working with open large language models",
    "text": "Unit 2-2: Working with open large language models\nThe lectures in this unit present various techniques that collectively empower users to maximize the utility and efficiency of open large language models in various applications and scenarios. They explore efficient fine-tuning methods and quantization techniques to optimize model performance during training and deployment. The final lecture discusses retrieval augmentation, a strategy to enrich LLMs’ responses by incorporating additional information from retrieval systems.\n\n\n\nTitle\nSlides\nVideo\n\n\n\n\nOpen LLMs\n\n[video]\n\n\nEfficient fine-tuning techniques\n[slides]\n[video]\n\n\nQuantization\n\n[video]\n\n\nQuantized fine-tuning\n\n[video]\n\n\nRetrieval augmentation\n\n[video]\n\n\n\n\nReading\n\nDettmers et al. (2023): QLORA: Efficient Finetuning of Quantized LLMs\nRam et al. (2023): In-Context Retrieval-Augmented Language Models\n\n\n\nSurveys and other optional material\n\nChen et al. (2023): ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up?\nLialin et al. (2023): Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning\nWan et al. (2023): Efficient Large Language Models: A Survey\nGao et al. (2023): Retrieval-Augmented Generation for Large Language Models: A Survey\nRetrieval augmentation video by Douwe Kiela, part of a Stanford course\n\n\n\nSoftware resources\n\nbitsandbytes\npeft\naccelerate\nOllama\nLlamaindex\nSome accessible models: Llama 3, Mistral, Falcon"
  },
  {
    "objectID": "modules/module2/index.html#unit-2-3-generating-text-applications-and-methodology",
    "href": "modules/module2/index.html#unit-2-3-generating-text-applications-and-methodology",
    "title": "Module 2",
    "section": "Unit 2-3: Generating text: Applications and methodology",
    "text": "Unit 2-3: Generating text: Applications and methodology\nThe third unit explores applications of large language models (LLMs) in various generation tasks. Specific tasks covered include summarization, condensing information effectively, and dialogue generation, facilitating natural and engaging conversations. The unit also introduces evaluation methods for assessing the efficacy of generation systems.\n\n\n\nTitle\nSlides\nVideo\n\n\n\n\nIntroduction to generation tasks\n[slides]\n[video]\n\n\nEvaluation of generation systems\n[slides]\n[video]\n\n\nSummarization\n[slides]\n[video]\n\n\nDialogue\n[slides]\n[video]\n\n\n\n\nReading\n\nEisenstein, chapter 19\nGoyal et al. (2022) News Summarization and Evaluation in the Era of GPT-3"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning.html",
    "href": "modules/module2/peft/parameter-efficient-finetuning.html",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "",
    "text": "Fine-tuning all parameters of pre-trained language models can be resource-intensive. Because of this, current research in natural language processing is looking into developing methods for adapting models to downstream tasks without full fine-tuning. These methods only tune a small number of model parameters while yielding performance comparable to that of a fully fine-tuned model.\nIn this exercise, you will implement LoRA, one of the most well-known methods for parameter-efficient fine-tuning. LoRA stands for “Low-Rank Adaptation of Large Language Models” and was originally described in a research article by Hu et al. (2021)."
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning.html#dataset",
    "href": "modules/module2/peft/parameter-efficient-finetuning.html#dataset",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Dataset",
    "text": "Dataset\nThe data for this lab comes from the Large Movie Review Dataset. The full dataset consists of 50,000 highly polar movie reviews collected from the Internet Movie Database (IMDB). Here, we use a random sample consisting of 2,000 reviews for training and 500 reviews for evaluation.\nTo load the dataset, we use the Hugging Face Datasets library.\n\nfrom datasets import load_dataset\n\nimdb_dataset = load_dataset('csv', data_files = {'train': 'train.csv', 'eval': 'eval.csv'})\n\nimdb_dataset\n\nAs we can see, each sample in the dataset is a record with three fields: an internal index (index, an integer), the text of the review (review, a string), and the sentiment label (label, an integer – 1 for “positive” and 0 for “negative” sentiment).\nHere is an example record:\n\nimdb_dataset['train'][645]"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning.html#tokeniser",
    "href": "modules/module2/peft/parameter-efficient-finetuning.html#tokeniser",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Tokeniser",
    "text": "Tokeniser\nAs our pre-trained language model, we will use DistilBERT, a compact encoder model with 40% less parameters than BERT base. DistilBERT is not actually a large language model by modern standards and thus does not benefit as much from parameter-efficient fine-tuning as other models. However, it has the benefit of being light and fast, and can be run even on consumer hardware.\nTo feed the movie reviews to DistilBERT, we need to tokenise them and encode the resulting tokens as integers in the model vocabulary. We start by loading the DistilBERT tokeniser using the Auto classes:\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n\nWe then create a tokenised version of the dataset:\n\ndef tokenize_function(batch):\n    return tokenizer(batch['review'], padding=True, truncation=True)\n\ntokenized_imdb_dataset = imdb_dataset.map(tokenize_function, batched=True)\n\ntokenized_imdb_dataset\n\nAs we can see, tokenising adds two additional fields to each review: input_ids is the list of token ids corresponding to the review, and attention_mask is the list of indices specifying which tokens the encoder should attend to.\nTo avoid trouble when fine-tuning the model later, the next cell disables tokeniser parallelism.\n\nimport os\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning.html#trainer",
    "href": "modules/module2/peft/parameter-efficient-finetuning.html#trainer",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Trainer",
    "text": "Trainer\nIn this section, we will set up our workflow for training and evaluating DistilBERT models. The central component in this workflow is the Trainer, which provides extensive configuration options. Here, we leave most of these options at their default value. Two changes we do make are to enable evaluation of the trained model after each epoch, and to log the training and evaluation loss after every 5 training steps (the default is 500).\n\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='tmp_trainer',\n    evaluation_strategy='epoch',\n    logging_steps=5,\n)\n\nIn addition to the loss, we also track classification accuracy. For this we import the Hugging Face Evaluate library and define a small helper function compute_metrics() that the trainer will call after each epoch.\n\nimport evaluate\n\naccuracy = evaluate.load('accuracy')\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    return accuracy.compute(predictions=predictions, references=labels)\n\nIn the next cell we define a convenience function make_trainer() that creates a readily-configured trainer for a specified model (model). We will use this trainer both to train the model on the training section of the tokenised review dataset, and to evaluate it on the evaluation section.\n\nfrom transformers import Trainer\n\ndef make_trainer(model):\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_imdb_dataset['train'],\n        eval_dataset=tokenized_imdb_dataset['eval'],\n        compute_metrics=compute_metrics,\n    )\n    return trainer"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning.html#problem-1-full-fine-tuning",
    "href": "modules/module2/peft/parameter-efficient-finetuning.html#problem-1-full-fine-tuning",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Problem 1: Full fine-tuning",
    "text": "Problem 1: Full fine-tuning\nIn the rest of this notebook, we will work our way to the implementation of LoRA, and compare LoRA to traditional fine-tuning methods. Our first point of reference is a fully fine-tuned DistilBERT model.\nWe start by loading the pre-trained model:\n\nfrom transformers import AutoModelForSequenceClassification\n\npretrained_model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n\npretrained_model\n\nThe architecture of DistilBERT is that of a standard Transformer encoder with an embedding layer (embeddings) followed by a stack of six Transformer blocks (transformer) and a feedforward network with two linear layers (pre_classifier and classifier) and a final dropout layer (dropout).\n\nCount the number of trainable parameters\nOne relevant measure in the context of parameter-efficient fine-tuning is the number of parameters that need to be changed when training a model. Your first task in this lab is to write a function num_trainable_parameters() that calculates this number for a given model.\n\ndef num_trainable_parameters(model):\n    # TODO: Replace the next line with your own code\n    return 0\n\nThe function should implement the following specification:\n\nnum_trainable_parameters (model)\nReturns the number of float-valued trainable parameters in the specified model as an integer.\n\n\n👍 Hint\nThe term parameter can refer to either complete tensors or the individual elements of these tensors. For example, a linear layer created by nn.Linear(3, 5) has 2 tensor-valued parameters (a weight matrix and a bias vector) and 20 float-valued parameters (the elements of these tensors). To get the tensor-valued parameters of a model, you can use the parameters() method. A parameter is trainable if it requires gradient.\n\n\n🤞 Test your code\nTo test your code, apply your function to the pre-trained model. The correct number of float-valued trainable parameters for this model is 66,955,010.\n\n\n\nFine-tuning\nWhen we load the pre-trained model, the Hugging Face Transformers library warns us that the weights of the feedforward network have not yet been trained. To do so, we pass the pre-trained model to a trainer and initiate the fine-tuning process. Because full fine-tuning is so resource-intensive, we save the fine-tuned model to disk. Alternatively, we load an already fine-tuned version of the model (provided on the course website).\n⚠️ Please note that fine-tuning the model will take some time! ⚠️\n\n# Alternative A: Fine-tuning\n\n# finetuned_trainer = make_trainer(pretrained_model)\n# finetuned_trainer.train()\n# finetuned_trainer.save_model('finetuned')\n\n# Alternative B: Load a fine-tuned model\n\nfinetuned_model = AutoModelForSequenceClassification.from_pretrained('finetuned')\n\n\n\nConvenience functions\nBecause we will repeat the steps we just took to fine-tune the pre-trained model several times in this notebook, we define two convenience functions:\n\ndef train(model):\n    print('Number of trainable parameters:', num_trainable_parameters(model))\n    trainer = make_trainer(model)\n    trainer.train()\n    return model\n\n\ndef evaluate(model):\n    trainer = make_trainer(model)\n    return trainer.evaluate()"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning.html#problem-2-tuning-the-final-layers-only",
    "href": "modules/module2/peft/parameter-efficient-finetuning.html#problem-2-tuning-the-final-layers-only",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Problem 2: Tuning the final layers only",
    "text": "Problem 2: Tuning the final layers only\nIf full fine-tuning marks one end of the complexity spectrum, the other end is marked by only tuning the final layers of the transformer – the head of the model. In the case of DistilBERT, the head consists of the pre_classifier and classifier layers.\nImplement the head-tuning strategy by coding the following function:\n\ndef make_headtuned_model():\n    # TODO: Replace the next line with your own code\n    raise NotImplementedError\n\nHere is the specification of this function:\n\nmake_headtuned_model ()\nReturns a model that is identical to the pre-trained model, except that the head layers have been trained on the sentiment data. (The other parameters of the pre-trained model are left untouched.)\n\n\n👍 Hint\nYou freeze a parameter by setting its requires_grad-attribute to False.\nOnce you have an implementation of the head-tuning strategy, evaluate it on the evaluation data. How much accuracy do we lose when only training the final layers of the pre-trained model, compared to full fine-tuning?\n\nheadtuned_model = make_headtuned_model()\n\n\n\n🤞 Test your code\nIf you configured your model correctly, num_trainable_parameters() should show 592,130 trainable parameters.\nFor future reference, we also save the head-tuned model:\n\nmake_trainer(headtuned_model).save_model('headtuned')"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning.html#problem-3-layer-surgery",
    "href": "modules/module2/peft/parameter-efficient-finetuning.html#problem-3-layer-surgery",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Problem 3: Layer surgery",
    "text": "Problem 3: Layer surgery\nLoRA works by “wrapping” frozen layers from the pre-trained Transformer model inside adapter modules. Conventionally, this wrapping is only applied to the linear layers that transform the queries and values in the self-attention mechanism. To implement the wrapping, we need functions to extract and replace layers in a model. Your task in this section is to code these functions.\n\nExtracting layers\nCode a function that extracts the query and value linear layers from a DistilBERT model:\n\ndef extract(model):\n    # TODO: Replace the next line with your own code\n    return {}\n\nImplement this function to match the following specification:\n\nextract (model)\nTakes a DistilBERT model (model) and extracts the query and value linear layers from each block of the Transformer. Returns a dictionary mapping the DistilBERT module names of these layers to the layers themselves (instances of nn.Linear).\n\n\n👍 Hint\nAs we saw earlier, the DistilBERT model consists of a hierarchy of nested submodules. Each of these can be addressed by a fully-qualified string name. Use get_submodule() to retrieve a layer by name. You can hard-wire the names of the layers you want to extract.\n\n\n🤞 Test your code\nTo test your code, check the number of trainable float-valued parameters in the extracted layers. This number should be 7,087,104.\n\n\n\nReplacing layers\nThe inverse of the extract() function replaces selected layers of a module using a dictionary of named layers.\n\ndef replace(model, named_layers):\n    for name, layer in named_layers.items():\n        components = name.split('.')\n        submodule = model\n        for component in components[:-1]:\n            submodule = getattr(submodule, component)\n        setattr(submodule, components[-1], layer)\n    return model\n\nThis function has the following specification:\n\nreplace (model, named_layers)\nTakes a DistilBERT model (model) and a dictionary in the format returned by extract() (named_layers) and injects the extracted layers into the model. More specifically, suppose that named_layers contains a key–value pair (name, layer). Then the function replaces the submodule of model addressed by the fully-qualified string name name by the layer layer. Returns the modified model.\n\n\n🤞 Test your code\nTo test your implementation, write code that (1) extracts the query and value linear layers from the fine-tuned model; (2) replaces these layers with clones with random weights; and (3) replaces these layers again with the original versions. Evaluating the modified model after step (2) should yield a near-random accuracy. Evaluating it again after step (3) should yield the original accuracy.\nThe following function should be helpful. It clones a linear layer, copying the weights and the bias from the original.\n\nimport torch.nn as nn\n\ndef clone_linear(original):\n    out_features, in_features = original.weight.shape\n    copy = nn.Linear(in_features, out_features)\n    copy.load_state_dict(original.state_dict())\n    return copy"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning.html#problem-4-lora",
    "href": "modules/module2/peft/parameter-efficient-finetuning.html#problem-4-lora",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Problem 4: LoRA",
    "text": "Problem 4: LoRA\nIn this section, you will implement the LoRA adapters and fine-tune the adapted model.\nRecall that the basic idea behind LoRA is to conceptualise fine-tuned weights as a sum \\(W_0 + \\Delta W\\) of the weights from the pre-trained model, \\(W_0\\), and a low-rank update matrix \\(\\Delta W\\). The goal of fine-tuning, then, is to learn the update matrix; this happens in the adapter layers.\n\nImplement the adapter\nA LoRA adapter implements the forward function\n\\[\ny = x W_0 + x \\Delta W = x W_0 + x A B\n\\]\nwhere \\(W_0\\) is a linear transformation from the pre-trained model and \\(\\Delta W\\) is a learned update matrix, deconstructed into the product \\(AB\\) of two rank-\\(r\\) matrices \\(A\\) and \\(B\\). LoRA scales the update matrix \\(\\Delta W\\) by a factor of \\(\\alpha / r\\), where \\(\\alpha\\) is a hyperparameter. (To keep the formula tidy, we ignore the fact that the linear transformation in the pre-trained model may additionally include a bias.)\n\nimport torch.nn as nn\n\nclass LoRA(nn.Module):\n\n    def __init__(self, pretrained, rank=12, alpha=24):\n        super().__init__()\n        # TODO: Add your code here\n\n    def forward(self, x):\n        # TODO: Replace the next line with your own code\n        raise NotImplementedError\n\nYour code must comply with the following specification:\ninit (self, pretrained, rank = 12, alpha = 24)\n\nInitialises the LoRA adapter. This sets up the matrices \\(A\\) and \\(B\\) from the equation above. The matrix \\(A\\) is initialised with random weights from a standard normal distribution; the matrix \\(B\\) is initialised with zeros. The argument pretrained is the linear layer from the pre-trained model that should be adapted. The arguments rank and alpha are the rank \\(r\\) and the hyperparameter \\(\\alpha\\) in the equation above.\n\nforward (self, x)\n\nSends an input x through the adapter, implementing the equation above.\n\n\n\nInject the adapter into the pre-trained model\nThe final step is to construct an adapted model by injecting the LoRA adapters into the pre-trained model.\n\ndef make_lora_model(rank):\n    # TODO: Replace the next line with your own code\n    raise NotImplementedError\n\nImplement the function to match the following specification:\n\nmake_lora_model (rank)\nReturns a model that is identical to the pre-trained model, except that the query and value linear layers have been wrapped in LoRA adapters, and the LoRA adapters and the head layers of the pre-trained model have been trained on the sentiment data. (The other parameters of the pre-trained model are left untouched.) The rank of the adapters is specified by the argument rank. The alpha value of the adapters is set to twice the rank (a common rule of thumb).\n\nRun the next cell to evaluate your model for \\(r = 6\\) and \\(\\alpha = 12\\). How many trainable parameters does the adapted model have? What accuracy do you get? How do these value relate to the number of trainable parameters and accuracy of the fully fine-tuned model, in terms of percentages?\n\nlora_model = make_lora_model(6)\n\nevaluate(lora_model)"
  },
  {
    "objectID": "modules/module2/peft/parameter-efficient-finetuning.html#problem-5-alternatives-to-transformer-based-models",
    "href": "modules/module2/peft/parameter-efficient-finetuning.html#problem-5-alternatives-to-transformer-based-models",
    "title": "Exercise: Parameter-efficient fine-tuning",
    "section": "Problem 5: Alternatives to Transformer-based models",
    "text": "Problem 5: Alternatives to Transformer-based models\nEven with methods for parameter-efficient fine-tuning, applying DistilBERT and other Transformer-based models comes at a significant cost – an investment that does not always pay off. In the final problem of this lab, we ask you to explore a more traditional approach to classification and contrast it with the pre-training/fine-tuning approach of neural language models.\nBrowse the web to find a tutorial on how to apply a classifier from the scikit-learn library to the problem of sentiment classification and implement the method here in this notebook. We suggest you use multinomial Naive Bayes or logistic regression. (Once you have code for one method, it is easy to switch to the other.) Evaluate your chosen classifier on the IMDB dataset.\n\nWhich classifier did you try? What results did you get? How long did it take you to train and run the classifier?\nWhat is your perspective on the trade-off between accuracy and resource requirements between the two approaches?"
  },
  {
    "objectID": "modules/module0/index.html",
    "href": "modules/module0/index.html",
    "title": "Review",
    "section": "",
    "text": "This page collects review material for the course."
  },
  {
    "objectID": "modules/module0/index.html#entry-requirements",
    "href": "modules/module0/index.html#entry-requirements",
    "title": "Review",
    "section": "Entry requirements",
    "text": "Entry requirements\nAs stated in the course syllabus, we assume you have a background in mathematics corresponding to the contents of the WASP course “Mathematics and Machine Learning”.\nYou will need solid programming experience in a high-level language; the programming assignments will use Python.\nYou should be comfortable with modern deep learning techniques and frameworks, for instance as taught by the WASP course “Deep Learning and GANs”. We do not assume previous knowledge of NLP."
  },
  {
    "objectID": "modules/module0/index.html#linguistics",
    "href": "modules/module0/index.html#linguistics",
    "title": "Review",
    "section": "Linguistics",
    "text": "Linguistics\nWe do not expect you to have any background in linguistics, but will often use linguistic terminology. (No natural language processing without language!) The following video provides a compact, 30-minute introduction to some of the essentials of linguistics that you may encounter in the course.\nEssentials of linguistics"
  },
  {
    "objectID": "modules/module0/index.html#basic-text-processing",
    "href": "modules/module0/index.html#basic-text-processing",
    "title": "Review",
    "section": "Basic text processing",
    "text": "Basic text processing\nThe exercises and assignments in this course require solid programming experience. In particular, you will benefit from previous experience with text processing in Python. If you need to get up to speed on this, we have a notebook that introduces some basic concepts and techniques for processing text data.\nBasic text processing"
  },
  {
    "objectID": "modules/module0/index.html#pytorch",
    "href": "modules/module0/index.html#pytorch",
    "title": "Review",
    "section": "PyTorch",
    "text": "PyTorch\nAs our deep learning framework, we will use PyTorch. Many good introductions to PyTorch are available online. The notebook we provide here is focused on those concepts and techniques that you will encounter in the exercises and assignments.\nIntroduction to PyTorch"
  },
  {
    "objectID": "modules/module0/index.html#using-colab",
    "href": "modules/module0/index.html#using-colab",
    "title": "Review",
    "section": "Using Colab",
    "text": "Using Colab\nMost of the assignments and exercises will require you to use a GPU-based machine. If you do not have easy access to such a machine at your institution, using Google’s free Colab service may be an alternative. We have written some instructions describing the basics of how to work with Colab.\nUsing Colab"
  },
  {
    "objectID": "modules/module1/index.html",
    "href": "modules/module1/index.html",
    "title": "Module 1",
    "section": "",
    "text": "This module introduces one of the most fundamental ideas in modern NLP: the idea that words can be represented as vectors which can be learned from text data. On the application side of things, we focus on one of the most fundamental NLP tasks: text categorization.\nBefore working with the material in this module, you may want to have a look at the following:\nWe will discuss the material during the first course meeting. Please see the meeting page for details."
  },
  {
    "objectID": "modules/module1/index.html#unit-1-1-introduction-to-representations-of-words-and-documents",
    "href": "modules/module1/index.html#unit-1-1-introduction-to-representations-of-words-and-documents",
    "title": "Module 1",
    "section": "Unit 1-1: Introduction to representations of words and documents",
    "text": "Unit 1-1: Introduction to representations of words and documents\nIn the first unit, we introduce the idea of word representations: the basic building block that we use to build deep learning models that process language. Specifically, we look at word embeddings and how they can be trained. We also discuss some challenges in representing words that can be solved by working with lower-level subword units.\n\n\n\nTitle\nSlides\nVideo\n\n\n\n\nIntroduction to word representations\n[slides]\n[video]\n\n\nLearning word embeddings with neural networks\n[slides]\n[video]\n\n\nSubword models\n[slides]\n[video]"
  },
  {
    "objectID": "modules/module1/index.html#unit-1-2-language-modelling-and-contextualized-embeddings",
    "href": "modules/module1/index.html#unit-1-2-language-modelling-and-contextualized-embeddings",
    "title": "Module 1",
    "section": "Unit 1-2: Language modelling and contextualized embeddings",
    "text": "Unit 1-2: Language modelling and contextualized embeddings\nThis unit begins with an overview of language modelling. It highlights the historical significance of n-gram models in NLP, which laid the foundation for the transition to neural language models. We continue with an exploration of pre-Transformer neural architectures, specifically focusing on recurrent neural networks (RNNs) and the pivotal Long Short-Term Memory (LSTM) architecture. At the end of the unit, we explore the use of RNNs as language models and introduce the concept of contextualised embeddings, which recognize the varying meanings of words across different contexts.\n\nLecture videos\n\n\n\nTitle\nSlides\nVideo\n\n\n\n\nIntroduction to language modelling\n[slides]\n[video] (14:49)\n\n\nN-gram language models\n[slides]\n[video] (14:46)\n\n\nNeural language models\n[slides]\n[video] (14:49)\n\n\nRecurrent neural networks\n[slides]\n[video] (14:47)\n\n\nThe LSTM architecture\n[slides]\n[video] (14:45)\n\n\nRNN language models\n[slides]\n[video] (14:32)\n\n\nContextualized word embeddings\n[slides]\n[video] (14:48)"
  },
  {
    "objectID": "modules/module1/index.html#unit-1-3-transformer-based-language-models",
    "href": "modules/module1/index.html#unit-1-3-transformer-based-language-models",
    "title": "Module 1",
    "section": "Unit 1-3: Transformer-based language models",
    "text": "Unit 1-3: Transformer-based language models\nThis unit explores the evolution of transformer-based language models, starting with the sequence-to-sequence or encoder–decoder architecture for neural machine translation. We then delve into the concept of attention, followed by the Transformer architecture as such, which sets a benchmark in machine translation and various natural language processing applications. We go through Transformer-based models, specifically the GPT family, which derives from the Transformer’s decoder side, and BERT, which utilizes the encoder side. The unit ends by discussing text generation algorithms, including beam search generation.\n\nLecture videos\n\n\n\nTitle\nSlides\nVideo\n\n\n\n\nNeural machine translation\n[slides]\n[video] (14:48)\n\n\nAttention\n[slides]\n[video] (14:49)\n\n\nThe Transformer architecture\n[slides]\n[video] (14:45)\n\n\nDecoder-based language models (GPT)\n[slides]\n[video] (14:50)\n\n\nEncoder-based language models (BERT)\n[slides]\n[video] (14:44)\n\n\nGeneration algorithms\n[slides]\n[video] (26:42)\n\n\n\n\n\nReading\n\nCheng et al. (2016)\nVaswani et al. (2017)\nSerrano and Smith (2019)\nRadford et al. (2018)\nDevlin et al. (2019)"
  }
]