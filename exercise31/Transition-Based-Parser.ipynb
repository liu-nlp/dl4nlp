{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Transition-based dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will implement and evaluate the implementation of a simple transition-based dependency parser in the style of [Chen and Manning (2014)](https://www.aclweb.org/anthology/D14-1082/). This parser is based on the arc-standard algorithm that was presented in the video lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for the exercise is the English Web Treebank from the [Universal Dependencies Project](http://universaldependencies.org), a corpus containing more than 16,000 sentences (254,000&nbsp;tokens) annotated with dependency trees (among other things). The Universal Dependencies Project distributes its data in the [CoNLL-U format](https://universaldependencies.org/format.html). The code in the next cell reads data in this format from a file-like object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = ('[ROOT]', '[ROOT]', 0)  # Pseudo-word; see comment below\n",
    "\n",
    "def read_data(source):\n",
    "    buffer = [ROOT]\n",
    "    for line in source:\n",
    "        if not line.startswith('#'):  # Skip lines with comments\n",
    "            line = line.rstrip()\n",
    "            if not line:\n",
    "                yield buffer\n",
    "                buffer = [ROOT]\n",
    "            else:\n",
    "                columns = line.split('\\t')\n",
    "                if columns[0].isdigit():  # Skip range tokens\n",
    "                    buffer.append((columns[1], columns[3], int(columns[6])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will use the training section and the development section of the [English Web Treebank](https://github.com/UniversalDependencies/UD_English-EWT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en_ewt-ud-train-projectivized.conllu', encoding='utf-8') as source:\n",
    "    TRAIN_DATA = list(read_data(source))\n",
    "\n",
    "with open('en_ewt-ud-dev.conllu', encoding='utf-8') as source:\n",
    "    DEV_DATA = list(read_data(source))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both datasets consist of syntactically analysed sentences. In this exercise, an analysed sentence is represented as a list of triples, where the first component of each triple represents a word form, the second component represents the wordâ€™s tag, and the third component is an integer specifying the position of the wordâ€™s syntactic head, i.e., its parent in the dependency tree. Run the following code cell to see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_DATA[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the head of the word *I* is the word at position&nbsp;2, *ran*; the dependents of *ran* are *I* (position&nbsp;1), *item* (position&nbsp;5), *Internet* (position&nbsp;8), as well as the final punctuation mark. Note that each sentence is preceded by the pseudo-word `[ROOT]` (position&nbsp;0), which represents the structural root of the dependency tree. (It is not dependent on any other word.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Encoding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a function that constructs the word and tag vocabularies from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD, PAD_IDX = '[PAD]', 0\n",
    "UNK, UNK_IDX = '[UNK]', 1\n",
    "\n",
    "def make_vocabs(gold_sentences):\n",
    "    vocab_words = {PAD: PAD_IDX, UNK: UNK_IDX}\n",
    "    vocab_tags = {PAD: PAD_IDX}\n",
    "    for sentence in gold_sentences:\n",
    "        for word, tag, _ in sentence:\n",
    "            if word not in vocab_words:\n",
    "                vocab_words[word] = len(vocab_words)\n",
    "            if tag not in vocab_tags:\n",
    "                vocab_tags[tag] = len(vocab_tags)\n",
    "    return vocab_words, vocab_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions implement the following specification:\n",
    "\n",
    "**make_vocabs** (*gold_sentences*)\n",
    "\n",
    "> Returns a pair of two vocabularies, represented as dictionaries: a *word vocabulary* and a *tag vocabulary*. The word vocabulary maps the unique words in the *gold_sentences* to a contiguous range of integers between $0$ and $W+1$, where $W$ is the total number of unique words. Similarly, the tag vocabulary maps the unique part-of-speech tags in the *gold_sentences* to a range between $0$ and $T$, where $T$ is the total number of unique tags. The special words `[PAD]` (used later for padding undefined values) and `[UNK]` (used in place of unknown words at prediction time) are mapped to the indexes&nbsp;0 and&nbsp;1, respectively. The special tag `[PAD]` is mapped to the index&nbsp;0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_vocabs(TRAIN_DATA)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Parser, static part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser consists of two parts: a **static part** that implements the logic of the arc-standard transition system, and a **non-static part** that contains the learning component. In this section we cover the static part.\n",
    "\n",
    "In the arc-standard algorithm, the next move (transition) of the parser is predicted based on features extracted from the current parser configuration, with references to the words and part-of-speech tags of the input sentence. On the Python side of things, we represent parser configurations as triples\n",
    "\n",
    "$$\n",
    "(i, \\mathit{stack}, \\mathit{heads})\n",
    "$$\n",
    "\n",
    "where $i$ is an integer specifying the position of the next word in the buffer, $\\mathit{stack}$ is a list of integers specifying the positions of the words currently on the stack (with the topmost element last in the list), and $\\mathit{heads}$ is a list of integers specifying the positions of the currently assigned head words. To illustrate this representation, the initial configuration for the sample sentence above is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0, [], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a possible final configuration is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(10, [0], [0, 2, 0, 5, 5, 2, 8, 8, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In the lecture, both the buffer and the stack were presented as list of words. Here we only represent the *stack* as a list of words. To represent the *buffer*, we simply record the position of the next word that has not been processed yet (the integer $i$). This acknowledges the fact that the buffer (in contrast to the stack) can never grow, but will be processed from left to right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains skeleton code for the static part of the parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserCore(object):\n",
    "\n",
    "    # Parser moves are specified as integers.\n",
    "\n",
    "    MOVES = tuple(range(3))\n",
    "\n",
    "    SH, LA, RA = MOVES\n",
    "\n",
    "    @staticmethod\n",
    "    def initial_config(num_words):\n",
    "        return 0, [], [0] * num_words\n",
    "\n",
    "    @staticmethod\n",
    "    def valid_moves(config):\n",
    "        pos, stack, heads = config\n",
    "        moves = []\n",
    "        if pos < len(heads):\n",
    "            moves.append(ParserCore.SH)\n",
    "        if len(stack) >= 2:\n",
    "            moves.append(ParserCore.LA)\n",
    "            moves.append(ParserCore.RA)\n",
    "        return moves\n",
    "\n",
    "    @staticmethod\n",
    "    def next_config(config, move):\n",
    "        pos, stack, heads = config\n",
    "        stack = list(stack)  # copy because we will modify it\n",
    "        if move == ParserCore.SH:\n",
    "            stack.append(pos)\n",
    "            pos += 1\n",
    "        else:\n",
    "            heads = list(heads)  # copy because we will modify it\n",
    "            s1 = stack.pop()\n",
    "            s2 = stack.pop()\n",
    "            if move == ParserCore.LA:\n",
    "                heads[s2] = s1\n",
    "                stack.append(s1)\n",
    "            if move == ParserCore.RA:\n",
    "                heads[s1] = s2\n",
    "                stack.append(s2)\n",
    "        return pos, stack, heads\n",
    "\n",
    "    @staticmethod\n",
    "    def is_final_config(config):\n",
    "        pos, stack, heads = config\n",
    "        return pos == len(heads) and len(stack) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to implement this interface according the following specification:\n",
    "\n",
    "**initial_config** (*num_words*)\n",
    "\n",
    "> Returns the initial configuration for a sentence with the specified number of words.\n",
    "\n",
    "**valid_moves** (*config*)\n",
    "\n",
    "> Returns the list of valid moves for the specified configuration. Note that moves are represented as integers.\n",
    "\n",
    "**next_config** (*config*, *move*)\n",
    "\n",
    "> Applies the specified move (an integer) to the specified configuration and returns the new configuration.\n",
    "\n",
    "**is_final_config** (*config*)\n",
    "\n",
    "> Tests whether the specified configuration is a final configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤ž Test your code\n",
    "\n",
    "To test your implementation, you can run the code below. This code creates the initial configuration for the example sentence, simulates a sequence of moves, and then checks that the resulting configuration is the expected final configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parser_core():\n",
    "    moves = [0, 0, 0, 1, 0, 0, 0, 1, 1, 2, 0, 0, 0, 1, 1, 2, 0, 2, 2]\n",
    "    parser = ParserCore()\n",
    "    config = parser.initial_config(len(DEV_DATA[100]))\n",
    "    for move in moves:\n",
    "        assert move in parser.valid_moves(config)\n",
    "        config = parser.next_config(config, move)\n",
    "    assert parser.is_final_config(config)\n",
    "    assert config == (10, [0], [0, 2, 0, 5, 5, 2, 8, 8, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parser_core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heart of the non-static part of the parser is the *next move classifier*. To train this classifier, we need training examples of the form $(\\mathbf{x}, m)$, where $\\mathbf{x}$ is a feature vector extracted from a given parser configuration $c$, and $m$ is the corresponding gold-standard move. To obtain $m$, we need an **oracle**.\n",
    "\n",
    "Recall that, in the context of transition-based dependency parsing, an oracle is a function that translates a gold-standard dependency tree (here represented as a list of head ids) into a sequence of moves such that, when the parser takes the moves starting from the initial configuration, then it recreates the original dependency tree. Here we ask you to implement the static oracle that was presented in the video lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle_moves(gold_heads):\n",
    "    remaining_count = [0] * len(gold_heads)\n",
    "    for node in gold_heads:\n",
    "        remaining_count[node] += 1\n",
    "\n",
    "    config = ParserCore.initial_config(len(gold_heads))\n",
    "    while not ParserCore.is_final_config(config):\n",
    "        pos, stack, heads = config\n",
    "        if len(stack) >= 2:\n",
    "            s1 = stack[-1]\n",
    "            s2 = stack[-2]\n",
    "            if gold_heads[s2] == s1 and remaining_count[s2] == 0:\n",
    "                move = ParserCore.LA\n",
    "                yield config, move\n",
    "                config = ParserCore.next_config(config, move)\n",
    "                remaining_count[s1] -= 1\n",
    "                continue\n",
    "            if gold_heads[s1] == s2 and remaining_count[s1] == 0:\n",
    "                move = ParserCore.RA\n",
    "                yield config, move\n",
    "                config = ParserCore.next_config(config, move)\n",
    "                remaining_count[s2] -= 1\n",
    "                continue\n",
    "        move = ParserCore.SH\n",
    "        yield config, move\n",
    "        config = ParserCore.next_config(config, move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should conform to the following specification:\n",
    "\n",
    "**oracle_moves** (*gold_heads*)\n",
    "\n",
    "> Translates a gold-standard head assignment for a sentence-specific head-assignment (*gold_heads*) into the corresponding stream of oracle moves. More specifically, this yields pairs $(c, m)$ where $m$ is a move and $c$ is the configuration in which $m$ was taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤ž Test your code\n",
    "\n",
    "You can test your oracle by executing the cell below. This extracts the oracle move sequence from the example sentence and compares it to the gold-standard move sequence `gold_moves`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oracle():\n",
    "    gold_heads = [h for w, t, h in DEV_DATA[100]]\n",
    "    gold_moves = [0, 0, 0, 1, 0, 0, 0, 1, 1, 2, 0, 0, 0, 1, 1, 2, 0, 2, 2]\n",
    "    assert list(m for _, m in oracle_moves(gold_heads)) == gold_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oracle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Next move classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next move classifier is implemented by a feedforward network. The input to this network is a vector of integers representing words and tags, as described in the article by [Chen and Manning (2014)](https://www.aclweb.org/anthology/D14-1082/). Here we will use a simple feature model would look at the next word in the buffer and the topmost two words on the stack (for details, see below). The network processes this input as follows:\n",
    "\n",
    "1. embed the words and tags and concatenate the resulting embeddings\n",
    "2. send the concatenated embeddings through a linear layer followed by a ReLU\n",
    "3. pass the output of the non-linearity into a final softmax layer\n",
    "\n",
    "Your task is to implement the next move classifier based on this specification. Skeleton code for this is available in the next cell. The following choices are reasonable defaults for the hyperparameters of the network architecture:\n",
    "\n",
    "* width of the word embedding: 50\n",
    "* width of the tag embedding: 10\n",
    "* size of the hidden layer: 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NextMoveClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_words, num_tokens, word_dim=50, tag_dim=10, hidden_dim=180):\n",
    "        super().__init__()\n",
    "        self.word_embedding = nn.Embedding(num_words, word_dim)\n",
    "        self.tag_embedding = nn.Embedding(num_words, tag_dim)\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.Linear(3 * word_dim + 3 * tag_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = [\n",
    "            self.word_embedding(x[..., 0]),\n",
    "            self.word_embedding(x[..., 1]),\n",
    "            self.word_embedding(x[..., 2]),\n",
    "            self.tag_embedding(x[..., 3]),\n",
    "            self.tag_embedding(x[..., 4]),\n",
    "            self.tag_embedding(x[..., 5]),\n",
    "        ]\n",
    "        return self.pipe(torch.cat(embedded, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Parser, non-static part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains skeleton code for the non-static part of the parser. We ask you to implement a fixed-window model with the following features:\n",
    "\n",
    "0. word form of the next word in the buffer\n",
    "1. word form of the topmost word on the stack\n",
    "2. word form of the second-topmost word on the stack\n",
    "3. part-of-speech tag of the next word in the buffer\n",
    "4. part-of-speech tag of the topmost word on the stack\n",
    "5. part-of-speech tag of the second-topmost word on the stack\n",
    "\n",
    "Whenever the value of a feature is undefined, you should use the special value `PAD_IDX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(ParserCore):\n",
    "\n",
    "    def __init__(self, vocab_words, vocab_tags):\n",
    "        self.vocab_words = vocab_words\n",
    "        self.vocab_tags = vocab_tags\n",
    "        self.model = NextMoveClassifier(len(vocab_words), len(vocab_tags))\n",
    "\n",
    "    def featurize(self, word_indices, tag_indices, config):\n",
    "        i, stack, heads = config\n",
    "        x = torch.zeros(6, dtype=torch.long)\n",
    "        x[0] = word_indices[i] if i < len(word_indices) else PAD_IDX\n",
    "        x[1] = word_indices[stack[-1]] if len(stack) >= 1 else PAD_IDX\n",
    "        x[2] = word_indices[stack[-2]] if len(stack) >= 2 else PAD_IDX\n",
    "        x[3] = tag_indices[i] if i < len(tag_indices) else PAD_IDX\n",
    "        x[4] = tag_indices[stack[-1]] if len(stack) >= 1 else PAD_IDX\n",
    "        x[5] = tag_indices[stack[-2]] if len(stack) >= 2 else PAD_IDX\n",
    "        return x\n",
    "    \n",
    "    def predict(self, words, tags):\n",
    "        words = [self.vocab_words.get(w, UNK_IDX) for w in words]\n",
    "        tags = [self.vocab_tags.get(t, UNK_IDX) for t in tags]\n",
    "        config = self.initial_config(len(words))\n",
    "        valid_moves = self.valid_moves(config)\n",
    "        while valid_moves:\n",
    "            features = self.featurize(words, tags, config)\n",
    "            with torch.no_grad():\n",
    "                scores = self.model.forward(features)\n",
    "\n",
    "            # We may only predict valid transitions\n",
    "            best_score, pred_move = float('-inf'), None\n",
    "            for move in valid_moves:\n",
    "                if scores[move] > best_score:\n",
    "                    best_score, pred_move = scores[move], move\n",
    "\n",
    "            config = self.next_config(config, pred_move)\n",
    "            valid_moves = self.valid_moves(config)\n",
    "        i, stack, pred_heads = config\n",
    "        return pred_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should comply with the following specification:\n",
    "\n",
    "**vocab_words**\n",
    "\n",
    "> The word vocabulary of this parser.\n",
    "\n",
    "**vocab_tags**\n",
    "\n",
    "> The tag vocabulary of this parser.\n",
    "\n",
    "**__init__** (*self*, *vocab_words*, *vocab_tags*)\n",
    "\n",
    "> Creates a new parser, including the next move classifier that you implemented in Step&nbsp;3. The arguments *vocab_words* and *vocab_tags* are the dictionaries that you created in Step&nbsp;1.\n",
    "\n",
    "**featurize** (*self*, *word_indices*, *tag_indices*, *config*)\n",
    "\n",
    "> Returns the input vector to the next move classifier for the specified parser configuration. For the feature model described above, this will be a vector of length&nbsp;6. The *word_indices* and *tag_indices* are the vocabulary indices of the words and part-of-speech tags for the current input sentence, and *config* is a parser configuration as in Step&nbsp;2.\n",
    "\n",
    "**predict** (*self*, *words*, *tags*)\n",
    "\n",
    "> Predicts the list of all heads for the specified input sentence. The input sentence is specified in terms of the list of its *words* and the list of its *tags*. This method runs the arc-standard parsing algorithm, at each step asking the next move classifier for the next transition to take. More specifically, the parser uses the highest-scoring transition that is valid in the current configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Next move dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains skeleton code for a class `NextMoveDataset` that holds the training samples of the next move classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NextMoveDataset(Dataset):\n",
    "\n",
    "    def __init__(self, gold_sentences, parser):\n",
    "        self.xs = []\n",
    "        self.ys = []\n",
    "        for parsed_sentence in gold_sentences:\n",
    "            # Separate the words, gold tags, and gold heads\n",
    "            words, tags, gold_heads = zip(*parsed_sentence)\n",
    "\n",
    "            # Encode words and tags using the vocabularies\n",
    "            words = [parser.vocab_words.get(w, UNK_IDX) for w in words]\n",
    "            tags = [parser.vocab_tags[t] for t in tags]\n",
    "\n",
    "            # Call the oracle\n",
    "            for config, gold_move in oracle_moves(gold_heads):\n",
    "                self.xs.append(parser.featurize(words, tags, config))\n",
    "                self.ys.append(gold_move)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.xs[idx], self.ys[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class conforms to the following specification:\n",
    "\n",
    "**NextMoveDataset** (*gold_sentences*, *parser*)\n",
    "\n",
    "> Constructs a PyTorch dataset for the next move classifier. The dataset is constructed by calling *oracle_moves* on each gold-standard sentence in *gold_sentences*, and applying the `featurize` method to the resulting configurations to obtain feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last piece of the implementation of the baseline parser is the training loop. This should hold no surprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(train_data, n_epochs=1, batch_size=100, lr=1e-2):\n",
    "    # Create the vocabularies\n",
    "    vocab_words, vocab_tags = make_vocabs(train_data)\n",
    "\n",
    "    # Instantiate the parser\n",
    "    parser = Parser(vocab_words, vocab_tags)\n",
    "\n",
    "    # Create the next move dataset\n",
    "    dataset = NextMoveDataset(train_data, parser)\n",
    "\n",
    "    # Instantiate the data loader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Instantiate the optimizer\n",
    "    optimizer = optim.Adam(parser.model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0\n",
    "        n_examples = 0\n",
    "        with tqdm(total=sum(2*len(s)-1 for s in train_data)) as pbar:\n",
    "            for bx, by in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                output = parser.model.forward(bx)\n",
    "                loss = F.cross_entropy(output, by)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                n_examples += 1\n",
    "                pbar.set_postfix(loss=running_loss/n_examples)\n",
    "                pbar.update(len(bx))\n",
    "\n",
    "    return parser                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSER = train(TRAIN_DATA, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation, we use **unlabelled attachment score (UAS)**, which is defined as the percentage of all tokens to which the parser assigns the correct head (as per the gold standard). Note that the calculation excludes the pseudoword at position&nbsp;0 in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uas(parser, gold_sentences):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for sentence in gold_sentences:\n",
    "        words, tags, gold_heads = zip(*sentence)\n",
    "        pred_heads = parser.predict(words, tags)\n",
    "        for gold, pred in zip(gold_heads[1:], pred_heads[1:]):\n",
    "            total += 1\n",
    "            correct += int(gold == pred)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to test the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:.4f}'.format(uas(PARSER, DEV_DATA)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a parser trained for one epoch, you should expect to see an UAS of around 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That&rsquo;s all, folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
