{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7rjDygZIiKk"
   },
   "source": [
    "# Assignment 3: Graph-based dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh5QIRp7Ik_l"
   },
   "source": [
    "In this assignment, you will implement a simplified version of the dependency parser used by [Glavaš and Vulić (2021)](http://dx.doi.org/10.18653/v1/2021.eacl-main.270) (Figure&nbsp;1). This parser consists of a transformer encoder followed by a bi-affine layer that computes arc scores for all pairs of words. These scores are then used as logits in a classifier that predicts the syntactic head of each word. In contrast to the parser described in the paper, your parser will only support unlabelled parsing, i.e., you will implement an *arc classifier* but no *relation classifier*. As the encoder, you will use the [uncased BERT base model](https://huggingface.co/bert-base-uncased) from the [Transformers](https://huggingface.co/docs/transformers/main/en/index) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iS03p7MsInoY"
   },
   "source": [
    "We start by importing PyTorch and setting the device we will use for training and evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sQtzzHODsCq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FinF-eGYDsCq"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvQXUzwKIuvL"
   },
   "source": [
    "The data for this lab comes from the English Web Treebank from the [Universal Dependencies Project](http://universaldependencies.org); we distribute it here in the form of two JSON files. The code in the next cell below defines a PyTorch Dataset wrapper for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWP0Aw_5DsCr"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ParserDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        super().__init__()\n",
    "        with open(filename, 'rt', encoding='utf-8') as fp:\n",
    "            self.items = [[tuple(x) for x in json.loads(l)] for l in fp]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VRSzmXEIzst"
   },
   "source": [
    "We can now load the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utloa6yUDsCr"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = ParserDataset('en_ewt-ud-train.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsLK0m3NI2r5"
   },
   "source": [
    "A data set consists of **parsed sentences**. A parsed sentence is represented as a list of pairs. The first component of each pair (a string) represents a word. The second component (an integer) specifies the position of the word’s syntactic head, i.e., its parent in the dependency tree. Note that word positions are numbered starting at&nbsp;1. The special head position&nbsp;0 marks the root of the tree.\n",
    "\n",
    "Run the following code cell to see an example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJwRgwhlI5Dv",
    "outputId": "7738299c-a6d7-419a-a360-8c17f86bfb7c"
   },
   "outputs": [],
   "source": [
    "EXAMPLE_SENTENCE = TRAIN_DATA[531]\n",
    "\n",
    "EXAMPLE_SENTENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItQt5FbHI7zD"
   },
   "source": [
    "In this example the head of the pronoun *I* is the word at position&nbsp;2 – the verb *like*. The dependents of *like* are *I* (position&nbsp;1) and the noun *blog* (position&nbsp;4), as well as the final punctuation mark. Note that the pronoun *your* (position&nbsp;3) is misspelled as *yuor*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVhRe6GeDsCs"
   },
   "source": [
    "## Problem 1: Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XeyM5mjI_iL"
   },
   "source": [
    "To feed parsed sentences to BERT, we need to tokenise them and encode the resulting tokens as integers in the model vocabulary. We start by loading the BERT tokeniser using the Auto classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PX020ULADsCs"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc-KSh_NJDrV"
   },
   "source": [
    "We can call the tokeniser on the example sentence as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j46u6v69JE_N",
    "outputId": "8d1c9de2-4c39-4f3b-8c7b-92877d138b1f"
   },
   "outputs": [],
   "source": [
    "TOKENIZER([w for w, _ in EXAMPLE_SENTENCE], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqjaNO_FJH7j"
   },
   "source": [
    "Note that we use the *is_split_into_words* keyword argument to indicate that the input is already pre-tokenised (split on whitespace).\n",
    "\n",
    "The BERT tokeniser segments each word (pre-token) into one or several subword tokens, and we want to keep track of which of these were introduced by which word. To this end, for each actual word in a parsed sentence we compute the corresponding span in the tokens list.\n",
    "\n",
    "To illustrate this, consider again the tokenisation of the example sentence. Note that in order to match the default behaviour when calling the tokeniser, we explicitly add the special tokens at the beginning and at the end of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2fFF4bEJKmZ",
    "outputId": "9fbb6b88-d7b4-4134-a4d1-bde532eb9f90"
   },
   "outputs": [],
   "source": [
    "TOKENIZER.tokenize([w for w, _ in EXAMPLE_SENTENCE], add_special_tokens=True, is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJyMTKZ5JNEA"
   },
   "source": [
    "For this sentence, we would like to compute the following token spans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQ0WW4vjJPnC",
    "outputId": "eddd659b-160a-478c-8a44-52474e5d398c"
   },
   "outputs": [],
   "source": [
    "[(1, 2), (2, 3), (3, 5), (5, 6), (6, 7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kw4lnuhCJSvW"
   },
   "source": [
    "Each of these spans covers a single token, except for the span `(3, 5)` which covers the tokens `yu` and `##or`. Note that token indices start at&nbsp;1, as position&nbsp;0 is occupied by the special `[CLS]` token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3IMDO6OJVWU"
   },
   "source": [
    "The next cell contains skeleton code for a function `encode` that takes a tokeniser and a batch of sentences and returns the tokeniser&rsquo;s encoded input as well as the corresponding token spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukRk8rg2JR_a"
   },
   "outputs": [],
   "source": [
    "def encode(tokenizer, sentences):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKgDc2EeJZF-"
   },
   "source": [
    "Implement this function to match the following specification:\n",
    "\n",
    "**encode** (*tokenizer*, *sentences*):\n",
    "\n",
    "> Uses the specified *tokenizer* to encode a batch of parsed sentences (*sentences*). This returns a pair consisting of a [`BatchEncoding`](https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.BatchEncoding) and a matching batch of token spans (as explained above). The `BatchEncoding` is the standard batch encoding, including the token ids to be fed to a model. Its inner content have been converted to PyTorch tensors. Token indexes start at&nbsp;1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Si4kaLKqJeCd"
   },
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "To test you code, call `encode` on the example sentence and check that output matches your expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgire81SDsCt"
   },
   "source": [
    "### Problem 2: Merging tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBxk-pHrJnEu"
   },
   "source": [
    "BERT gives us a representation for each *token* in the input sequence. To compute scores between pairs of *words*, we need to combine the token representations that correspond to each word. A standard strategy for this is to take their element-wise mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cU18rchvJtCl"
   },
   "source": [
    "The next cell contains skeleton code for a function `merge_tokens` that implements this strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5RvfLl2JuJ6"
   },
   "outputs": [],
   "source": [
    "def merge_tokens(tokens, token_spans):\n",
    "    # TODO: Replace the next line with your own code\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ouws6LlBJvjB"
   },
   "source": [
    "Implement this function to match the following specification:\n",
    "\n",
    "**merge_tokens** (*tokens*, *token_spans*)\n",
    "\n",
    "> Takes a batch of token vectors (*tokens*) and a batch of token spans (*token_spans*) and returns a new batch of word-level representations, computed as explained above. The token vectors are a tensor of shape (*batch_size*, *num_tokens*, *hidden_dim*). The token spans are a nested list of integer pairs as computed in Problem&nbsp;1. The result is a tensor of shape (*batch_size*, *max_num_words*, *hidden_dim*), where *max_num_words* denotes the maximum number of words in any sentence in the batch. Entries corresponding to padding are represented by the zero vector of size *hidden_dim*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VznhtnyfJ1VL"
   },
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "To test you code, create a sample input to `merge_tokens` and check that the output matches your expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xa1mDVyDsCv"
   },
   "source": [
    "## Problem 3: Biaffine layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXN0-zxHKI9b"
   },
   "source": [
    "Your next task is to implement the bi-affine layer. Given matrices $X \\in \\mathbb{R}^{m \\times h}$ and $X' \\in \\mathbb{R}^{n \\times h}$, this layer computes a matrix $Y \\in \\mathbb{R}^{m \\times n}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVGYLy4yKKpz"
   },
   "source": [
    "$$\n",
    "Y = X W X'{}^\\top + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK-KP4uBKPC2"
   },
   "source": [
    "where $W \\in \\mathbb{R}^{h \\times h}$ and $b \\in \\mathbb{R}$ are learnable weight and bias parameters. In the context of the dependency parser, the matrices $X$ and $X'$ hold the encodings of all words in the input sentence, and the entries of the matrix $Y$ are interpreted as scores of dependency arcs between words. More specifically, the entry $Y_{ij}$ represents the score of an arc from a head word at position&nbsp;$j$ to a dependent at position&nbsp;$i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu8CUBquKYCS"
   },
   "source": [
    "\n",
    "The following cell contains skeleton code for the implementation of the bi-affine layer. Implement this layer according to the specification above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wg9aXqufKZOW"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Biaffine(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        # TODO: Replace the next line with your own code\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3PcvIFaKbZp"
   },
   "source": [
    "**⚠️ Note that your implementation should be able to handle *batches* of input sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAd-zzVNKjDY"
   },
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "To test you code, create a sample input to the bi-affine layer as well as suitable weights and biases and check that the output of the `forward` method matches your expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WSJBt2TDsCw"
   },
   "source": [
    "## Problem 4: Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6dSBKcaKr2h"
   },
   "source": [
    "We are now ready to put the two main components of the parser together: the encoder (BERT) and the bi-affine layer that computes the arc scores. We also add a dropout layer between the two components. The following code cell contains skeleton code for the parsing model with the `init` method already complete. Your task is to implement the `forward` method. Have another look at the paper to understand how things need to be wired up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdk-gzAwKx3R"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from transformers import BertConfig, BertModel, BertPreTrainedModel\n",
    "\n",
    "class BertForParsing(BertPreTrainedModel):\n",
    "\n",
    "    config_class = BertConfig\n",
    "\n",
    "    def __init__(self, config, dropout=0.1):\n",
    "        super().__init__(config)\n",
    "        self.encoder = BertModel(config)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.biaffine = Biaffine(config.hidden_size)\n",
    "\n",
    "    def forward(self, encoded_input, token_spans):\n",
    "        # TODO: Replace the next line with your own code\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5xkXMBLKyjB"
   },
   "source": [
    "Implement the `forward` method to match the following specification:\n",
    "\n",
    "**forward** (*encoded_input*, *token_spans*)\n",
    "\n",
    "> Takes a tokeniser-encoded batch of sentences (of type `BatchEncoding`) and a corresponding batch of token spans and returns a tensor with scores between all words in the input. More specifically, the output tensor $Y$ has shape (*batch_size*, *num_words*, *num_words+1*), where the entry $Y_{bij}$ represents the score of an arc from a head word at position&nbsp;$j$ to a dependent at position&nbsp;$i$ in the $b$th sentence of the batch. Note that the number of possible heads is one greater than the number of possible dependents because the heads include the special token `[CLS]` (at position&nbsp;0), which the paper uses to represent the root vertex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AljpxatKLCT1"
   },
   "source": [
    "### 🤞 Test your code\n",
    "\n",
    "To test you code, instantiate the parsing model and feed it the tokenised example sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMURWRFxDsCz"
   },
   "source": [
    "## Batching the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQyosu7iLLSe"
   },
   "source": [
    "We are now almost ready to train the parser. The missing piece is a data collator that prepares a batch of parsed sentences:\n",
    "\n",
    "* tokenises the sentences and extracts token spans using `encode` (Problem&nbsp;1)\n",
    "* constructs the ground-truth head tensor needed to compute the loss (Problem&nbsp;2)\n",
    "\n",
    "The code in the next cell implements these two steps. For pseudo-words introduced through padding, we assign a head index of −100. This value is ignored by PyTorch’s cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fADPsMqBDsCz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ParserBatcher(object):\n",
    "\n",
    "    def __init__(self, tokenizer, device=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, parser_inputs):\n",
    "        encoded_input, start_indices = encode(self.tokenizer, parser_inputs)\n",
    "\n",
    "        # Get the maximal number of words, for padding\n",
    "        max_num_words = max(len(s) for s in parser_inputs)\n",
    "\n",
    "        # Construct tensor containing the ground-truth heads\n",
    "        all_heads = []\n",
    "        for parser_input in parser_inputs:\n",
    "            words, heads = zip(*parser_input)\n",
    "            heads = list(heads)\n",
    "            heads.extend([-100] * (max_num_words - len(heads)))  # -100 will be ignored\n",
    "            all_heads.append(heads)\n",
    "        all_heads = torch.LongTensor(all_heads)\n",
    "\n",
    "        # Send all data to the specified device\n",
    "        if self.device:\n",
    "            encoded_input = encoded_input.to(self.device)\n",
    "            all_heads = all_heads.to(self.device)\n",
    "\n",
    "        return encoded_input, start_indices, all_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0Q3t3FCDsC1"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDpnXdhrLP0Y"
   },
   "source": [
    "Finally, here is the training loop of the parser. Most of it is quite standard. The training loss of the parser is the cross-entropy between the head scores and the ground truth head positions. In other words, the parser is trained as a classifier that predicts the position of each word&rsquo;s head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fEggHBVDsC1"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(dataset, n_epochs=1, lr=1e-5, batch_size=8):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForParsing.from_pretrained('bert-base-uncased').to(DEVICE)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=ParserBatcher(tokenizer, device=DEVICE))\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        n_batches = 0\n",
    "        with tqdm(total=len(dataset)) as pbar:\n",
    "            pbar.set_description(f'Epoch {epoch+1}')\n",
    "            for encoded_input, token_spans, gold_heads in data_loader:\n",
    "                optimizer.zero_grad()\n",
    "                head_scores = model.forward(encoded_input, token_spans)\n",
    "                loss = F.cross_entropy(head_scores.flatten(0, -2), gold_heads.view(-1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                n_batches += 1\n",
    "                pbar.set_postfix(loss=running_loss/n_batches)\n",
    "                pbar.update(len(token_spans))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ors-KG5GL_Dc"
   },
   "source": [
    "We are now ready to train the parser. With a GPU, you should expect training times of approximately 3&nbsp;minutes per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_Esf9zeDsC1",
    "outputId": "126f4b76-9cdc-43af-f9fa-92f4f2df1755"
   },
   "outputs": [],
   "source": [
    "PARSING_MODEL = train(TRAIN_DATA, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4K-LoRaDsC1"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEMuZ5jFMpOA"
   },
   "source": [
    "The parser is evaluated using unlabelled attachment score (UAS), which is the percentage of words that have been assigned their correct heads. Note that pseudo-words corresponding to padding (which we marked with the special head index −100 above) must be excluded from this calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbLj708kDsC1"
   },
   "outputs": [],
   "source": [
    "DEV_DATA = ParserDataset('en_ewt-ud-dev.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uu3EbTvJDsC2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate(model, dataset, batch_size=8):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    data_loader = DataLoader(DEV_DATA, batch_size=batch_size, collate_fn=ParserBatcher(tokenizer, device=DEVICE))\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    model.eval()\n",
    "    with tqdm(total=len(dataset)) as pbar:\n",
    "        for encoded_input, token_spans, gold_heads in data_loader:\n",
    "            with torch.no_grad():\n",
    "                head_scores = model.forward(encoded_input, token_spans)\n",
    "                pred_heads = torch.argmax(head_scores, dim=-1)\n",
    "            mask = gold_heads.ne(-100)\n",
    "            n_correct += torch.sum(pred_heads[mask] == gold_heads[mask])\n",
    "            n_total += torch.sum(mask)\n",
    "            pbar.update(len(token_spans))\n",
    "    return n_correct / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3ms2kDiDsC2"
   },
   "outputs": [],
   "source": [
    "evaluate(PARSING_MODEL, DEV_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5Mn04REMVuC"
   },
   "source": [
    "**Your notebook must contain output demonstrating at least 88% UAS on the development data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiRFt3cjMbCO"
   },
   "source": [
    "That’s it! Congratulations on finishing the last assignment of this course! 🥳"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_Parser Panic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
