{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d272c02",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db260ca",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to introduce you to the basics of [PyTorch](https://pytorch.org), the deep learning framework that we will be using for the labs.\n",
    "\n",
    "Many good introductions to PyTorch are available online, including the [60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) on the official PyTorch website. This notebook is designed to put focus on those basics that you will encounter in the labs. Beyond the notebook, you will also need to get comfortable with the [PyTorch documentation](https://pytorch.org/docs/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349afbb4",
   "metadata": {},
   "source": [
    "We start by importing the PyTorch module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b4ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16747627",
   "metadata": {},
   "source": [
    "The following code prints the current version of the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1443b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a301db",
   "metadata": {},
   "source": [
    "The version of PyTorch at the time of writing this notebook was 1.10.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2c610b",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de64ccf",
   "metadata": {},
   "source": [
    "The fundamental data structure in PyTorch is the **tensor**, a multi-dimensional matrix containing elements of a single numerical data type. Tensors are similar to *arrays* as you may know them from NumPy or MATLAB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad407e",
   "metadata": {},
   "source": [
    "### Creating tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a090537",
   "metadata": {},
   "source": [
    "One way to create a tensor is to call the function `torch.tensor()` on a Python list or NumPy array.\n",
    "\n",
    "The code in the following cell creates a 2-dimensional tensor with 4 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18445c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0, 1], [2, 3]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71a273",
   "metadata": {},
   "source": [
    "Each tensor has a *shape*, which specifies the number and sizes of its dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a08d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae63de4",
   "metadata": {},
   "source": [
    "Each tensor also has a *data type* for its elements. [More information about data types](https://pytorch.org/docs/stable/tensors.html#data-types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00cceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7434e642",
   "metadata": {},
   "source": [
    "When creating a tensor, you can explicitly pass the intended data type as a keyword argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([[0, 1], [2, 3]], dtype=torch.float)\n",
    "y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335acc0",
   "metadata": {},
   "source": [
    "For many data types, there also exists a specialised constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271dffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([[0, 1], [2, 3]])\n",
    "z.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859d7b0",
   "metadata": {},
   "source": [
    "### More creation operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c479202",
   "metadata": {},
   "source": [
    "Create a 3D-tensor of the specified shape filled with the scalar value zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408406a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(2, 3, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c1bb22",
   "metadata": {},
   "source": [
    "Create a 3D-tensor filled with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf74ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd69703",
   "metadata": {},
   "source": [
    "Create a tensor with the same shape as another one, but filled with ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.ones_like(x)\n",
    "y    # shape: [2, 3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f402305",
   "metadata": {},
   "source": [
    "For a complete list of tensor-creating operations, see [Creation ops](https://pytorch.org/docs/stable/torch.html#creation-ops)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd8e89",
   "metadata": {},
   "source": [
    "### Embrace vectorisation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2fd635",
   "metadata": {},
   "source": [
    "Iteration is of one the most useful techniques for processing data in Python. However, you should **not loop over tensors**. Instead, you should be looking at *vectorising* any operations. This is because looping over tensors is slow, while vectorised operations on tensors are fast (and can be made even faster when the code is run on a GPU). To illustrate this point, let us create a 1D-tensor containing the first 1M integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64607413",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(1000000)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac363b4",
   "metadata": {},
   "source": [
    "Summing up the elements of the tensor using a loop is relatively slow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f7446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(i for i in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfd223e",
   "metadata": {},
   "source": [
    "Doing the same thing using a tensor operation is much faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c969127",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb92c5",
   "metadata": {},
   "source": [
    "### Indexing and slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b09b2ca",
   "metadata": {},
   "source": [
    "To access the contents of a tensor, you can use an extended version of Python’s syntax for indexing and slicing. Essentially the same syntax is used by NumPy. For more information, see [Indexing on ndarrays](https://numpy.org/doc/stable/user/basics.indexing.html).\n",
    "\n",
    "To illustrate this, we create a 3D-tensor with random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cda991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8158d193",
   "metadata": {},
   "source": [
    "Index an element by a 3D-coordinate; this gives a 0D-tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead7098",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828b34e",
   "metadata": {},
   "source": [
    "(If you want the result as a non-tensor, use the method [`item()`](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html#torch.Tensor.item).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad7eb6",
   "metadata": {},
   "source": [
    "Index the second element; this gives a 2D-tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e664be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5cc13e",
   "metadata": {},
   "source": [
    "Index the second-to-last element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c682a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d37e5",
   "metadata": {},
   "source": [
    "Slice out the sub-tensor with elements from index 1 onwards; this gives a 3D-tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9938c",
   "metadata": {},
   "source": [
    "Here is a more complex example of slicing. As in Python, the colon `:` selects all indices of a dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073cd642",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,:,2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561a6fa7",
   "metadata": {},
   "source": [
    "The syntax for indexing and slicing is very powerful. For example, the same effect as in the previous cell can be obtained with the following code, which uses the ellipsis (`...`) to match all dimensions but the ones explicitly mentioned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[...,2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb92b3",
   "metadata": {},
   "source": [
    "### Creating views"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a61b1",
   "metadata": {},
   "source": [
    "You will sometimes want to use a tensor with a different shape than its initial shape. In these situations, you can **re-shape** the tensor or create a **view** of the tensor. The latter is preferable because views can share the same data as their base tensors and thus do not require copying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dbee10",
   "metadata": {},
   "source": [
    "We create a 3D-tensor of 12 random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d13169",
   "metadata": {},
   "source": [
    "Create a view of this tensor as a 2D-tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.view(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759152f",
   "metadata": {},
   "source": [
    "When creating a view, the special size `-1` is inferred from the other sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde21b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.view(3, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182c249",
   "metadata": {},
   "source": [
    "Modifying a view affects the data in the base tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(2, 3, 2)\n",
    "z = y.view(3, 4)\n",
    "z[2, 3] = 42\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b180762",
   "metadata": {},
   "source": [
    "### More viewing operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46f63e",
   "metadata": {},
   "source": [
    "There are a few other useful methods that create views. [More information about views](https://pytorch.org/docs/stable/tensor_view.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdb2d6",
   "metadata": {},
   "source": [
    "The [`permute()`](https://pytorch.org/docs/stable/generated/torch.permute.html) method returns a view of the base tensor with some of its dimensions permuted. In the example, we maintain the first dimension but swap the second and the third dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e672ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.permute(0, 2, 1)\n",
    "print(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51068591",
   "metadata": {},
   "source": [
    "The [`unsqueeze()`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html) method returns a tensor with a dimension of size one inserted at the specified position. This is useful e.g. in the training of neural networks when you want to create a batch with just one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2fe5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.unsqueeze(0)\n",
    "print(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c6449",
   "metadata": {},
   "source": [
    "The inverse operation to [`unsqueeze()`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html) is [`squeeze()`](https://pytorch.org/docs/stable/generated/torch.squeeze.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88aa622",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.squeeze(0)\n",
    "print(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73239829",
   "metadata": {},
   "source": [
    "### Re-shaping a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa2b33c",
   "metadata": {},
   "source": [
    "There are some cases where you cannot create a view and need to explicitly re-shape a tensor. In particular, this happens when the data in the base tensor and the view are not in contiguous regions of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69762d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5add2f3",
   "metadata": {},
   "source": [
    "We permute the tensor `x` to create a new tensor `y` in which the data is no longer consecutive in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.permute(0, 2, 1)\n",
    "# y = y.view(-1)    # raises a runtime error\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716d06e",
   "metadata": {},
   "source": [
    "In such a case, you can explicitly re-shape the tensor, which will *copy* the data if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faaca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.permute(0, 2, 1)\n",
    "y = y.reshape(-1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c057aa8c",
   "metadata": {},
   "source": [
    "Modifying a reshaped tensor *will not necessarily* change the data in the base tensor. This depends on whether the reshaped tensor is able to share the data with the base tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b135ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(2, 3, 2)\n",
    "y = y.permute(0, 2, 1)    # if commented out, data can be shared\n",
    "z = y.reshape(-1)\n",
    "z[0] = 42\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f756729",
   "metadata": {},
   "source": [
    "## Computing with tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f823be",
   "metadata": {},
   "source": [
    "### Element-wise operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e6a54",
   "metadata": {},
   "source": [
    "Unary mathematical operations defined on numbers can be ‘lifted’ to tensors by applying them element-wise. This includes multiplication by a constant, exponentiation (`**`), taking roots ([`torch.sqrt()`](https://pytorch.org/docs/stable/generated/torch.sqrt.html)), and the logarithm ([`torch.log()`](https://pytorch.org/docs/stable/generated/torch.sqrt.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3)\n",
    "print(x)\n",
    "x * 2    # element-wise multiplication with 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678ef81",
   "metadata": {},
   "source": [
    "Similarly, we can do binary mathematical operations on tensors with the same shape. For example, the Hadamard product of two tensors $X$ and $Y$ is the tensor $X \\odot Y$ obtained by the element-wise multiplication of the elements of $X$ and $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec606e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3)\n",
    "y = torch.rand(2, 3)\n",
    "torch.mul(x, y)    # shape: [2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295cb664",
   "metadata": {},
   "source": [
    "The Hadamard product can be written more succinctly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb34f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2acb1c5",
   "metadata": {},
   "source": [
    "### Matrix product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b51a7",
   "metadata": {},
   "source": [
    "When computing the matrix product between two tensors $X$ and $Y$, the sizes of the last dimension of $X$ and the first dimension of $Y$ must match. The shape of the resulting tensor is the concatenation of the shapes of $X$ and $Y$, with the last dimension of $X$ and the first dimension of $Y$ removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3)\n",
    "y = torch.rand(3, 5)\n",
    "torch.matmul(x, y)    # shape: [2, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3533dbe",
   "metadata": {},
   "source": [
    "The matrix product can be written more succinctly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3867dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568d1da",
   "metadata": {},
   "source": [
    "### Sum and argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec492a9",
   "metadata": {},
   "source": [
    "Let us define a tensor of random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351fbeb",
   "metadata": {},
   "source": [
    "You have already seen that we can compute the sum of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb154dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08717601",
   "metadata": {},
   "source": [
    "There is a second form of the sum operation where we can specify the dimension along which the sum should be computed. This will return a tensor with the specified dimension removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(x, dim=0)    # shape: [3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1229dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(x, dim=1)   # shape: [2, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9508eed4",
   "metadata": {},
   "source": [
    "The same idea also applies to the operation [`argmax()`](https://pytorch.org/docs/stable/generated/torch.argmax.html) which returns the index of the component with the maximal value along the specified dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d587eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(x)    # index of the highest component across all dimensions, numbered in consecutive order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(x, dim=0)   # index of the highest component across the first dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a39aa",
   "metadata": {},
   "source": [
    "### Concatenating tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa1842",
   "metadata": {},
   "source": [
    "A list of tensors can be combined into one long tensor by concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4757f8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3)\n",
    "y = torch.rand(3, 3)\n",
    "z = torch.cat([x, y])\n",
    "print(z)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbde559",
   "metadata": {},
   "source": [
    "You can also concatenate along a specific dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "print(torch.cat([x, y], dim=0))\n",
    "print(torch.cat([x, y], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b90bd",
   "metadata": {},
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2aebcf",
   "metadata": {},
   "source": [
    "The term *broadcasting* describes how PyTorch treats tensors with different shapes. Subject to certain constraints, the ‘smaller’ tensor is ‘broadcast’ across the larger tensor so that they have compatible shapes. Broadcasting is a way to avoid looping. In short, if a PyTorch operation supports broadcasting, then its Tensor arguments can be automatically expanded to be of equal sizes (without making copies of the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee39878",
   "metadata": {},
   "source": [
    "In the simplest case, two tensors have the same shapes. This is the case for the matrix `x @ W` and the bias vector `b` in the linear model below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 2)\n",
    "W = torch.rand(2, 3)\n",
    "b = torch.rand(1, 3)\n",
    "z = x @ W    # shape: [1, 3]\n",
    "z = z + b    # shape: [1, 3]\n",
    "print(z)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b29c1e",
   "metadata": {},
   "source": [
    "Now suppose that we have a whole batch of inputs. Watch what happens when adding the bias vector `b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b37cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(5, 2)\n",
    "Z = X @ W    # shape: [5, 3]\n",
    "Z = Z + b    # shape: [5, 3]    Broadcasting happens here!\n",
    "print(Z)\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f06a524",
   "metadata": {},
   "source": [
    "In the example, broadcasting expands the shape of `b` from $[1, 3]$ into $[5, 3]$. The matrix `Z` is formed by effectively adding `b` *to each row* of `X`. However, this is not implemented by a Python loop but happens implicitly through broadcasting.\n",
    "\n",
    "PyTorch uses the same broadcasting semantics as NumPy. [More information about broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11403a9",
   "metadata": {},
   "source": [
    "To be expanded!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
