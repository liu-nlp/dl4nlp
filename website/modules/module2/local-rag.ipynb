{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc09aa7d",
   "metadata": {},
   "source": [
    "## Local RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9208bf",
   "metadata": {},
   "source": [
    "This is the local RAG system that Marco showcased during the last session of Meeting&nbsp;2. It requires you to install [Ollama](https://www.ollama.com/) and download the `nomic-text-embed` (for querying) and `llama-3` (for generating queries and answering questions) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2edd44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --q unstructured langchain\n",
    "!pip install --q \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6395c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59db99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"some.pdf\"    # insert your PDF filename here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e3d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab849b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'001\\n\\n002\\n\\n003\\n\\n004\\n\\n005\\n\\n006\\n\\n007\\n\\n008\\n\\n009\\n\\n010\\n\\n011\\n\\n012\\n\\n013\\n\\n014\\n\\n015\\n\\n016\\n\\n017\\n\\n018\\n\\n019\\n\\n020\\n\\n021\\n\\n022\\n\\n023\\n\\n024\\n\\n025\\n\\n026\\n\\n027\\n\\n028\\n\\n029\\n\\n030\\n\\n031\\n\\n032\\n\\n033\\n\\n034\\n\\n035\\n\\n036\\n\\n037\\n\\n038\\n\\n039\\n\\n040\\n\\nPost-Project Paper, TDDD09\\n\\nJoline Hellström\\n\\n1 Describe\\n\\nIn our project, we explored a method called back- translation and investigated its impact on machine translation. Moreover, we examined if the effi- ciency of back-translation depends on how closely related the source and target language are. To in- vestigate this, we trained models to translate from Swedish to the respective target languages Nor- wegian, Finnish, and Northern Sámi (also known as Davvisámegiella). Meanwhile Norwegian is closely related to Swedish, neither Finnish nor Northern Sámi comes from the same family of languages as Swedish.\\n\\nmodel that translates from Swedish to the target lan- guage and a reverse model that translates from the target language to Swedish. In the next step, the Swedish sentences in the second half of the original bilingual data are removed to resemble monolin- gual data. This monolingual data is translated with the reverse model to create corresponding Swedish translations. These translations and the monolin- gual data form a synthetic bilingual dataset and the back-translation has been performed. The syn- thetic bilingual dataset is finally combined with the original bilingual data that was used to train the baseline and reverse model, and a final model translating from Swedish to the target language is finally trained with this combined data.\\n\\nThe Nordic languages are small in comparison to other widely spoken languages like English. By doing this project, I wanted to learn more about the issues surrounding the limited availability of bilingual corpora for low-resource languages in the field of NLP. To translate between Swedish and Northern Sámi is especially interesting in my opin- ion since Northern Sámi is one of the languages spoken by the native Sámi people in Sweden and the state is obligated to provide information in their language.\\n\\nBilingual training data is needed to train machine translation models. In the absence of it, an alterna- tive option is to translate monolingual text written in a low-resource language to the desired source language with a machine translation model. The original monolingual data and the translations then form a new bilingual dataset. To create synthetic bilingual data in this way is the idea behind back- translation. To investigate the impacts of back- translation, we designed a system consisting of multiple machine translation models, and the archi- tecture is illustrated in Figure 1.\\n\\nThe system starts with a bilingual dataset consist- ing of sentences in Swedish and the target language. The system then splits this data into approximately two halves. One half is used to train both a baseline\\n\\nFigure 1: The figure shows an illustration of the system developed in the project.\\n\\nThe bilingual corpus MultiParaCrawl from Opus was used for the Swedish-to-Norwegian and the Swedish-to-Finnish model. The Swedish-Northern Sámi dataset was retrieved by a master’s student who web-scraped Norwegian sites with Sámi trans- lations. The Norwegian sentences were translated into Swedish with Google Translate to create a Swedish-Northern Sámi dataset. Since we could not find any other dataset for the language pair, we\\n\\n1\\n\\n041\\n\\n042\\n\\n043\\n\\n044\\n\\n045\\n\\n046\\n\\n047\\n\\n048\\n\\n049\\n\\n050\\n\\n051\\n\\n052\\n\\n053\\n\\n054\\n\\n055\\n\\n056\\n\\n057\\n\\n058\\n\\n059\\n\\n060\\n\\n061\\n\\n062\\n\\n063\\n\\n064\\n\\n065\\n\\n066\\n\\n067\\n\\n068\\n\\n069\\n\\n070\\n\\n071\\n\\n072\\n\\n073\\n\\n074\\n\\n075\\n\\n076\\n\\n077\\n\\n078\\n\\n079\\n\\n080\\n\\n081\\n\\n082\\n\\n083\\n\\n084\\n\\n085\\n\\n086\\n\\n087\\n\\n088\\n\\n089\\n\\n090\\n\\n091\\n\\n092\\n\\n093\\n\\n094\\n\\n095\\n\\n096\\n\\n097\\n\\n098\\n\\n099\\n\\n100\\n\\n101\\n\\n102\\n\\n103\\n\\n104\\n\\n105\\n\\n106\\n\\n107\\n\\n108\\n\\n109\\n\\n110\\n\\n111\\n\\n112\\n\\n113\\n\\n114\\n\\n115\\n\\nended up using it. In total, 82K sentences from each dataset were used during the training.\\n\\nAs a group, we read the article Feldman and Coto-Solano (2020) to understand the concept of back-translation. However, I also did some addi- tional reading on the topic and found out that back- translation is not really a method on its own as we first thought, but rather a concept for which mul- tiple implementation approaches exist even if the goal always is to generate synthetic source data. In Edunov et al. (2018), multiple implementations of back-translation are presented and the results sug- gest that greedy search or beam search are the most suitable in low-resource settings like ours with 80K sentences of training data. Both options are simply searches as suggested by the name, but the rele- vant difference is that only the best translation is outputted while beam search is an algorithm that produces several possible \"best guesses\" for the translation.\\n\\nOne of the tasks I worked on during the project was the implementation of the baseline. This in- cluded deciding what type of machine translation model we wanted to use in the project. From the additional reading I did on the topic of back- translation, I found a lot of existing work such as Edunov et al. (2018), Dione et al. (2022), and Przystupa and Abdul-Mageed (2019) where the transformer was used in combination with back- translation and it therefore seemed like a good op- tion for us. However, I also learned that the trans- former model can struggle with low-resource lan- guages and that statistic machine translation mod- els might even be a preferable choice (Koehn and Knowles, 2017). In Przystupa and Abdul-Mageed (2019), both the transformer and an LSTM model were used to implement back-translation success- fully but the LSTM model performed better for the language pair with the smallest dataset. In the end, we decided to use the transformer model, which is described in more detail in section 2, as it is a widely used state-of-the-art choice even if I had learned about its drawbacks.\\n\\nThe open-source neural machine translation framework openNMT provides the tools needed to pre-process training data, train transformer mod- els, and evaluate their performance. We used open- NMT in the project and I gained experience with the framework by implementing the baseline. I solely had to create a configuration to adjust the model’s parameters and start the training with our\\n\\n2\\n\\ndatasets. In our project, we had limited GPU hours and scheduled time, which resulted in us only train- ing the model for 3000 steps even if it could com- promise the final performance. This means that the model was trained on 3000 batches of 4096 tokens. After the back-translation was implemented, we trained all models in the system. We evaluated the performance of the models by using BLEU scores and the metric is described in more detail in section 2. A high score indicates that the trans- lations from the model are good. We received the results presented in Table 1 and they indicate that the performance improved from the baseline to the final model for all of the target languages since the BLEU score increased. Our results also in- dicate that back-translation improves the perfor- mance more if the source and target language are not related since the smallest improvement (∆) was achieved for the Swedish-to-Norwegian model.\\n\\nTable 1: The results from the evaluation for each lan- guage pair’s baseline and final model and the difference between them, ∆, are shown in the table.\\n\\nLanguage Pair Baseline Final model ∆ Swe-Nor Swe-Fin Swe-Sámi\\n\\n35.86 22.68 4.64\\n\\n36.41 30.95 24.35\\n\\n0.55 8.27 19.71\\n\\n2 Examine\\n\\nSince we used BLEU (Bilingual Evaluation Under- study) (Papineni et al., 2002) scores to evaluate our models, it is a relevant concept to understand to interpret our results. The goal of the metric is to provide an alternative evaluation method to using human references to judge the quality of machine translations. BLEU is based on n-grams which is a sequence of n consecutive tokens in a sentence. Simplified, the BLEU score is calculated by first counting all n-grams in the suggested translation that also exists in the reference translations, and then dividing this number by the total number of n-grams in the suggested translation. This ratio lies in the range of 0 to 1 but is often scaled to the range 0-100 like in our presented results in Table 1.\\n\\nSince, the topics of low-resource data and back- translation were not discussed thoroughly in the course, and, therefore, I did not know if the results were reasonable. To gain more insight into what BLEU scores one might expect in our experimental setting, I found the results in Dione et al. (2022)\\n\\n116\\n\\n117\\n\\n118\\n\\n119\\n\\n120\\n\\n121\\n\\n122\\n\\n123\\n\\n124\\n\\n125\\n\\n126\\n\\n127\\n\\n128\\n\\n129\\n\\n130\\n\\n131\\n\\n132\\n\\n133\\n\\n134\\n\\n135\\n\\n136\\n\\n137\\n\\n138\\n\\n139\\n\\n140\\n\\n141\\n\\n142\\n\\n143\\n\\n144\\n\\n145\\n\\n146\\n\\n147\\n\\n148\\n\\n149\\n\\n150\\n\\n151\\n\\n152\\n\\n153\\n\\n154\\n\\n155\\n\\n156\\n\\n157\\n\\n158\\n\\n159\\n\\n160\\n\\n161\\n\\n162\\n\\n163\\n\\n164\\n\\n165\\n\\n166\\n\\n167\\n\\n168\\n\\n169\\n\\n170\\n\\n171\\n\\n172\\n\\n173\\n\\n174\\n\\n175\\n\\n176\\n\\n177\\n\\n178\\n\\n179\\n\\n180\\n\\n181\\n\\n182\\n\\n183\\n\\n184\\n\\n185\\n\\n186\\n\\n187\\n\\n188\\n\\n189\\n\\n190\\n\\n191\\n\\n192\\n\\n193\\n\\n194\\n\\n195\\n\\n196\\n\\n197\\n\\n198\\n\\n199\\n\\n200\\n\\n201\\n\\n202\\n\\n203\\n\\n204\\n\\n205\\n\\n206\\n\\n207\\n\\nhelpful. The results show that back-translation im- proved the transformer model’s result with 1.2 and 1.4 BLEU points in one direction of translation, but decreased it with 9.0 and 8.5 in the other, which means that the results differ quite a lot from what we found for Swe-Fin and Swe-Sámi. By compar- ing our results to Feldman and Coto-Solano (2020), one can also conclude that our ∆-values in Table 1. One suggested explanation for this could be that the poor initial baseline performances enabled large improvements since the trend is that a lower performance for the baseline results in a larger im- provement.\\n\\nSince the BLEU scores are based on the provided translations, it becomes difficult to know how well our system performs as we, for instance, cannot ensure that the Swedish to Northern Sámi transla- tions even are of high quality. From my previous knowledge, I did not quite understand this aspect of BLEU as I just had considered it to be a met- ric that reassembled human judgment. This new insight emphasized an area in which I found my knowledge inadequate, namely the importance of quality. Since I do not speak Northern Sámi, it was impossible to even try to reflect upon whether or not the translations seemed reasonable which made it difficult to fully understand the performance of the model.\\n\\nAs mentioned earlier, the openNMT framework provides tools for preprocessing data. Preprocess- ing refers to modifying the text data to prepare it for training. This can for instance include remov- ing sentences that only is included in the bilingual data in one of the languages and removing dupli- cate sentences. The tools for preprocessing in the openNMT framework allowed us to perform these two operations on our data. Even though we did not have time to consider other types of preprocess- ing, I gained more insight into different types of preprocessing methods and how various techniques can be used in a low-resource language setting with the transformer model from reading Stenlund et al. (2023). In the paper, the results show that pre- processing with byte-pair-encoding and (random) stemming improved the performance of the low- resource language Inuktitut. Byte-pair-encoding is a technique used to break down words into smaller subword units and stemming refers to using the root/base form of the word. They also describe that the limited amount of training data resulted in lacking variance in the translations. Consequently,\\n\\n3\\n\\nthe translations had an abundance of unknown to- kens. I found this finding interesting as we saw the same problem for the models in our Swedish-Sámi system at first, and refining our preprocessing steps could therefore be something I would consider if I started working on the project now.\\n\\nAnother concept that is important for our project is the transformer model as all of the machine trans- lation models in our system are transformers from the open-source neural machine translation frame- work openNMT. I needed to understand the archi- tecture of the transformer to choose the various parameters that had to be included in the configura- tion of the openNMT transformer.\\n\\nThe transformer architecture (Vaswani et al., 2023) consists of two parts – an encoder and a decoder. On a high level, the transformer trans- lates an input sentence by first letting the encoder process it into an encoded representation of its to- kens. This encoded representation is then passed to the decoder which produces the translation of the sentence. Transformers utilize what is called atten- tion in the field of NLP. Attention describes which tokens in the complete sentence a model should take into consideration when a specific token shall be translated. Hence, attention can be used to al- low models to consider how a word relates to the other words in the sentence during translation. The transformer model uses multi-head-self-attention, to provide multiple perspectives of how the words are related in the sentence.\\n\\nThe transformer is a complex neural network approach to machine translation, and to exemplify, each of our models required approximately 115M parameters that needed to be learned from the train- ing. In the course, the sizes of state-of-the-art NLP models were discussed to some extent, but my understanding of the consequences of using huge state-of-the-art NLP models was enhanced by the hands-on experience with transformers. In the course, we did not deal with much longer training times than approximately 10 minutes, but to run our system with one of our datasets took around 2-3 hours on a GPU and this meant that we did not have time to train the model for as long as we wanted. The fact that we did not have time to use more than 3000 training steps, might have affected the performance of our models negatively. To put this in perspective, it took 3.5 days to train the orig- inal transformer until convergence when 300,000 training steps were used and up to 36M training\\n\\n208\\n\\n209\\n\\n210\\n\\n211\\n\\n212\\n\\n213\\n\\n214\\n\\n215\\n\\n216\\n\\n217\\n\\n218\\n\\n219\\n\\n220\\n\\n221\\n\\n222\\n\\n223\\n\\n224\\n\\n225\\n\\n226\\n\\n227\\n\\n228\\n\\n229\\n\\n230\\n\\n231\\n\\n232\\n\\n233\\n\\n234\\n\\n235\\n\\n236\\n\\n237\\n\\n238\\n\\n239\\n\\n240\\n\\n241\\n\\n242\\n\\n243\\n\\n244\\n\\n245\\n\\n246\\n\\n247\\n\\n248\\n\\n249\\n\\n250\\n\\n251\\n\\n252\\n\\n253\\n\\n254\\n\\n255\\n\\n256\\n\\n257\\n\\n258\\n\\n259\\n\\n260\\n\\n261\\n\\n262\\n\\n263\\n\\n264\\n\\n265\\n\\n266\\n\\n267\\n\\n268\\n\\n269\\n\\n270\\n\\n271\\n\\n272\\n\\n273\\n\\n274\\n\\n275\\n\\n276\\n\\n277\\n\\n278\\n\\n279\\n\\n280\\n\\n281\\n\\n282\\n\\n283\\n\\n284\\n\\n285\\n\\n286\\n\\n287\\n\\n288\\n\\n289\\n\\n290\\n\\n291\\n\\n292\\n\\n293\\n\\n294\\n\\n295\\n\\n296\\n\\n297\\n\\n298\\n\\n299\\n\\n300\\n\\n301\\n\\n302\\n\\n303\\n\\n304\\n\\n305\\n\\n306\\n\\n307\\n\\n308\\n\\n309\\n\\nsentences (Vaswani et al., 2023).\\n\\nSince our models took a long time to train, I started to consider if there exist other pre-trained models we could have used. BERT is a pre-trained encoder that was discussed during the course, and I have previously learned that it is possible to reduce the number of trainable parameters by freezing a majority of the pre-trained parameters and fine- tuning the rest to adjust the model to the new data. I found interesting results from using BERT as the encoder in a transformer model in a low-resource setting presented in Imamura and Sumita (2019). In the article, fine-tuning BERT and using it in the transformer model improved the BLEU score largely. In the article, the usage of BERT in en- coders is also compared to back-translation. As stated in the article, BERT encoders do not need bilingual corpora for training as the encoder only encodes the input data and not the actual transla- tion. However, the monolingual training data must be huge to fine-tune a BERT encoder. Due to this, it is suggested that the BERT approach is suitable when a high-resource language is the source lan- guage and a low-resource language is the target. In contrast, some bilingual data is needed to train a back-translation model (the reverse model in our work), but less monolingual data is required to cre- ate the synthetic bilingual data, and hence this ap- proach is suggested for middle-resource language pairs by the authors. I find this BERT approach to be an interesting extension to the work we have done, and it could have been an option if I had started working on the project today.\\n\\nThe choice of hyperparameters when training low-resource language has been shown to impact on the final model (Lankford et al., 2021). Hence, it might be possible to improve our models’ per- formances by changing the hyperparameters. Via the work I did previously in the course, I gained some understanding of the various parameters used when training a model, but since they often were provided, I received an enhanced understanding of their importance by working on the project. In (Lankford et al., 2021), a random search approach to finding the hyperparameters is shown to result in higher performance and an increased dropout rate (a mechanism to prevent a model from overfitting to the training data) of 0.3 instead of 0.1 as we used in the project can be beneficial in the trans- former architecture. If I would redo the project after learning this, I would put more consideration\\n\\n4\\n\\ninto the choice of hyperparameters based on my further reading.\\n\\n3 Articulate\\n\\nThe main learning outcome of this project is that I now know that synthetic data can be generated and used to train machine translation models. I have learned what back-translation is and how it can be implemented by reading existing research papers on the topic as it was not at all covered in the course. By actually implementing it I be- lieve that my understanding of the concept became even deeper as it required proper understanding of the gained theoretical knowledge. The concept of back-translation is not limited to specifically low- resource languages or one method, and I believe that this learning can be useful if I lack enough training data in future projects. Before, I have only worked with synthetic image data, and to me, the learning matters since I now am aware of the fact that synthetic data can be used in the field of NLP as well.\\n\\nSince I had never worked with openNMT be- fore, I also learned that this framework exists and how to use it and I learned this by implementing the baseline in our project. This learning matters since the framework provides me with tools to use state-of-the-art neural machine translation models simply and conveniently. I believe that I will use the framework in the future if I decide to work in the field of NLP since it was widely used in the research articles I read during the project.\\n\\nAs existing work on the topic of back-translation already has proved that the method is useful to improve the performance for both high- and low- resource languages, I believe that the importance of our work mainly lies in the chosen languages. The fact that we barely found a Swedish-Sami dataset strongly suggests that not much work has been done on machine translation with Sami – the lan- guage does not even exist on Google Translate. Our results indicate that it is possible to create a ma- chine translation model for the language despite the limited amount of existing data. Even if the BLEU scores are not close to the state-of-the-art models, this can not be considered expected based on the small amount and, possibly, low-quality data we found. However, if larger datasets are retrieved, our back-translation system could probably per- form even better.\\n\\n310\\n\\n311\\n\\n312\\n\\n313\\n\\n314\\n\\n315\\n\\n316\\n\\n317\\n\\n318\\n\\n319\\n\\n320\\n\\n321\\n\\n322\\n\\n323\\n\\n324\\n\\n325\\n\\n326\\n\\n327\\n\\n328\\n\\n329\\n\\n330\\n\\n331\\n\\n332\\n\\n333\\n\\n334\\n\\n335\\n\\n336\\n\\n337\\n\\n338\\n\\n339\\n\\n340\\n\\n341\\n\\n342\\n\\n343\\n\\n344\\n\\n345\\n\\n346\\n\\n347\\n\\n348\\n\\n349\\n\\n350\\n\\n351\\n\\n352\\n\\n353\\n\\n354\\n\\n355\\n\\n356\\n\\n357\\n\\n358\\n\\n359\\n\\n360 361 362 363 364 365 366\\n\\n367 368 369 370 371 372\\n\\n373 374 375 376 377 378 379\\n\\n380 381 382 383 384\\n\\n385 386 387 388 389\\n\\n390 391 392 393 394\\n\\n395 396 397 398 399 400 401\\n\\n402 403 404 405 406 407 408\\n\\n409 410 411 412 413 414 415\\n\\nReferences\\n\\nCheikh M. Bamba Dione, Alla Lo, Elhadji Mamadou Nguer, and Sileye Ba. 2022. Low-resource neural machine translation: Benchmarking state-of-the-art transformer for Wolof<->French. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 6654–6661, Marseille, France. Eu- ropean Language Resources Association.\\n\\nSergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding back-translation at In Proceedings of the 2018 Conference on scale. Empirical Methods in Natural Language Processing, pages 489–500, Brussels, Belgium. Association for Computational Linguistics.\\n\\nIsaac Feldman and Rolando Coto-Solano. 2020. Neu- ral machine translation models with back-translation for the extremely low-resource indigenous language Bribri. In Proceedings of the 28th International Con- ference on Computational Linguistics, pages 3965– 3976, Barcelona, Spain (Online). International Com- mittee on Computational Linguistics.\\n\\nKenji Imamura and Eiichiro Sumita. 2019. Recycling a pre-trained BERT encoder for neural machine trans- lation. In Proceedings of the 3rd Workshop on Neu- ral Generation and Translation, pages 23–31, Hong Kong. Association for Computational Linguistics.\\n\\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal- lenges for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation, pages 28–39, Vancouver. Association for Computa- tional Linguistics.\\n\\nSeamus Lankford, Haithem Alfi, and Andy Way. 2021. Transformers for low-resource languages: Is féidir linn! In Proceedings of Machine Translation Summit XVIII: Research Track, pages 48–60, Virtual. Associ- ation for Machine Translation in the Americas.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\\n\\nMichael Przystupa and Muhammad Abdul-Mageed. 2019. Neural machine translation of low-resource and similar languages with backtranslation. In Pro- ceedings of the Fourth Conference on Machine Trans- lation (Volume 3: Shared Task Papers, Day 2), pages 224–235, Florence, Italy. Association for Computa- tional Linguistics.\\n\\nMathias Hans Erik Stenlund, Mathilde Nanni, Micaella Bruton, and Meriem Beloucif. 2023. Improving translation quality for low-resource Inuktitut with var- ious preprocessing techniques. In Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing, pages 475–479, Varna, Bulgaria. INCOMA Ltd., Shoumen, Bulgaria.\\n\\n5\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention is all you need.\\n\\n416 417 418 419'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a37b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --q chromadb\n",
    "!pip install --q langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ce365bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32ec47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e445f22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|███████████████████████████| 4/4 [00:01<00:00,  2.73it/s]\n"
     ]
    }
   ],
   "source": [
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model='nomic-embed-text', show_progress=True),\n",
    "    collection_name='local-rag',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9361c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core. runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b56cedc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = 'llama3'\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd13c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the prompt format for Llama 3; see their documentation for details\n",
    "\n",
    "def make_prompt(message):\n",
    "    return f'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e635d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables = ['question'],\n",
    "    template = make_prompt('You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines.\\n\\nOriginal question: {question}'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39815675",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(),\n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9115b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = make_prompt(\"Answer the question based ONLY on the following context:\\n\\n{context}\\n\\nQuestion: {question}\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "076e85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {'context': retriever, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20468531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 81.55it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 67.02it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 93.30it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 59.18it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 95.18it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 64.52it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 92.63it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 64.02it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 95.38it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 62.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, this document appears to be a thesis or research paper in the field of Natural Language Processing (NLP). Specifically, it discusses the application of machine translation models, particularly transformer models, to translate text from Swedish to Northern Sámi. The document also touches on topics such as preprocessing data, attention mechanisms in transformers, and the challenges of working with low-resource language settings.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('What is this document about?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45c9d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 86.80it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 63.46it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 91.68it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 63.27it/s]\n",
      "OllamaEmbeddings: 100%|██████████████████████████| 1/1 [00:00<00:00, 101.10it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 60.05it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 93.73it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 53.69it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 94.35it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 46.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the context, the following languages are mentioned:\\n\\n1. Swedish\\n2. Northern Sámi\\n3. Norwegian\\n4. Finnish'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('What languages are mentioned in the paper?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7eda1bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 73.37it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 62.36it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 47.21it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 95.07it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 60.57it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 59.58it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 60.27it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 59.50it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 60.70it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 81.88it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 51.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"According to the text, the results from the evaluation for each language pair's baseline and final model are presented in Table 1. For the Swedish-Sámi model, the BLEU score of the final model is:\\n\\n24.35\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('What is the BLEU score of the final Swedish-Sámi model?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5da553e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 99.41it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 56.55it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 97.99it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 66.19it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 84.92it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 61.73it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 93.36it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 55.96it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 82.73it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 42.92it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 58.08it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 40.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, the main learnings articulated by the author are:\\n\\n1. The importance of quality in machine translation, as highlighted by the BLEU scores and the difficulty in evaluating the performance of the model.\\n2. The value of preprocessing data to prepare it for training, including techniques such as removing duplicate sentences and using byte-pair-encoding and stemming.\\n3. The transformer architecture's ability to utilize attention mechanisms to allow models to consider how words relate to each other in a sentence during translation.\\n4. The complexity and resource requirements of the transformer model, including its large number of parameters and need for long training times.\\n\\nThese learnings were gained through hands-on experience with the openNMT framework and by reading papers on machine translation, such as Stenlund et al. (2023) and Vaswani et al. (2023).\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('List the main learnings articulated by the author.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
