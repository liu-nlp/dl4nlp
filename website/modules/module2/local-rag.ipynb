{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc09aa7d",
   "metadata": {},
   "source": [
    "## Local RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9208bf",
   "metadata": {},
   "source": [
    "This is the local RAG system that Marco showcased during the last session of Meeting&nbsp;2. It requires you to install [Ollama](https://www.ollama.com/) and download the `nomic-text-embed` (for querying) and `llama-3` (for generating queries and answering questions) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2edd44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --q unstructured langchain\n",
    "!pip install --q \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6395c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59db99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"some.pdf\"    # insert your PDF filename here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e3d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab849b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].page_content    # shows the raw text content of the first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a37b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --q chromadb\n",
    "!pip install --q langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ce365bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32ec47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e445f22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|███████████████████████████| 4/4 [00:01<00:00,  2.73it/s]\n"
     ]
    }
   ],
   "source": [
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model='nomic-embed-text', show_progress=True),\n",
    "    collection_name='local-rag',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9361c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core. runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b56cedc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = 'llama3'\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd13c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the prompt format for Llama 3; see their documentation for details\n",
    "\n",
    "def make_prompt(message):\n",
    "    return f'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e635d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables = ['question'],\n",
    "    template = make_prompt('You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines.\\n\\nOriginal question: {question}'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39815675",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(),\n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9115b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = make_prompt(\"Answer the question based ONLY on the following context:\\n\\n{context}\\n\\nQuestion: {question}\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "076e85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {'context': retriever, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20468531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 81.55it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 67.02it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 93.30it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 59.18it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 95.18it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 64.52it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 92.63it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 64.02it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 95.38it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 62.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, this document appears to be a thesis or research paper in the field of Natural Language Processing (NLP). Specifically, it discusses the application of machine translation models, particularly transformer models, to translate text from Swedish to Northern Sámi. The document also touches on topics such as preprocessing data, attention mechanisms in transformers, and the challenges of working with low-resource language settings.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('What is this document about?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45c9d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 86.80it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 63.46it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 91.68it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 63.27it/s]\n",
      "OllamaEmbeddings: 100%|██████████████████████████| 1/1 [00:00<00:00, 101.10it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 60.05it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 93.73it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 53.69it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 94.35it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 46.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the context, the following languages are mentioned:\\n\\n1. Swedish\\n2. Northern Sámi\\n3. Norwegian\\n4. Finnish'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('What languages are mentioned in the paper?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7eda1bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 73.37it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 62.36it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 47.21it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 95.07it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 60.57it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 59.58it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 60.27it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 59.50it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 60.70it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 81.88it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 51.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"According to the text, the results from the evaluation for each language pair's baseline and final model are presented in Table 1. For the Swedish-Sámi model, the BLEU score of the final model is:\\n\\n24.35\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('What is the BLEU score of the final Swedish-Sámi model?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5da553e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 99.41it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 56.55it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 97.99it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 66.19it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 84.92it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 61.73it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 93.36it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 55.96it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 82.73it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 42.92it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 58.08it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 40.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, the main learnings articulated by the author are:\\n\\n1. The importance of quality in machine translation, as highlighted by the BLEU scores and the difficulty in evaluating the performance of the model.\\n2. The value of preprocessing data to prepare it for training, including techniques such as removing duplicate sentences and using byte-pair-encoding and stemming.\\n3. The transformer architecture's ability to utilize attention mechanisms to allow models to consider how words relate to each other in a sentence during translation.\\n4. The complexity and resource requirements of the transformer model, including its large number of parameters and need for long training times.\\n\\nThese learnings were gained through hands-on experience with the openNMT framework and by reading papers on machine translation, such as Stenlund et al. (2023) and Vaswani et al. (2023).\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('List the main learnings articulated by the author.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
