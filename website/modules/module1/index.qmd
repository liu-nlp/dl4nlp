---
title: "Module 1"
---

This module introduces one of the most fundamental ideas in modern NLP: the idea that words can be represented as vectors which can be learned from text data. On the application side of things, we focus on one of the most fundamental NLP tasks: text categorization.

Before working with the material in this module, you may want to have a look at the following:

* Capsule introduction to NLP [[slides](slides/intro.pdf)] [[video](https://youtu.be/6u7u1cpVT7Y)]
* [Review materials](review.md)

## Unit 1-1: Introduction to representations of words and documents

| Title | Slides | Video |
|---|---|---|
| Introduction to the module | | |
| Learning word and document representations; embeddings | | |
| Tokenization | | |
| Deep learning in NLP: practical considerations | | |
: {.striped}

## Unit 1-2: Language modelling and contextualized embeddings

| Title | Slides | Video |
|---|---|---|
| Introduction to language modelling | [[slides](slides/slides-121.pdf)] | [[video](https://youtu.be/_TlvRtoGOn8)] (15:03) |
| N-gram language models | [[slides](slides/slides-122.pdf)] | [[video](https://youtu.be/GuTAMrqiFSM)] (16:35) |
| Recurrent neural networks | [[slides](slides/slides-123.pdf)] | [[video](https://youtu.be/RWxoiA2uGZU)] (18:34) |
| The LSTM architecture | [[slides](slides/slides-124.pdf)] | [[video](https://youtu.be/8es6TKvw7qI)] (12:55) |
| Recurrent neural network language models | [[slides](slides/slides-125.pdf)] | [[video](https://youtu.be/J-KglYsuJ28)] (11:14) |
| Contextualized word embeddings | [[slides](slides/slides-126.pdf)] | [[video](https://youtu.be/5oXWnSBoe7A)] (13:00) |
: {.striped}

## Unit 1-3: Transformer-based language models

| Title | Slides | Video |
|---|---|---|
| Attention | [[slides](slides/slides-221.pdf)] | [[video](https://youtu.be/HHUR6VX5CeU)] (15:13) |
| The Transformer architecture | [[slides](slides/slides-222.pdf)] | [[video](https://youtu.be/dSd0-RFZLnk)] (13:43) |
| Pre-trained transformer models 1 (GPT) | [[slides](slides/slides-223.pdf)] | [[video](https://youtu.be/9QE1bQTSbx8)] (22:53) |
| Pre-trained transformer models 2 (BERT) | [[slides](slides/slides-224.pdf)] | [[video](https://youtu.be/JeY6N1012Sg)] (18:59) |
| Generation algorithms | [[slides](https://www.cse.chalmers.se/~richajo/dat450/lectures/l7/m7_3.pdf)] | [[video](https://youtu.be/QtwpM-OGOew)] (26:42) |
: {.striped}
  
**Reading:**

* [Cheng et al. (2016)](https://www.aclweb.org/anthology/D16-1053/)
* [Vaswani et al. (2017)](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
* [Serrano and Smith (2019)](https://www.aclweb.org/anthology/D18-1216/)
* [Radford et al. (2018)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
* [Devlin et al. (2019)](dx.doi.org/10.18653/v1/N19-1423)


## Assignment 1

The first programming assignment will be about training a language model and using it to generate text.
